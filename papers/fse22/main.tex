%% The first command in your LaTeX source must be the \documentclass command.
% \documentclass[sigconf, review]{acmart}
\documentclass[sigconf,review,anonymous]{acmart}

% ----------------------------------------

%% Packages

\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{color}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{caption}
\usepackage[ruled,vlined,noend]{algorithm2e}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{tikzscale}
\usepackage{pgfplots}
\usepackage{paralist}
\usepackage{enumitem}
\usepackage{titlesec}

\titlespacing*{\section}{0pt}{5pt}{2pt}
\titlespacing*{\subsection}{0pt}{5pt}{2pt}
\titlespacing*{\subsubsection}{0pt}{5pt}{2pt}

\titlespacing*{\paragraph}{10pt}{0pt}{10pt}

\settopmatter{printacmref=false}

\setitemize{noitemsep,nosep,wide,topsep=0pt,parsep=0pt,partopsep=0pt}

\pgfplotsset{compat=1.3}

\usetikzlibrary{trees}

\makeatletter
\patchcmd{\@verbatim}
  {\verbatim@font}
  {\verbatim@font\footnotesize}
  {}{}
\makeatother

\let\oldv\verbatim
\let\oldendv\endverbatim

\def\verbatim{\par\setbox0\vbox\bgroup\oldv}
\def\endverbatim{\oldendv\egroup\fboxsep0pt \noindent\colorbox[gray]{0.95}{\usebox0}\par}

\linepenalty=1000
\widowpenalty=10000
\clubpenalty=10000

\setlength{\skip\footins}{5pt}

\setlength{\textfloatsep}{5pt}
\setlength{\floatsep}{5pt}

\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}

% ----------------------------------------

%% Rights management information. This information is sent to you when you
%% complete the rights form. These commands have SAMPLE values in them; it is
%% your responsibility as an author to replace the commands and values with
%% those provided to you when you complete the rights form.
\setcopyright{acmcopyright}
%% \copyrightyear{2021}
%% \acmYear{2021}
%% \acmDOI{10.1145/1122445.1122456}

%% These commands are for a JOURNAL article.
\acmConference[ESEC/FSE 2022]{The 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{14 - 18 November, 2022}{Singapore}
%% \acmJournal{JACM}
%% \acmVolume{37}
%% \acmNumber{4}
%% \acmArticle{111}
%% \acmMonth{8}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%% \citestyle{authoryear}

% ----------------------------------------
%% Macros

\newcommand{\quickcheck}{\textit{QuickCheck}\xspace}
\newcommand{\quickchick}{\textit{QuickChick}\xspace}
\newcommand{\fuzzchick}{\textit{FuzzChick}\xspace}
\newcommand{\mutagen}{\textsc{Mutagen}\xspace}

% ----------------------------------------

%% end of the preamble, start of the body of the document source.

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[\mutagen]{\mutagen: Coverage-Guided, Property-Based Testing using
  Exhaustive Structure-Preserving Mutations}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

%% Hidden authors in the generated PDF.

\author{Agust\'in Mista}
\email{mista@chalmers.se}
\affiliation{%
  \institution{Chalmers University of Technology}
  \city{Gothenburg}
  \country{Sweden}
}

\author{Alejandro Russo}
\email{russo@chalmers.se}
\affiliation{%
  \institution{Chalmers University of Technology}
  \city{Gothenburg}
  \country{Sweden}
}

%% By default, the full list of authors will be used in the page headers. Often,
%% this list is too long, and will overlap other information printed in the page
%% headers. This command allows the author to define a more concise list of
%% authors' names for this purpose.

%% \renewcommand{\shortauthors}{Mista and Russo}


% ----------------------------------------
%% The abstract is a short summary of the work to be presented in the article.

\begin{abstract}
Automatically synthesized random data generators are an appealing option when
using property-based testing.
%
There exists a variety of techniques that extract static information from the
codebase to produce random test cases.
%
Sadly, such techniques cannot derive generators capable of producing random
values satisfying complex invariants, often needed to test properties with
sparse preconditions.


Coverage-guided, property-based testing (CGPT) is a technique that tackles this
limitation by enhancing synthesized generators with structure-preserving
mutations guided by execution traces.
%
Albeit effective, CGPT relies largely on randomness and exhibits poor
scheduling, preventing its applicability in real-world software.


This work presents \mutagen, a CGPT framework that overcomes such limitations by
generating mutants \emph{exhaustively}.
%
Our tool incorporates heuristics to help minimizing scalability issues as well
as covering the search space in a principled manner.   
%
% schedule mutants based on their
% novelty and to minimize mutating certain subexpressions leading to scalability
% issues of exhaustiveness.
%of large enumeration types more
%than necessary.
%
Our evaluation show that \mutagen not only outperforms existing CGPT tools, but
also finds previously unknwown bugs in real-world software.
\end{abstract}

% ----------------------------------------

%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. Please
%% copy and paste the code instead of the example below.
\begin{CCSXML}
% CCS XML here
\end{CCSXML}

%% \ccsdesc[500]{Computer systems organization~Embedded systems}
%% \ccsdesc[300]{Computer systems organization~Redundancy}
%% \ccsdesc{Computer systems organization~Robotics}
%% \ccsdesc[100]{Networks~Network reliability}

% ----------------------------------------

%% Keywords. The author(s) should pick words that accurately describe the work
%% being presented. Separate the keywords with commas.
\keywords{random testing, mutations, heuristics}

% ----------------------------------------

%% This command processes the author and affiliation and title information and
%% builds the first part of the formatted document.
\maketitle

% ----------------------------------------

% \vspace{-5pt}

\section{Introduction}
\label{sec:intro}

%% PBT and generation of interesting data for free
%
Popularized by \quickcheck \cite{ClaessenH00}, Random Property-Based Testing
(RPBT) is a popular technique for finding bugs
\cite{ClaessenH00,hughes2003erlang, papadakis2011proper, bulwahn2012new,
  denes2014quickchick}.
%
% This is achieved by using randomly generated inputs to instantiate executable
It uses randomly generated inputs to instantiate executable properties encoding
the expected behavior of the system under test.
%
%% Exemplified by frameworks like Haskell's \quickcheck \cite{ClaessenH00} and
%the % Abundant ports of this tool in other programming languages %
%\cite{hughes2003erlang, papadakis2011proper, bulwahn2012new, %
%denes2014quickchick}
%
%% The most common PBT approach is to instantiate the testing properties using %
%randomly generated inputs.
%
% However, users of RPBT in general) are well aware that one of the biggest
% challenges while using RPBT is to provide suitable random data generators
% needed to instantiate the testing properties.
However, a limiting factor of RPBT is having to provide the random data
generators used to instantiate the testing properties. 
%
Writing highly-tuned generators capable of exercising every part of a complex
system on a reasonable basis can take several thousand person-hours of trial and
error \cite{lampropoulos2019coverage}.
%
Luckily, there exist several approaches to automatically synthesize random data
generators by reifying the static information present in the codebase, e.g.,
data type definitions, application public interfaces (APIs), etc.
\cite{GriecoCB16, DBLP:conf/haskell/MistaRH18, Mista2019GeneratingRS,
DuregardJW12, Lampropoulos2017, Bendkowski2017}.
%
These approaches, however, are unable to synthesize generators capable of
producing data satisfying complex invariants not easily derivable from the
codebase.
%
Randomly generating valid programs to test compilers is an example of this
limitation \cite{chen2020asurvey}, where developers are forced to put
substantial efforts in writing specialized generators by hand \cite{Palka11,
perenyi2020stack, yang2011finding}.
%
Moreover, automatically synthesized generators behave poorly when used to test
properties with sparse preconditions.
%
In this scenario, most tests are simply thrown away before reaching the main
components of the system under test.


%% FuzzChick: PBT + code instrumentation + type-preserving mutations
%% \fuzzchick \cite{lampropoulos2019coverage} is a property-based testing framework
%% for the Coq programming language that
%
Coverage-Guided, Property-Based Testing (CGPT) \cite{lampropoulos2019coverage}
is a technique that borrows ideas from the fuzzing community to generate
highly-structured values while still using automatically derived generators.
%
%% Notably, this tool is implemented as an extension of \quickchick
%% \cite{denes2014quickchick}, Coq's own reimplementation of \quickcheck.
%
Instead of continuously generating random invalid test cases from scratch, CGPT
keeps queues of \emph{interesting} previously executed test cases that can be
transformed using structure-preserving mutations to produce new ones.
%
%% Mutations are achieved using high-level, structure-preserving transformations
%% that can be derived directly from the codebase.
%
Intuitively, mutating an existing interesting test case is more likely to
produce a new interesting test case than generating a new one from scratch.


Unlike the generic bit-level mutators often used in the fuzzing community
\cite{}, structure-preserving mutations specified at the data type level can
effectively produce syntactically valid mutants. 
%
% %
% High-level mutators avoid wasting time with generic low-level mutators that work
% at the bit level and know very little about data structures, thus producing
% syntactically broken mutants most of the time.
%
This grammar-aware technique has shown to be effective when fuzzing systems
accepting structurally complex inputs \cite{holler2012fuzzing, wang2019superion,
xsmith}.
%
In practice, most tools use existing grammars to tailor mutations to the input
structure used by the system under test. 
%
% Notably external grammars are not required in CGPT.
%
%% Tools like LangFuzz \cite{holler2012fuzzing}, Superion \cite{wang2019superion},
%% XSmith \cite{xsmith} use existing grammars to tailor the mutators to the
%% specific input structure used by the system under test.
%
% In CGPT, external grammars are not required.
%
In contast, CGPT relies on the data type information of the inputs to the
testing properties to derive specialized structure-preserving mutators directly
--- making strongly-typed programming languages an ideal match for this
technique.
%
% already provide a good source of information about how data can be mutated,
% e.g., by changing one data constructor by another.
%
%
% The data types used by the inputs of the testing properties already describe the
% structure of the random data one needs to mutate in a concrete manner, and
% Specialized mutators can be automatically derived directly from data types
% definitions ---
%

%% In this light, CGPT is likely to preserve the semantic structure of the mutated
%% data, as mutations are applied directly at the data type level, e.g., the
%% program's AST in case of generating code.
%
% Moreover, CGPT uses execution traces to distinguish which executed test cases
Moreover, CGPT uses execution traces to distinguish interesting test cases --- a
popular technique known as \emph{coverage-guided fuzzing} and popularized by
tools like \emph{AFL} \cite{afl}.
%
% Mutated test cases are considered interesting for mutation
Here, test cases are interesting (and therefore worth mutating) only when they
produce new execution traces --- any other test case is simply thrown away.

% Worth mentioning, CGPT is not meant to replace sophisticated manually-written
% random generators, but rather to provide an adequate testing solution for early
% stages of development.
%, and while a better test suite using manually-written generators is
%still under construction.


%% Of course, this technique is not meant to replace smart manually-written
%% generators.
%% %
%% In turn, it provides an acceptable solution that can be used in the early stages
%% of development, and while a better test suite using manually-written generators
%% is still under construction.
%% %
%% %% Problems of FuzzChick: mutations are superficial, long queues, power
%% %% scheduler
%% %
%% %% In this work, we take a close look at \fuzzchick for opportunities to improve.
%% %
%% %% Notably, we establish several aspects of \fuzzchick that can be
%% %% revised.%
%% %
%% But \fuzzchick suffers several disadvantages.%
%% \footnote{Some of these were already identified by the authors of \fuzzchick in
%%   their original work.}
%% %
%% In particular:
%% %
%% \begin{inparaenum}
%% \item the type-preserving mutations produced by this tool are superficial
%%   and rely heavily on randomness to produce diverse mutants,
%% \item the queueing mechanism can cause large delays when interesting
%%   values are enqueued frequently, and
%% \item the heuristic used to assign a certain ``mutation energy'' to each
%%   interesting test case (often referred to as the \emph{power schedule})
%%   requires fine tuning and can be hard to generalize to work well under
%%   different testing scenarios.
%% \end{inparaenum}
%% %
%% Moreover, we replicated the Information-Flow Control (IFC) stack machine case
%% study bundled with \fuzzchick and observed a surprising limitation:
%% %
%% \emph{after repeating each experiment 30 times, \fuzzchick was only able to find
%%   5 out of the 20 injected bugs with a 100\% efficacy, the hardest one being
%%   found only around 13\% of the time after an hour of testing.}
%% %
%% Despite this limitation, \citeauthor{lampropoulos2019coverage} show that
%% \fuzzchick is far better than using normal random testing coupled with na\"ive
%% random generators by comparing the mean-time-to-failure (MTTF) against
%% \emph{QuickChick}.
%% %
%% However, we believe that a proper evaluation ought to
%% %
%% take failure to find a counterexample as an
%% %
%% important metric when comparing PBT tools, not just mean time to failure ---
%% when one is found.

%
%% Problems of FuzzChick: mutations are superficial, long queues, power
%% scheduler
%
%% In this work, we take a close look at \fuzzchick for opportunities to improve.
%
%% Notably, we establish several aspects of \fuzzchick that can be
%% revised.%
%
%% The approach taken by \fuzzchick leaves considerable room for
%% improvements in several aspects.%
%

In this work, we establish several aspects%
%
\footnote{Some of these limitations were already identified by the authors in
their original work.}
%
of the original CGPT approach by \citeauthor{lampropoulos2019coverage} that
leave considerable room for improvement (see \S \ref{sec:background}).
%
In particular:
%
\begin{inparaenum}
\item automatically derived structure-preserving mutators can turn out
"shallow", ineffective at transforming deep test cases more than superficially;
  % superficial and rely heavily on randomness to produce diverse mutants,
\item the queuing mechanism can cause delays if interesting test cases are
  enqueued frequently and there is no way to prioritize them; and
\item the heuristic used to assign a ``mutation budget'' to each interesting
  test case (often referred to as a \emph{power schedule}) requires fine tuning
  and can be hard to generalize.
%to different kinds of testing properties.
\end{inparaenum}
%
%% Moreover, we replicated the Information-Flow Control (IFC) stack machine case
%% study bundled with \fuzzchick and observed a surprising limitation:
%% %
%% after repeating each experiment 30 times, \fuzzchick could only find 5 out of
%% the 20 injected bugs with a 100\% efficacy.
%% %
%% %% the hardest one being found only around 13\% of the time after an hour of
%% %% testing.
%% %
%% Despite this, \citeauthor{lampropoulos2019coverage} show that \fuzzchick is far
%% better than using normal random testing coupled with na\"ive random generators
%% by comparing the mean-time-to-failure (MTTF) against \emph{QuickChick}.
%% %
%% However, we believe that a more meticulous evaluation ought to take failure to
%% find a counterexample as an important metric when comparing PBT tools, not just
%% mean time to failure --- when one is found.


%% Our solution
To tackle these limitations, we introduce \mutagen, a CGPT framework that
applies mutations \emph{exhaustively} (see \S \ref{sec:mutagen}).
%
%% implemented in Haskell
%% that also builds upon code instrumentation and type-preserving mutations
%
%
% the basic ideas behind \fuzzchick, i.e.,
%
%% \mutagen is introduced in detail in \S \ref{sec:mutagen}.
%
%% Exhaustive mutations
%% Unlike \fuzzchick, mutations in \mutagen are applied \emph{exhaustively}.
%
That is, given an interesting test case, our tool precomputes and schedules
every structure-preserving mutation that can be applied to it exactly once.
%
This approach has two inherent advantages. %when compared to \fuzzchick.
%
On one hand,
%  mutations do not rely on randomness to be generated.
%
every subexpression of the input test case is mutated on the same basis,
guaranteeing that no interesting transformation is omitted due to randomness.
%
%% On the other hand, scheduling mutations exhaustively eliminates the need for a
%% power schedule to assign a given mutation energy to each input test case.
%
On the other hand, scheduling mutations exhaustively eliminates the need for a
power schedule to assign a mutation budget to every interesting test case.


%% Notably, \emph{scheduling mutations exhaustively does not mean that mutations
%%   are always exhaustive.}
%
Internally, our tool distinguishes two kinds of mutations.
%  those that can be computed
% deterministically, 
% and those that can be
% obtained non-deterministically.
%
On one hand, \emph{deterministic (pure) mutations} encode transformations that
yield a single mutated test case obtained by swapping data constructors around,
as well as rearranging or returning subexpressions.
%
On the other hand, \emph{non-deterministic (random) mutations} are used to
represent transformations over large enumeration types.
%
% like numbers or characters.
%
This mechanism let us selectively escape the scalability issues of
exhaustiveness by yielding a random generator that replaces a specific
subexpression of an input test case with a randomly generated one.
%
Such generator is then sampled a \emph{small} number of times. 
%
This way \mutagen avoids, for instance, mutating every numeric subexpresion of a
test case into every other possible number.


%% Heuristics to improve testing loop
In addition, the testing loop of \mutagen incorporates two novel heuristics that
help finding bugs faster and more reliably (\S \ref{sec:heuristics}).
%
In the first place, our tool uses last-in first-out (LIFO) scheduling with
priority when enqueueing interesting test cases for mutation.
%
The priority is obtained by comparing the novelty of each new test case with
respect to already seen ones.
%
This way, interesting test cases that discover new parts of the code earlier
during execution are given a higher priority.
%
Moreover, this scheduling allows the testing loop to jump back and forth between
enqueued test cases as soon as new more interesting ones become available,
eliminating potential delays when the mutation queues grow large.


The second heuristic adjusts the number of test cases generated by random
mutations. 
%
% number of times our tool samples random
% mutants, i.e., those corresponding to large enumeration types as described
% above.
%
For this, we keep track of how often we generate interesting test cases.
%
If this frequency stalls, \mutagen resets the testing loop and increases the
effort put into sampling random mutations.
%
This automatically finds a suitable value for this parameter on the fly.


%% Validation
We validated our ideas using two case studies (described in detail in \S
\ref{sec:casestudies}): the Information-Flow Control (IFC) stack machine used by
\citeauthor{lampropoulos2019coverage} in their original work, and an existing
{Web-Assembly} engine of industrial strength written in Haskell.
%
In both case studies, we compare the effect of the heuristics implemented on top
of the base testing loop of \mutagen.
%
In the IFC stack machine case study, we compare \mutagen against \fuzzchick, the
reference CGPT implementation developed in Coq \cite{lampropoulos2019coverage}.
%
Our results (\S \ref{sec:evaluation}) indicate that \mutagen outperforms
\fuzzchick both in terms of time to failure and failure rate.
%
On the other hand, our WebAssembly case study shows the performance of our tool
in a realistic scenario.
%
There, \mutagen is capable of reliably finding 15 planted bugs in the validator
and interpreter, as well as 3 previously unknown ones.
% bugs that flew under the radar
%of the existing unit test suite of this engine.
%
% This case study also help us to compare the performance of our tool against a
% more traditional RPBT approach that does not rely on code instrumentation.
In this case study we also compare the performance versus the overhead of our
tool (and our custom code instrumentation mechanism) by comparing it against
\quickcheck (black-box).  
%
All in all, our evaluation encompasses more than 600 hours of computing time and
indicates that \emph{testing mutants exhaustively together with our heuristics
to escape scalability issues} can be an appealing technique for finding bugs
reliably without sacrificing speed.%
%
\footnote{A replication package \cite{anon_2021} is available for reviewing
  purposes.}

%% Last two sections
Finally, we discuss related work in \S \ref{sec:related} and conclude
in \S \ref{sec:conclusions}.


% ----------------------------------------
%% \vspace{-10pt}

\section{Background}
\label{sec:background}

This section briefly introduces the motivation, ideas and limitations behind
CGPT \cite{lampropoulos2019coverage}.
%
To illustrate this technique, we focus on a simple property defined over binary
trees.
%
Such a data structure can be defined in Haskell using a custom algebraic data
type with two data constructors for leaves and branches respectively:

\begin{verbatim}
data Tree a = Leaf a | Branch (Tree a) a (Tree a)
\end{verbatim}

\noindent The type parameter \texttt{a} indicates that trees can be instantiated
using any type as payload, so the value \texttt{Leaf True} has type \texttt{Tree
Bool}, whereas the value \texttt{Branch (Leaf 1) 2 (Leaf 3)} has type
\texttt{Tree Int}.
%
%% Then, we can define tree reflections using a simple recursive function that
%% pattern matches against the two possible constructors, inverting the order of
%% the subtrees whenever it encounters a branch:
%
%% \begin{verbatim}
%% mirror :: Tree a -> Tree a
%% mirror (Leaf x)       = Leaf x
%% mirror (Branch l x r) = Branch (mirror r) x (mirror l)
%% \end{verbatim}
%
%% Later, a reasonable requirement to assert for is that \texttt{mirror} must be
%% \emph{involutive}, i.e., reflecting a tree twice always yields the original
%% tree.
%% %
%
If we assume existence of a function \texttt{balanced} of type \texttt{Tree a ->
Bool} that asserts that a tree satisfies a certain notion of balancedness, we
can write testing properties to validate that the operations defined over the
\texttt{Tree} data type preserve this invariant.
%
For instance, the following can be used to validate the implementation of an
\texttt{insert} function:

\begin{verbatim}
prop_insert_inv :: a -> Tree a -> Property
prop_insert_inv x t = balanced t ==> balanced (insert x t)
\end{verbatim}

\noindent This property asserts that, given an element \texttt{x} and a balanced
tree \texttt{t} as input, inserting \texttt{x} into \texttt{t} will produce a
balanced tree as output.
%
(The implementation details of \texttt{balanced} and \texttt{insert} are not
important here.)
%
The arrow operator (\texttt{==>}) indicates that \texttt{balanced t} is a
precondition of this property, and so using an unbalanced tree as input will
result in the test case getting \emph{discarded}.


The only missing piece is a random generator of trees.
%
For this, we can define a na\"ive generator for trees of integers as:
%
%% In \quickcheck, this is commonly done via the type class mechanism
%% \cite{jones1997type}, instantiating the \texttt{Arbitrary} type class with a
%% random generator as the implementation of the overloaded \texttt{arbitrary}
%% operation:
%% \footnote{In practice, this code will most likely be implemented using Haskell's
%%   type-class mechanism, hiding the complexity behind the size limit and the
%%   payload generator.}%

%% \begin{verbatim}
%% instance Arbitrary a => Arbitrary (Tree a) where
%%   arbitrary = sized gen where
%%   gen 0 = Leaf <$> arbitrary
%%   gen n = oneof [ Leaf <$> arbitrary
%%                 , Branch <$> gen (n-1) <*> arbitrary <*> gen (n-1) ]
%% \end{verbatim}

% gen_tree :: Size -> Gen a -> Gen (Tree a)

%% \begin{verbatim}
%% genTree size = case size of
%%   0 -> Leaf <$> genA
%%   n -> oneof [ Leaf <$> genA
%%              , Branch <$> genTree (n-1) <*> genA <*> genTree (n-1) ]
%% \end{verbatim}

\begin{verbatim}
genTree(size) = if size == 0 
  then do { x <- genInt; return (Leaf x) }
  else oneof [ do { x <- genInt; return (Leaf x) }
             , do { l <- genTree(size-1);
                    x <- genInt;
                    r <- genTree(size-1);
                    return (Branch l x r) } ]
\end{verbatim}

\noindent This definition (simplified to make it more accessible) follows a
common type-directed approach, and illustrates what to expect from a generator
synthesized automatically from the codebase.
%
At each step, this generator randomly picks a \texttt{Tree} data constructor
with uniform probability (i.e, either a \texttt{Leaf} or a \texttt{Branch}), and
calls itself for every recursive subexpression, carefully reducing the input
size limit \texttt{size} by a unit at a time.
%
This enforces termination by generating only leaves when the size reaches zero
(case \texttt{size == 0}).
%
Integers are generated simply by delegating the task to an external random
generator (\texttt{genInt}) defined elsewhere.
%

%% Subexpressions of any other type are generated
%% simply by delegating the task to their corresponding \texttt{Arbitrary} instance
%% --- as noted by the use of \texttt{arbitrary} when generating random payloads
%% inside of the \texttt{Leaf} and \texttt{Branch} constructors.

Readers familiar with RBPT will recognize that using \texttt{genTree} to test
\texttt{prop\_insert\_inv} with \quickcheck directly does not work.
%
In practice, \quickcheck gives up if most of the generated test cases get
discarded.
%
%because they do not satisfy the balanced precondition.
%
Sadly, most of the test cases produced by \texttt{genTree} consist of unbalanced
trees, thus the interesting part of the property (\texttt{balanced (insert x
t)}) is rarely exercised.
%% %
%% In other words, \emph{the precondition of our testing property becomes very
%%   sparse when we use simple, automatically derived generators.}

% The next subsection shows how GCPT helps overcoming this limitation without
% having to write specialized generators by hand.


%% At this point it is reasonable to think that, to obtain the best results when
%% using PBT over complex systems, one is forced to put a large amount of time into
%% developing manually-written generators.
%% %
%% In practice, that is most often the case, no automatic effort can beat a
%% well-thought manually-written generator that produces interesting complex values
%% and finds bugs in very few tests.
%% %
%% Not all is lost, however.
%% %
%% It is still possible to obtain acceptable results automatically by incorporating
%% dynamic information from the system under test into the testing loop.
%% %
%% The next subsection introduces the clever technique used by \fuzzchick to find
%% bugs in complex systems while using simple automatically derived random
%% generators.

%% Far from trying to compete against manual-efforts, our work focuses on improving
%% automatic approaches so they can be used paliatively in earlier development
%% stages.

\subsection{Coverage-Guided Property-Based Testing}

To alleviate the problem of testing properties with non-trivial preconditions
while using automatically derived random generators, CGPT
%
%% \citeauthor{lampropoulos2019coverage} introduced \emph{coverage-guided,
%%   property-based testing} (CGPT), a technique that
%
enhances the testing process with two key characteristics:
%
\begin{inparaenum}
\item \emph{target code instrumentation}, to capture execution information from
  each test case; and
\item \emph{high-level, structure-preserving mutations}, to produce
  syntactically valid test cases by altering existing ones at the data
  constructor level.
\end{inparaenum}

Using code instrumentation in tandem with mutations is a well-known technique in
the fuzzing community.
%
Generic fuzzing tools like \emph{AFL} \cite{afl}, \emph{HonggFuzz}
\cite{honggfuzz}, or \emph{libFuzzer} \citeyearpar{libfuzzer} as well as
language-specific ones like Crowbar \cite{dolan2017testing} or \emph{Kelinci}
\cite{kersten2017poster} use execution traces to recognize interesting test
cases, 
%
% e.g, those that exercise previously undiscovered parts of the target
% code.
%
Later, such tools use generic mutators to combine and produce new test cases
from previously executed interesting ones.
%
%% Instead of mutating any previously executed test case that discovers a new part
%% of the code, this technique integrates these fuzzing mechanisms into the PBT
%% testing loop itself.
%
In contrast with traditional fuzzing approaches, CGPT can distinguish
semantically valid test cases from invalid ones, i.e., those passing the testing
property preconditions as opposed to those that are discarded early.
%
This information is used to favour mutating valid test cases over discarded
ones.

%% Since it is possible to distinguish semantically valid test cases from invalid
%% ones, i.e., those passing the testing property preconditions as opposed to those
%% that are discarded early, CGPT exploits this information to prefer mutating
%% valid test cases than discarded ones.

The testing loop in CGPT uses two queues to store valid and discarded previously
executed test cases.
%
Enqueued test cases are stored along with a given mutation budget that controls
how many times a given test case can be mutated before being finally thrown
away.
%
This budget is calculated using a heuristic derived from AFL's power schedule,
i.e., more budget to test cases that lead to shorter executions, or that
discover more parts of the code.
%
Then, on each iteration the algorithm selects the next test case by mutating the
first value on the queue of valid test cases.
%
If that queue is empty, it instead mutates the first test case from the queue of
discarded test cases.
%
If both queues are empty, CGPT generates a new random value from scratch.
%
The algorithm then run this test case and evaluates whether it was interesting
(i.e., it exercises a new path) based on its trace information.
%
If the test case does in fact discover a new path, it is enqueued at the end of
its corresponding queue, depending on whether it passed the property
precondition or was discarded.
%
%% (i.e., it failed the precondition).
%
This process alternates between generation and mutation until a bug is found or
it reaches the test limit.


%% %
%% Enqueued values are stored along with a given mutation budget, that controls how
%% many times a given test case can be mutated before being finally thrown away.


%% Once inside of the main loop, the algorithm picks the next test case using a
%% simple criterion: if there are valid values enqueued for mutation, it picks the
%% first one, mutates it and returns it, decreasing its energy by one.
%% %
%% If \textit{QValid} is empty, then the same is attempted using \textit{QDiscarded}.
%% %
%% If none of the mutation queues contain any candidates, CGPT generates a new
%% value from scratch.
%% %
%% This selection process is illustrated in detail in Algorithm
%% \ref{algo:fuzzchick:pick}.


%% Having selected the next test case, the main loop proceeds to execute it,
%% capturing both the result (passed, discarded due to preconditions, or failed)
%% and its execution trace over the system under test.
%% %
%% If the test case fails, it is immediately reported as a bug.
%% %
%% If not, the algorithm evaluates whether it was interesting (i.e., it exercises a
%% new path) based on its trace information and the one from previously executed
%% test cases (represented by \textit{TLog}).
%% %
%% If the test case does in fact discover a new path, it is enqueued at the end of
%% its corresponding queue, depending on whether it passed or was discarded.
%% %
%% %% (i.e., it failed the precondition).
%% %
%% This process alternates between generation and mutation until a bug is found or
%% we reach the test limit.


%% In addition, high-level, structure-preserving mutators are better suited for
%% producing syntactically valid mutants, avoiding the time wasted by using generic
%% low-level mutators that act over the ``serialized'' test cases and know very
%% little about their structure, thus producing syntactically broken mutants most
%% of the time.
%% %
%% This grammar-aware mutation technique has shown to be quite useful when fuzzing
%% systems accepting structurally complex inputs.
%% %
%% Tools like LangFuzz \cite{holler2012fuzzing}, Superion \cite{wang2019superion},
%% XSmith \cite{xsmith} use existing grammars to tailor the mutators to the
%% specific input structure used by the system under test.
%% %
%% In CGPT, external grammars are not required.
%% %
%% The data types used by the inputs of the testing properties already describe the
%% structure of the random data we want to mutate in a concrete manner, and
%% specialized mutators acting at the data constructor level can be automatically
%% derived directly from their definition.


%% \paragraph{Testing Loop}
%% Outlined in Algorithm \ref{algo:fuzzchick:loop}, CGPT starts by creating two
%% queues, \textit{QValid} and \textit{QDiscarded} for valid and discarded previously
%% executed test cases, respectively.
%% %
%% Enqueued values are stored along with a given mutation budget, that controls how
%% many times a given test case can be mutated before being finally thrown away.


%% Once inside of the main loop, the algorithm picks the next test case using a
%% simple criterion: if there are valid values enqueued for mutation, it picks the
%% first one, mutates it and returns it, decreasing its energy by one.
%% %
%% If \textit{QValid} is empty, then the same is attempted using \textit{QDiscarded}.
%% %
%% If none of the mutation queues contain any candidates, CGPT generates a new
%% value from scratch.
%% %
%% This selection process is illustrated in detail in Algorithm
%% \ref{algo:fuzzchick:pick}.


%% Having selected the next test case, the main loop proceeds to execute it,
%% capturing both the result (passed, discarded due to preconditions, or failed)
%% and its execution trace over the system under test.
%% %
%% If the test case fails, it is immediately reported as a bug.
%% %
%% If not, the algorithm evaluates whether it was interesting (i.e., it exercises a
%% new path) based on its trace information and the one from previously executed
%% test cases (represented by \textit{TLog}).
%% %
%% If the test case does in fact discover a new path, it is enqueued at the end of
%% its corresponding queue, depending on whether it passed or was discarded.
%% %
%% %% (i.e., it failed the precondition).
%% %
%% This process alternates between generation and mutation until a bug is found or
%% we reach the test limit.


%% \begin{algorithm}
%%   \SetArgSty{textnormal}
%%   %% \SetInd{0em}{0.75em}
%%   \SetKw{KwNot}{not}
%%   \SetAlgoLined
%%   \DontPrintSemicolon
%%   \SetKwFunction{FC}{Loop}
%%   \SetKwProg{Fn}{Function}{:}{}
%%   \Fn{\FC{P, N, gen, mut}}{
%%     i $\gets$ 0\;
%%     TLog, QValid, QDiscarded $\gets$ $\varnothing$\;
%%     \While{i $<$ N}{
%%       x $\gets$ Pick(QValid, QDiscarded, gen, mut)\;
%%       (result, trace) $\gets$ WithTrace(P(x))\;
%%       \lIf{\KwNot result}{\KwRet Bug(x)}
%%       \If{Interesting(TLog, trace)}{
%%         e $\gets$ Energy(TLog, x, trace)\;
%%         \eIf{\KwNot Discarded(result)}{
%%           Enqueue(QValid, (x, e))\;
%%         }{
%%           Enqueue(QDiscarded, (x, e))\;
%%         }
%%       }
%%       i $\gets$ i+1\;
%%     }
%%     \KwRet Ok\;
%%   }
%% \caption{\label{algo:fuzzchick:loop}CGPT Testing Loop}
%% \end{algorithm}

%% \begin{algorithm}
%%   \SetArgSty{textnormal}
%%   %% \SetInd{0em}{0.75em}
%%   \SetKw{KwNot}{not}
%%   \SetAlgoLined
%%   \DontPrintSemicolon
%%   \SetKwFunction{Pick}{Pick}
%%   \SetKwProg{Fn}{Function}{:}{}
%%   \Fn{\Pick{QValid, QDiscarded, gen, mut}}{
%%     \If{\KwNot Empty(QValid)}{
%%       (x,e) $\gets$ Deque(QValid)\;
%%       \If{e $>$ 0}{
%%         PushFront(QValid, (x, e-1))
%%       }
%%       \KwRet Sample(mut(x))\;
%%     }
%%     \uElseIf{\KwNot Empty(QDiscarded)}{
%%       (x,e) $\gets$ Deque(QDiscarded)\;
%%       \If{e $>$ 0}{
%%         PushFront(QDiscarded, (x, e-1))
%%       }
%%       \KwRet Sample(mut(x))\;
%%     }
%%     \lElse{\KwRet Sample(gen)}
%%   }
%% \caption{\label{algo:fuzzchick:pick}CGPT Seed Selection}
%% \end{algorithm}

%% %% \subsubsection{Type-preserving mutations}

%% \paragraph{Structure-preserving Mutators}

%% %% are no more than specialized random generators, parameterized
%% %% by the original input to be mutated.
%% %% %
%% %% They

%% Mutators in GCPT use a simple set of mutation operations that are randomly
%% applied to their inputs at the data type level.
%% %
%% In simple terms, these operations encompass
%% %
%% \begin{inparaenum}
%% \item \emph{swapping a data constructor}, replacing its outermost data
%%   constructor by another one, e.g., a \texttt{Leaf} by a \texttt{Branch} and
%%   vice-versa, either dropping unused subexpressions or generating new random
%%   ones when the new data constructor contains extra fields that cannot be filled
%%   using existing ones;
%% %% \item \emph{shrinking the value}, replacing its outermost data constructor with
%% %%   one that contains a subset of its fields, reusing existing subexpressions;
%% %% \item \emph{growing the value}, replacing its outermost data constructor with
%% %%   one that contains a superset of its fields, reusing existing subexpressions
%% %%   and generating random ones when needed;
%% \item \emph{returning a subexpression of the same type}, e.g, returning a left
%%   or right subtree when applied to a \texttt{Branch}; and
%% \item \emph{mutating recursively}, applying a mutation operation over an
%%   immediate subexpression.
%% \end{inparaenum}
%% %
%% We will provide more details about structure-preserving mutations in \S
%% \ref{sec:mutagen}.

%
% FuzzChick Limitations
%
\paragraph{Limitations of CGPT}

\citeauthor{lampropoulos2019coverage} demonstrated empirically that CGPT lies
comfortably in the middle ground between using pure random testing with na\"ive
automatically derived random generators and complex manually-written ones.
%
%% Their results suggest that CGPT is an appealing technique for finding bugs while
%% still using a mostly automated workflow.
%
However, the authors acknowledge that certain parts of its implementation have
room for improvement, especially when it comes to the mutator's design.
%
%% Moreover, we replicated the Information-Flow Control (IFC) stack machine case
%
Moreover, we replicated the IFC stack machine case study bundled with \fuzzchick
and observed a surprising limitation:
%
\emph{after repeating each experiment 30 times, \fuzzchick could only find 5 out
  of the 20 injected bugs with a 100\% efficacy, the hardest one being found
  only around 13\% of the time after an hour of testing.}
%
Despite this, the authors show that CGPT outperforms normal random testing
coupled with na\"ive random generators by comparing the mean-time-to-failure
(MTTF) against normal random property-based testing.
%
In turn, we believe that a more meticulous evaluation ought to take failure to find a
counterexample as an important metric when comparing PBT tools, not just mean
time to failure --- when one is found.
%
%% In tandem with the lack of efficacy observed when replicating the evaluation of
%% the IFC stack machine case study,

All these observations led us to consider three main aspects of CGPT that can be
improved upon:
%
%% --- and that constitute the main goal of this work.
%
%% As mentioned earlier, when we recreated the evaluation of the IFC stack machine
%% case study (described in detail in \S \ref{sec:casestudies}), we found that
%% after 30 runs (as opposed to the 5 runs used original by
%% \citeauthor{lampropoulos2019coverage}), \emph{\fuzzchick was only able to find 5
%%   out of the 20 injected bugs with a failure rate of 1}, the hardest one being
%% found only around 13\% of the time after an hour of testing.
%% %
%% These results are presented in detail in \S \ref{sec:evaluation}.
%
%% At the light of these observations, we identified three main aspects of
%% \fuzzchick that can be improved upon --- and that constitute the main goal of
%% this work.
%
%% In no particular order:

\begin{itemize}
\item \emph{Mutators distribution:}
%
for the sake of simplicity, the mutators proposed by
\citeauthor{lampropoulos2019coverage} are derived to follow a top-down approach.
%% similar to that used by the automatically derived random generators.
%
This means that mutations can happen at the top-level or be recursively applied
to an immediate subexpression of the input test case with approximately the same
probability.
%
%%   if we inspect the mutator defined in Fig. \ref{fig:fuzzchick:mutator}, there are
%% two compromises that the authors of \fuzzchick adopted for the sake of
%% simplicity.
%
This makes deep recursive mutations very unlikely, as their probability
decreases multiplicatively with each recursive call.
%
%% For instance, mutating a subexpression that lies on the fourth level of a
%% \texttt{Tree} happens with a probability smaller than $(1/6)^3 = \sim 0.0046$,
%% i.e., when applying a recursive mutation (with probability $1/6$) three
%% consecutive times.
%
%% This only worsens as the type of the mutated value becomes more complex.
%
Hence, these simple mutators can only be effectively used to transform shallow
data structures, excluding applications that might require producing deeper
mutations.
%
%e.g., programs, network protocol interactions, etc.
%
Ideally, \emph{mutations should be able to happen on every subexpression of the
  input test case on a reasonable basis}.


%% On the other hand, using random generators inside mutators to produce new
%% subexpressions when ``growing'' a data constructor can be dangerous, as this
%% introduces the same ``uncontrolled'' randomness we want to mitigate in the first
%% place!
%% %
%% If such generator produces an invalid subexpression, it might invalidate the
%% whole mutated test case.
%
%% Hence, growing data constructors must be done carefully.

%% %
%% For instance, by using just a minimal piece of data to make the overall mutated
%% test case type correct.
%% %
%% If that mutated test case turns to be interesting, that new subexpression can
%% always be mutated later.

\item \emph{Enqueuing mutation candidates:}
%
the CGPT testing loop uses two single-ended queues for keeping valid and
discarded interesting test cases.
%
Whenever a new test case is found interesting, it is placed \emph{at the end of
  its corresponding queue}.
%
If this test case happens to have discovered a whole new portion of the target
code, it will not be further mutated until the rest of the queue ahead of it
gets processed.
%
This can limit the effectiveness of the testing loop if the queues grow more
often than they shrink, as interesting test cases can get ``buried'' at the end
of a long queue and,
%
%% that only exercises the same portion of the target code.
%
in an extreme case, not get processed at all within the testing budget.
%
Ideally, \emph{we need a mechanism that prioritizes mutating test cases that
  discover new portions of the code right away}.
%
%% , and that is capable of jumping
%% back and forth from  whenever this happens.
%
%% We show in \S \ref{sec:heuristics} how this can be achieved by analyzing
%% the execution information in order to prioritize test cases with novel execution
%% traces.

\item \emph{Power schedule:}
%
it is not clear how the heuristic used to assign a given budget to each mutable
test case in CGPT works in the context of high-level structure-preserving
mutations.
%
If it assigns too much budget to certain not-so-interesting test cases, some
bugs might not be discovered on a timely basis.
%
Conversely, assigning too little budget to interesting test cases might mean
that some bugs cannot be discovered at all --- randomly generating the same test
case again later does not help, as it becomes uninteresting based on historic
trace information.

To keep the comparison fair, \citeauthor{lampropoulos2019coverage} replicated
the same power schedule configuration used in AFL, which is tailored to work in
tandem with generic bit-level mutators.
%
It is unclear what the best heuristic configuration is when using a high-level
mutation approach --- something challenging to characterize in general given the
expressivity of the data types used to drive the mutators.
\end{itemize}

The next section introduces \mutagen, our revised CGPT that aims to tackle the
main limitations of CGPT to make it practical in real-world scenarios.

% ----------------------------------------

\section{\mutagen: No Mutant Left Behind}
\label{sec:mutagen}

In this section we describe the main ideas behind \mutagen, our CGPT tool
written in Haskell.
%
Notably, these ideas extend beyond Haskell and functional programming languages
in general.
%
%% Later, \S \ref{sec:heuristics} introduces heuristics that help finding bugs
%% faster in certain testing scenarios.

In contrast to CGPT and many other popular fuzzing tools, \mutagen works by
mutating test cases on an exhaustive and precise manner, where
%
\begin{inparaenum}
  \item each subexpression of a mutable test case is associated with a set of
    structure-preserving mutations, and
  \item each one of these mutations is scheduled \emph{exactly once}.
\end{inparaenum}
%
%% There is a small exception to this rules that we will introduce soon.
%
Our realization is that, by using an exhaustive mutation approach, we avoid
needing a heuristic to assign a mutation budget to each mutated test case.
%
Moreover, producing mutants exhaustively ensures that no interesting mutation is
omitted due to randomness, as well a no mutation is evaluated more than once.
%
This is inspired by exhaustive bounded testing tools like \emph{SmallCheck}
\cite{runciman2008smallcheck} or \emph{Korat} \cite{boyapati2002korat}, that
produce test cases exhaustively --- please refer to \S \ref{sec:related}
for a detailed discussion.


The rest of this section describes our exhaustive mutation mechanism, as well as
the adapted testing loop used in \mutagen.

%% In simple words, such tools work by enumerating all possible values of the data
%% types used in the testing properties, and then executing them from smaller to
%% larger until a bug is found, or a certain size bound is reached.
%% %
%% The main problem with this approach is that the space of all possible test cases
%% often grows exponentially as we increment the size bound, and the user
%% experiences what it looks like ``hitting a wall'', where no larger test cases
%% can be evaluated until we exhausted all the immediately smaller ones
%% \cite{DuregardJW12}.
%% %
%% To alleviate this problem, such tools often rely on heuristics to prune the
%% search space based on detecting unused subexpressions --- this is discussed in
%% \S \ref{sec:related}.

%% In consequence, such tools can only be applied to relatively simple systems,
%% where the space of inputs does not grow extremely fast.

%% Not to be confused by these tools, in \mutagen we do not enumerate all possible
%% test cases exhaustively.
%% %
%% Our tool uses random generators to find interesting initial seeds, and when it
%% finds them, only then proceeds to schedule all possible mutations.
%% %
%% Moreover, as in \fuzzchick, the testing loop of our tool automatically filters
%% the test cases that are worth mutating by keeping only those that discover new
%% execution paths in the code under test.


\subsection{Exhaustive Mutations}

%% Before describing \mutagen's testing loop, we first need to introduce the
%% mechanism used for testing mutations exhaustively.
%% %
%% In contrast to \fuzzchick, where mutators are parameterized random generators,
%% in our tool

Mutations in \mutagen are defined as the set of mutants that can be obtained by
transforming the input test case \emph{at the top-level} (the root).
%
We define mutations for a value of type \texttt{a} (written \texttt{Mutations
  a}) as \emph{a function that takes a value of such a type and returns a list of
mutants} (written \texttt{Mutant a}).
%
In Haskell, we introduce the following type synonym:
%
\begin{verbatim}
type Mutations a = a -> [Mutant a]
\end{verbatim}
%
\noindent Where \texttt{Mutant}s come in two flavours, pure and random:

\begin{verbatim}
data Mutant a = PURE a | RAND (Gen a)
\end{verbatim}

Pure mutants are used most of the time, and encode simple deterministic
transformations over the outermost data constructor of the input --- recursive
mutations will be introduced soon.
%
These transformations can:
%
\begin{inparaenum}
\item return an immediate subexpression of the same type as the input;
\item swap the outermost data constructor with any other constructor of the
  same type, reusing existing subexpressions; and
\item rearrange and replace fields using existing ones of the same type.
\end{inparaenum}
%
To illustrate this, Fig. \ref{fig:mutagen:mutator} outlines a mutator for the
\texttt{Tree} data type.
%
Notably, this definition simply enumerates mutants that transform the outermost
data constructor, hence no recursion is needed here.
%
Moreover, notice how a default value (\texttt{def}) used to fill the subtrees
when transforming a leaf into a branch.
%
This value corresponds to the simplest expression we can construct for the
mutant to be type-correct.
%
In our example, the default value of a \texttt{Tree} is a leaf containing the
smallest value of the tree payload type, e.g., in the case of trees with
integers \texttt{def} is defined as \texttt{Leaf 0}.
%
%% \texttt{Leaf def}, where
%% the inner \texttt{def} is the default value of the payload.%
\footnote{We use the type class system to abstract this complexity away in our
  implementation.}
%
Using a small default value, as oppossed to a randomly generated one (as done by
the original CGPT), is also inspired by exhaustive bounded testing tools, and
avoids introducing unnecessary randomness when ``growing'' mutated expressions.

%% \begin{figure}
%% \begin{verbatim}
%% mutate (Leaf x)       = [ PURE (Branch def x def) ]   -- Swap constructor
%% mutate (Branch l x r) = [ PURE l, PURE r              -- Return subexpression
%%                         , PURE (Leaf x)               -- Swap constructor
%%                         , PURE (Branch l x l)         -- Rearrange subexpressions
%%                         , PURE (Branch r x r)         -- Rearrange subexpressions
%%                         , PURE (Branch r x l) ]       -- Rearrange subexpressions
%% \end{verbatim}
%% \vspace{-5pt}
%% \caption{\label{fig:mutagen:mutator}\mutagen mutator for the \texttt{Tree} data
%%   type.}
%% \vspace{-10pt}
%% \end{figure}

\begin{figure}[b]
\begin{verbatim}
mutate (Leaf x) =
  [ PURE (Branch def x def) ]
mutate (Branch l x r) =
  [ PURE l, PURE r
  , PURE (Leaf x)
  , PURE (Branch l x l), PURE (Branch r x r), PURE (Branch r x l) ]
\end{verbatim}
\vspace{-5pt}
\caption{\label{fig:mutagen:mutator}\mutagen mutator for the \texttt{Tree} data
  type.}
\vspace{-5pt}
\end{figure}

%% At this point, the reader might consider:
%% %
%% what about large enumeration types like integers or characters?
%% %
%% Does \mutagen transform them exaustively too?
%% %
%% \emph{Certainly not.}
%
Random mutants, on the other hand, serve as a way to \emph{selectively avoid
  exhaustiveness} when mutating values of large enumeration types --- or any
other type the user might want to use random mutations with.
%
Instead of mutating every numerical or character subexpression exhaustively, we
sample a small number of values from a given random generator.
%
This amount is a tunable parameter of \mutagen.
%
Then, for instance, a mutator for integers becomes:

\begin{verbatim}
mutate n = [ RAND genInt ]
\end{verbatim}


\paragraph{Mapping top-level mutations everywhere}

So far we have defined mutations that transform only the root of the input.
%
Now it is time to apply these mutations to every subexpression as well.
%
To do so, we use two utility functions that can be automatically synthesized.

In first place, a function \texttt{Positions} traverses the input and builds a
Rose tree of \emph{mutable positions}, where positions are lists of indices
encoding the path from the root to every mutable subexpression.
%
% For instance, the positions of a \texttt{Tree} are computed as follows:
%
% \begin{verbatim}
% positions (Leaf x) =
%   node [ (0, positions x) ]
% positions (Branch l x r) =
%   node [ (0, positions l), (1, positions x), (2, positions r) ]
% \end{verbatim}
%
% \noindent where the function \texttt{node} simply builds a node of the positions
% Rose tree and propagates the accumulated prefix (the path from the root) to its
% children.
%
For instance, the mutable positions of the value \texttt{Branch (Leaf 1) 2
  (Leaf 3)} are as follows:

\vspace{-5pt}
\begin{equation}
  \nonumber
  \footnotesize
  \texttt{Positions}\quad
  \left(
  \tikz[
    baseline=-6mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.5cm}
  ]{
    \node {\texttt{Branch}}
      child {node {\texttt{Leaf}}
        child {node {\texttt{1}}}
      }
      child {node {\texttt{2}}}
      child {node {\texttt{Leaf}}
        child {node {\texttt{3}}}
      };
  }
  \right)
  \quad
  =
  \quad
  \tikz[
    baseline=-6mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.5cm}
  ]{
    \node {\texttt{[]}}
      child {node {\texttt{[0]}}
        child {node {\texttt{[0,0]}}}
      }
      child {node {\texttt{[1]}}}
      child {node {\texttt{[2]}}
        child {node {\texttt{[2,0]}}}
      };
  }
\end{equation}

Later, we define a function \texttt{MutateInside} that takes a desired position
within an input test case and mutates its corresponding subexpression, returning
a list of mutants.
%
%% given the desired position to mutate within a test case, we
%% define another function \texttt{inside} that finds the subexpression
%% corresponding to it and mutates it.
%
% A slightly simplified version of this function for the \texttt{Tree} data type is
% as follows:
%
% \begin{verbatim}
% inside []     t              = mutate t
% inside (0:ps) (Leaf x)       = [ Leaf x'       | x' <- inside ps x ]
% inside (0:ps) (Branch l x r) = [ Branch l' x r | l' <- inside ps l ]
% inside (1:ps) (Branch l x r) = [ Branch l x' r | x' <- inside ps x ]
% inside (2:ps) (Branch l x r) = [ Branch l x r' | r' <- inside ps r ]
% \end{verbatim}
%
This function simply traverses the desired position, calling itself recursively
until it reaches the desired subexpression, where a mutation encoded by \texttt{mutate} can be
applied.
% locally at the top-level (case \texttt{inside [] t}).
%
The rest of the \texttt{MutateInside}'s functionality simply unwraps and rewraps the
intermediate subexpressions and is not particularly relevant for the point being
made.


\subsection{Testing loop}

Having the exhaustive mutation mechanism in place, we can now introduce the base
testing loop used by \mutagen.
%
This is outlined in Algorithm \ref{algo:mutagen:loop}.
%
Like in CGPT, we use two queues, QValid and QDiscarded to keep valid and discarded mutable test cases,
respectively.
%
%% The testing loop enqueues and retrieves interesting test cases from them before
%% falling back to random generation.


\begin{algorithm}[t]
  \SetArgSty{textnormal}
  %% \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{MG}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\MG{P, N, R, gen}}{
    i $\gets$ 0\;
    TLog, QValid, QDiscarded $\gets$ $\varnothing$\;
    \While{i $<$ N}{
      x $\gets$ Pick(QValid, QDiscarded, gen)\;
      (result, trace) $\gets$ WithTrace(P(x))\;
      \lIf{\KwNot result}{\KwRet Bug(x)}
      \If{Interesting(TLog, trace)}{
        \If{\KwNot Discarded(result)}{
          batch $\gets$ CreateMutationBatch(x, R)\;
          Enqueue(QValid, batch)\;
        }
        \uElseIf{\KwNot Discarded(Parent(x))}{
          batch $\gets$ CreateMutationBatch(x, R)\;
          Enqueue(QDiscarded, batch)\;
        }
      }
      i $\gets$ i+1\;
    }
    \KwRet Ok\;
  }
\caption{\label{algo:mutagen:loop}\mutagen Testing Loop}
\end{algorithm}

\begin{algorithm}[t]
  \SetArgSty{textnormal}
  %% \SetInd{1em}{em}
  \SetKw{KwNot}{not}
  \SetKw{KwIn}{in}
  \SetKwFor{KwRepeat}{repeat}{times}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{Muts}{CreateMutationBatch}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Muts{x, R}}{
    batch $\gets$ $\varnothing$\;
    \For{pos \KwIn Flatten(Positions(x))}{
      \For{mutant \KwIn MutateInside(pos, x)}{
        \Switch{mutant}{
          \Case{\texttt{PURE} $\hat{x}$}{
            Enqueue($\hat{x}$, batch)\;
          }
          \Case{\texttt{RAND} gen}{
            \KwRepeat{R}{
              $\hat{x}$ $\gets$ Sample(gen)\;
              Enqueue($\hat{x}$, batch)\;
            }
          }
        }
      }
    }
    \KwRet batch\;
  }
\caption{\label{algo:mutagen:init}Mutation Batch Initialization}
\end{algorithm}



In \mutagen, we precompute all the mutations of a given mutable test case before
enqueueing them.
%
These mutations are put together into lists that we call \emph{mutation batches}
--- one for each mutated test case.
%
To initialize a mutation batch (outlined in Algorithm \ref{algo:mutagen:init}),
we first flatten all the mutable positions of the input test case in level order
(recall that positions are stored as a Rose tree).
%
Then, we iterate over all of them and retrieve all the mutants defined for each
subexpression.
%
For each one of these, there are two possible cases:
%
\begin{inparaenum}
\item if it is a pure mutant carrying a concrete mutated value, we enqueue it
  into the mutation batch directly; otherwise
\item it is a random mutant that carries a random generator with it (e.g.,
  corresponding to a numeric subexpression), in which case we sample and enqueue
  \textit{R} random values using this generator, where \textit{R} is a parameter
  set by the user.
\end{inparaenum}
%
At the end, we simply return the accumulated batch.


Finally, the seed selection algorithm (Algorithm \ref{algo:mutagen:pick}) picks
the next test case using the same criteria as CGPT, prioritizing valid test
cases over discarded ones, falling back to random generation when both queues
are empty.
%
Since mutations are precomputed, this function simply picks the next test case
from the current batch, until it becomes empty and can switch to the next
precomputed one in line.


Having selected the next test case, the testing loop proceeds to execute it,
capturing both the result (valid, discarded, or failed) and its execution trace
over the system under test.
%
If the test case fails, it is reported as a bug.
%
If not, the algorithm evaluates whether it was interesting (i.e., it exercises a
new path) based on its trace information and the one from previously executed
test cases (represented by TLog).
%
If the test case discovers a new path, it is enqueued on its corresponding
queue.
% depending on whether it was valid or got discarded.
%
%% (i.e., it failed the precondition).
%
This process alternates between generation and mutation until it finds a bug or
reaches the test limit N.


\begin{algorithm}[t]
  \SetArgSty{textnormal}
  %% \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{Pick}{Pick}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Pick{QValid, QDiscarded, gen}}{
    \If{\KwNot Empty(QValid)}{
      batch $\gets$ Deque(QValid)\;
      \lIf{Empty(batch)}{ Pick(QValid, QDiscarded, gen) }
      \uElse{
        PushFront(QValid, Rest(batch))\;
        \KwRet First(batch)\;
      }
    }
    \If{\KwNot Empty(QDiscarded)}{
      batch $\gets$ Deque(QDiscarded)\;
      \lIf{Empty(batch)}{ Pick(QValid, QDiscarded, gen) }
      \uElse{
        PushFront(QDiscarded, Rest(batch))\;
        \KwRet First(batch)\;
      }
    }
    \lElse{\KwRet Sample(gen)}
  }
  \caption{\label{algo:mutagen:pick}\mutagen Seed Selection}
\end{algorithm}

Another difference with CGPT's testing loop is the criterion for enqueuing
discarded tests.
%
We found that, especially for large data types, the queue of discarded
candidates tends to grow disproportionately large during testing, making them
hardly usable and consuming large amounts of memory.
%
To improve this, we resort to mutate discarded tests cases only when we have
some evidence that they are ``almost valid.''
%
For this, each mutated test case remembers whether its parent (the original test
case they derive from before being mutated) was valid.
%
Then, we enqueue discarded test cases only if they meet this condition.
%
As a result, we fill the discarded queue with lesser but much more interesting
mutable test cases.
%% %
%% Moreover, this can potentially lead to \emph{2-step mutations}, where an initial
%% mutation breaks a valid test case in a small way, it gets enqueued as discarded,
%% and later a subsequent mutation fixes it by changing a different subexpression.


The next section describes two heuristics we added to \mutagen's testing loop
based on the limitations we found in CGPT.
%
%% These heuristics can that offer a substantial improvement in practice.

% ----------------------------------------


\section{\mutagen Heuristics}
\label{sec:heuristics}


In this section we introduce two heuristics implemented on top of the base
testing loop of our tool.
%% %
%% \mutagen enables them by default, and can be individually disabled by the user
%% if needed.

\subsection{Priority LIFO Scheduling}

This heuristic first tackles the issue of enqueuing new interesting mutation
candidates at the end of (possibly) long queues of not-so-interesting previously
executed ones.

%% \fuzzchick uses AFL instrumentation under the hood, which in turn uses an
%% \emph{edge coverage} criterion to distinguish novel executions and to assign
%% each mutation candidate a given energy.
%% %
Execution traces in \mutagen represent the specific \emph{path} in the code
taken by the program, as opposed to just the (unordered) set of edges traversed
in the control-flow graph (CFG) used by default by most coverage-guided fuzzers.
%
Using this criterion lets us gather precise information from each new execution.
%
%% To keep track of the execution path of each test, our tool keeps an internal
%% prefix tree that records every new entry by traversing this data structure and
%% inserting new nodes after a new path prefix gets discovered.
%% %
In particular, we are interested in the \emph{depth} where each new execution
branches from already seen ones.
%
Our assumption is that test case executions that branch at shallower depths from
the ones already executed are more likely to discover new portions of the code
under test, and hence we want to prioritize them.

\begin{figure}[b]
%% \vspace{-15pt}
\scriptsize
\newcommand\stacked[2]{\underset{\textstyle #2\mathstrut}{#1}}
\begin{align*}
  \nonumber
  %% \qquad\qquad\qquad\qquad
  \left[
  \tikz[
    baseline=-8mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.5cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{4}}}
        }
      }
  }
  \right]
  \;
  +
  \;
  \tikz[
    baseline=-8mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.5cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{5}}}
        }
      }
  }
  \;
  =
  \;
  \left[
  \tikz[
    baseline=-8mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.5cm}
  ]{
    \node {\color{red} \small \texttt{1}}
      child {node {\color{red} \small \texttt{2}}
        child {node {\color{red} \small \texttt{3}}
          child {node {\small \texttt{4}}}
          child {node {\color{blue} \small \texttt{5}}}
        }
      }
  }
  \right]
  %
  \stacked{{\color{blue} new=1\phantom{x}}}{{\color{red} depth=3}}
  %% \end{equation}
  %% \begin{equation}
  %% \nonumber
  %% \\
  %% \qquad
  \quad
  \left[
  \tikz[
    baseline=-8mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.5cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{4}}}
          child {node {\small \texttt{5}}}
        }
      }
  }
  \right]
  \;
  +
  \;
  \tikz[
    baseline=-8mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.5cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{6}}
          child {node {\small \texttt{7}}
          }
        }
      }
  }
  \;
  =
  \;
  \left[
  \tikz[
    baseline=-8mm,
    level distance=0.5cm,
    level 1/.style={sibling distance=0.5cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\color{red} \small \texttt{1}}
      child {node {\color{red} \small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{4}}}
          child {node {\small \texttt{5}}}
        }
        child {node {\color{blue}\small \texttt{6}}
          child {node {\color{blue}\small \texttt{7}}}
        }
      }
  }
  \right]
  %
  \stacked{{\color{blue} new=2\phantom{x}}}{{\color{red} depth=2}}
\end{align*}
\vspace{-10pt}
\caption{\label{fig:mutagen:tracelog}Inserting two new execution traces into the
  internal execution trace log (represented using brackets).}
\end{figure}

\begin{algorithm}[t]
  \SetKwComment{Comment}{/* }{}
  \SetArgSty{textnormal}
  %% \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{MG}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\MG{P, N, R, gen}}{
    $\cdots$\;
    x $\gets$ Pick(QValid, QDiscarded, gen)\;
    (result, trace) $\gets$ WithTrace(P(x))\;
    $\cdots$\;
    \If{Interesting(TLog, trace)}{
      \If{\KwNot Discarded(result)}{
        batch $\gets$ CreateMutationBatch(x, R)\;
        {\color{red} prio $\gets$ BranchDepth(TLog, trace)}\; %% NEW
        {\color{red} PushFront(QValid, prio, batch)}\;          %% NEW
      }
      $\cdots$\;
    }
  }
  %% \vspace{5pt}
  \SetKwFunction{Pick}{Pick}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Pick{QValid, QDiscarded, gen}}{
    \If{\KwNot Empty(QValid)}{
      {\color{red} (batch, prio) $\gets$ DequeMin(QValid)}\; %%NEW
      \lIf{Empty(batch)}{ Pick(QValid, QDiscarded, gen) }
      \uElse{
        {\color{red} PushFront(QValid, prio, Rest(batch))}\; %% NEW
        \KwRet First(batch)\;
      }
    }
    \If{\KwNot Empty(QDiscarded)}{\Comment {Analogous to the case above */} }
    $\cdots$\;
  }

\caption{\label{algo:mutagen:lifo}Priority LIFO Heuristic}
\end{algorithm}


In this light, every time we insert a new execution path into the internal trace
log (TLog), we calculate the number of \emph{new nodes} that were executed, as
well as the \emph{branching depth} where they got inserted.
%
The former is used to distinguish interesting test cases (whether or not new
nodes were inserted), whereas the latter is used by this heuristic to schedule
mutation candidates.
%
Fig. \ref{fig:mutagen:tracelog} illustrates this idea, inserting two execution
traces (one after another) into a trace log that initially contains a single
execution path.
%
The second insertion (with trace $1 \rightarrow 2 \rightarrow 6 \rightarrow 7$)
branches at a shallower depth than the first one (2 vs. 3), and so its
corresponding test case is given a higher priority.


Using this mechanism, we can modify \mutagen's base testing loop by replacing
each mutation queue with a priority queue indexed by the branching depth of each
new execution.
%
These changes are illustrated in Algorithm \ref{algo:mutagen:lifo}.
%
Statements in {\color{red} red} indicate important changes to the base
algorithm, whereas ellipses denote parts of the code that are not relevant for
the point being made.

To pick the next test case, we retrieve the one with the highest priority (the
smallest branching depth).
%
Then, when we find a new interesting test case, it gets enqueued at the
\emph{beginning} of the queue at its corresponding priority.
%
This allows the testing loop to jump immediately onto mutating new interesting
test cases as soon as they are found (even at the same priority), and to jump
back to previous test cases as soon as their mutants become less novel.

%% \caption{\label{fig:heuristics} The two main heuristics implemented in \mutagen.
%%   Statements in {\color{red} red} indicate important changes to the base
%%   algorithm. Ellipses denote parts of the code that are not relevant for the
%%   point being made. }



\subsection{Detecting Trace Space Saturation And Tuning Random Mutations Parameter}

As introduced in \S \ref{sec:mutagen}, our tool is parameterized by the
number of random mutations to be generated over each mutable subexpression
defined using a random mutant, e.g., numeric values.
%
But, how many random mutations should we use? A single one? A few tenths? A few
hundred?
%% %
%% Using too little can put finding bugs at risk.
%% %
%% For instance, when the system under test branches based on numeric values, we
%% should make sure that we set enough random mutations to test each branch on a
%% reasonable basis.
%% %
%% Using too many, on the other hand, can degrade the performance of the testing
%% loop, as it will spend too much time producing uninteresting mutations.
%% %
%% This can happen for instance if the subexpressions defined using random mutants
%% are only used as payloads, and their value does not affect the execution in any
%% way.
%
Answering this question precisely is not an easy task, and this second heuristic
aims to tackle this issue.

We found that, the smaller the number of random mutations we set, the easier it
is for the trace log that records executions to start getting saturated, i.e.,
when interesting test cases stop getting discovered or are discovered very
sporadically.
%
We realized that we can use this information to automatically optimize the
number of random mutations used by our tool.
%
This idea is described in Algorithm \ref{algo:mutagen:reset}.
%
The process is simple:
%
\begin{inparaenum}
  \item we start the testing loop with the number of random mutations set to
    one, and
  %
  \item each time we find that a test is not interesting (i.e. boring), we
    increment a counter.
  %
  Then,
  \item if we have not produced any interesting test case after a certain
    threshold (1000 tests seems to be a reasonable value in practice), we
    increment the number of random mutations and the threshold by twice the
    current amount.
    %
    Additionally, each time this happens we also reset the trace log, so
    interesting test cases found on a previous iteration can be found and
    enqueued for mutation again --- this time with a higher effort dedicated to
    producing random mutations.
\end{inparaenum}

Notably, this heuristic can be particularly useful when the execution of the
system under test depends heavily on some invariant encoded by a subexpression
of its input that get transformed using a random mutant (e.g, the number of
pixels declared by the header of an image matching the size of its actual
payload).
%
There, starting with a single random mutation will quickly saturate the trace
space with discarded (invalid) tests, and this heuristic will continuously
increase the random mutations parameter until the random mutant is given enough
chances to randomly generate the value that satisfies the required invariant and
makes the overall test case valid.

\begin{algorithm}[t]
  \SetArgSty{textnormal}
  %% \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{MG}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\MG{P, N, gen}}{
    {\color{red} boring $\gets$ 0}; {\color{red} reset $\gets$ 1000}; {\color{red}R $\gets$ 1}\;
    $\cdots$\;
    \While{i $<$ N}{
      {\color{red}
      \If{boring $>$ reset}{
        TLog   $\gets$ $\varnothing$\;
        reset $\gets$ reset * 2\;
        R     $\gets$ R * 2\;
      }
      }
      $\cdots$\;
      \lIf{\KwNot result}{\KwRet Bug(x)}
      \If{Interesting(TLog, trace)}{
        {\color{red} boring  $\gets$ 0}\;
        $\cdots$\;
      }
      \lElse {
        {\color{red} boring $\gets$ boring + 1}
      }
      $\cdots$\;
    }
  }

\caption{\label{algo:mutagen:reset}Trace Saturation Heuristic}
\end{algorithm}


%% \subsection{Mutation Inheritance}

%% The last heuristic we discuss in this work attempts to reduce the amount of
%% mutations required when testing properties that lazily consume foldable inputs
%% like lists or trees.
%% %
%% To illustrate this, suppose we define a property that aims to test a list
%% sorting algorithm:


% ----------------------------------------

%% \vspace{-10pt}
\section{Case studies}
\label{sec:casestudies}

We evaluated the performance of \mutagen using two case studies.
%
The first one is a simple abstract stack machine that enforces the hyperproperty
\emph{noninterference} \cite{goguen1982security} using runtime checks.
%
Its implementation was proven correct by \citeauthor{10.1145/2578855.2535839}
\citeyearpar{10.1145/2578855.2535839} in Coq, and subsequently degraded by
systematically introducing 20 bugs on its enforcing mechanism.
%
\citeauthor{lampropoulos2019coverage} used this case study to compare \fuzzchick
against RPBT using na\"ive- and hand-tuned random generators.
%
Here, we replicate their results and compare them against our tool.
%
Worth mentioning, we reimplemented this case study from scratch in Haskell in
order to run \mutagen on its test suite.


The second case study aims to evaluate \mutagen in a realistic scenario, and
targets \textit{haskell-wasm} \cite{haskellwasm}, an existing WebAssembly engine
of industrial strength.
%
%
Sadly, comparing \mutagen with \fuzzchick on this case study has been out of the
scope of this work, since porting \textit{haskell-wasm} into Coq, and later
adapting our test suite to work with \fuzzchick requires a subtantial effort.
%
Instead, we compare \mutagen against a traditional RPBT approach, evaluating its
effectiveness versus the relative overhead of the code instrumentation used by
our tool.




\subsection{IFC Stack Machine}

This abstract stack machine consists of four main elements: a program counter, a
stack, and data- and instruction memories.
%
Moreover, every runtime value is labeled with a security level, i.e., L (for
``low'' or public) or H (for ``high'' or secret).
%
Labels form a trivial 2-lattice where information can either stay at the same
level, or public information ``can flow'' to secret one but not the opposite
(represented using the familiar $\sqsubseteq$ binary operator).
%
Security labels are propagated throughout the execution of the program whenever
the machine executes an instruction.
%
These instructions are defined as:
%
\begin{verbatim}
data Instr = Nop | Push Int | Call Int | Ret | Add | Load | Store | Halt
\end{verbatim}

\noindent Control flow is achieved using the \texttt{Call} and \texttt{Ret}
instructions, which let the program jump back and forth within the instruction
memory using specially labeled values in the stack representing memory
addresses.
%
The argument in the \texttt{Push} instruction represents a value to be
inserted in the stack, whereas the argument of the \texttt{Call} instruction
encodes the number of elements of the stack to be treated as arguments.
%
Then, programs are simply modeled as sequences of instructions.
%
Finally, machine states are modeled using a 4-tuple \textit{(pc, stk, m, i)}
representing a particular configuration of the program counter, the stack, and
the data- and instruction memories, respectively.
%
To preserve space, we encourage the reader to refer to the work of
\citeauthor{hritcu2013testing} \citeyearpar{hritcu2013testing,
  hrictcu2016testing} and \citeauthor{lampropoulos2019coverage} for more details
about the implementation and semantics of this case study.

\paragraph{Single-Step Noninterference (SSNI)}

This abstract machine is designed to enforce noninterference, which is based on
the notion of \emph{indistinguishability}.
%
Intuitively, two machine states are indistinguishable if they only differ on
secret data.
%
Using this notion, the variant of noninterference we are interested in is called
\emph{single-step noninterference} \cite{hritcu2013testing} (SSNI).
%
Given two indistinguishable machine states, SSNI asserts that running a single
instruction on both machines brings them to resulting states that are also
indistinguishable.


The tricky part about testing this property is to satisfy its sparse
precondition: we need to generate two valid indistinguishable machine states in
order to even proceed to execute the next instruction.
%
\citeauthor{lampropoulos2019coverage} demonstrated that generating two
independent machine states using \quickcheck has virtually no chance of
producing valid indistinguishable ones.
%
However, using the mutation mechanism, we can obtain a pair of valid
indistinguishable machine states by generating a single valid machine state
(something still hard but much easier than before), and then producing a similar
mutated copy.
%
This way, we have a higher chance of producing two almost identical states that
pass the sparse precondition.

\paragraph{IFC Rules}

The IFC rules enforced by this machine are encoded in a table indexed by its
different instructions.
%
This table contains: the dynamic check that the machine needs to perform to
enforce the IFC policy, along with the corresponding security label of the
program counter and the instruction result after the instruction is executed.
%
For instance, to execute the \texttt{Store} instruction (which stores a value in
a memory pointer) the machine checks that both the labels of the program counter
and the pointer together can flow to the label of the destination memory cell.
%
If this condition is not met, the machine then halts as indication of a
violation in the IFC policy.
%
After this check, the machine overwrites the value at the destination cell and
updates its label with the maximum sensibility of the involved labels.
%
In the rule table, this looks as follows:

%% \vspace{-10pt}
\begin{center}
\scriptsize
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Instruction}
& \textbf{Precondition Check}
& \textbf{Final PC Label}
& \textbf{Final Result Label} \\
\hline
\texttt{Store}
& $l_{pc} \vee l_{p} \sqsubseteq l_{v}$
& $l_{pc}$
& $l_{v'} \vee l_{pc} \vee l_{p}$ \\
\hline
\end{tabular}
\end{center}

\noindent Where $l_{pc}$, $l_{p}$, $l_{v}$, and $l_{v'}$ represent the labels
of: the program counter, the memory pointer, and the old and new values stored in
that memory cell.
%
The symbol $\vee$ simply denotes the join of two labels, i.e., the
\emph{maximum} of their sensibilities.
%
Later, bugs are systematically introduced in the IFC enforcing mechanism by
removing or weakening the checks stored in this rule table.



\subsection{WebAssembly Engine}

WebAssembly \cite{haas2017bringing} is an assembly-like language designed for
executing low-level code in the web.
%% %
%% \footnote{Although it has become increasingly popular in standalone, non-web
%%   contexts as well.}
%
WebAssembly programs are first validated
%
%(using a simple form of type-checking)
%
and later executed in a sandboxed environment.
%
%% , making this language an attractive target for virtualization in
%% \emph{functions-as-a-service} platforms.
%
The language is relatively simple, in essence
%
\begin{inparaenum}
  \item it contains only four numerical types, representing both integers and
    IEEE754 floating-point numbers of either 32 or 64 bits;
  \item values of these types are manipulated by functions written using
    sequences of stack instructions;
  \item functions are organized in modules and must be explicitly imported and
    exported;
  \item memory blocks can be imported, exported and grown dynamically;
\end{inparaenum}
%
among others.
%
%% To give the reader a taste, Fig. \ref{fig:wasm:factorial} the WebAssembly text
%% representation of module that exports a recursive implementation of the
%% factorial function.
%
WebAssembly semantics are fully specified, and programs must be consistently
interpreted across engines --- despite some subtle details that we will address
soon.
%
For this, the WebAssembly standard provides a reference implementation with all
the functionality expected from a compliant engine.
%
%% \begin{wrapfigure}{r}{0.35\textwidth}
%% \vspace{-10pt}
%% \begin{verbatim}
%% (module
%% (func $f (param f64) (result f64)
%%   get_local 0
%%   f64.const 1
%%   f64.lt
%%   if (result f64)
%%     f64.const 1
%%   else
%%     get_local 0
%%     get_local 0
%%     f64.const 1
%%     f64.sub
%%     call $fact
%%     f64.mul
%%   end)
%% (export "fact" (func $f)))
%% \end{verbatim}
%% \caption{\label{fig:wasm:factorial}Factorial in WebAssembly}
%% \vspace{-10pt}
%% \end{wrapfigure}

\begin{table}[b]
\scriptsize
\begin{tabular}{|c|c|c|l|}
\hline
\textbf{Id}
& \textbf{Subsystem}
& \textbf{Category}
& \textbf{Description} \\
\hline
1
& Validator
& Bug
& Invalid memory alignment validation \\
\hline
2
& Validator
& Discrepancy
& Validator accepts blocks returning multiple values  \\
\hline
3
& Interpreter
& Bug
& Instance function invoker silently ignores arity mismatch \\
\hline
4
& Interpreter
& Bug
& Allowed out-of-bounds memory access \\
\hline
5
& Interpreter
& Discrepancy
& NaN reinterpretation does not follow reference \\
\hline
\end{tabular}
\caption{\label{table:wasm:bugs}Issues found by \mutagen in
  \textit{haskell-wasm}.}
\vspace{-10pt}
\end{table}

\begin{table}[b]
\scriptsize
\begin{tabular}{|c|c|l|}
\hline
\textbf{Id}
& \textbf{Subsystem}
& \textbf{Description} \\
\hline
1
& Validator
& Wrong if-then-else type validation on else branch \\
\hline
2
& Validator
& Wrong stack type validation \\
\hline
3
& Validator
& Removed function type mismatch assertion \\
\hline
4
& Validator
& Removed max memory instances assertion \\
\hline
5
& Validator
& Removed function index out-of-range assertion \\
\hline
6
& Validator
& Wrong type validation on \texttt{i64.eqz} instruction \\
\hline
7
& Validator
& Wrong type validation on \texttt{i32} binary operations \\
\hline
8
& Validator
& Removed memory index out-of-range assertion \\
\hline
9
& Validator
& Wrong type validation on \texttt{i64} constants \\
\hline
10
& Validator
& Removed alignment validation on \texttt{i32.load} instruction \\
\hline
11
& Interpreter
& Wrong interpretation of \texttt{i32.sub} instruction \\
\hline
12
& Interpreter
& Wrong interpretation of \texttt{i32.lt\_u} instruction \\
\hline
13
& Interpreter
& Wrong interpretation of \texttt{i32.shr\_u} instruction \\
\hline
14
& Interpreter
& Wrong local variable initialization \\
\hline
15
& Interpreter
& Wrong memory address casting on \texttt{i32.load8\_s} instruction \\
\hline
\end{tabular}
\caption{\label{table:wasm:injectedbugs}Bugs injected into
  \textit{haskell-wasm}.}
\vspace{-30pt}
\end{table}

Our tool is an attractive match for testing WebAssembly engines:
%
%% and \textit{haskell-wasm} in particular,
%
most of the programs that can be represented using WebAssembly's AST are
invalid, and automatically derived random generators cannot satisfy the
invariants required to produce interesting test cases.


In this work, we are interested in using \mutagen to test the two most complex
subsystems of \textit{haskell-wasm}: the \emph{validator} and the
\emph{interpreter} --- both being previously tested using a unit test suite.


We avoided spending countless hours writing an extensive property-based
specification to mimic the WebAssembly specification.
%
Instead, we used the reference implementation to find discrepancies (that could
potentially lead to bugs) via differential testing
\cite{mckeeman1998differential}.
%
In this manner, our testing properties assert that any result produced by
\textit{haskell-wasm} matches that of the reference implementation.

Notably, this engine had several latent bugs that were not caught by the
existing unit tests and that we discovered by \mutagen while developing our test
suite.
%
Moreover, \mutagen exposed two other discrepancies between \textit{haskell-wasm}
and the reference implementation.
%
These discrepancies trigger parts of the specification that are either not yet
supported by the reference implementation (multi-value blocks), or that produce
a well-known non-deterministic undefined behavior (NaN reinterpretation)
\cite{perenyi2020stack}.
%
These findings are briefly outlined in Table \ref{table:wasm:bugs}.
%
All of them were reported and later confirmed by the authors of
\textit{haskell-wasm}.
%
Having sorted these issues out, we injected 10 new bugs in the validator as well
as 5 new bugs in the interpreter of this engine (see Table
\ref{table:wasm:injectedbugs}).
%
These bugs were mechanically injected by either
%
\begin{inparaenum}
\item removing an existing integrity check (to weaken the WebAssembly
  type-system/validator); or by
\item simulating a copy-and-paste induced bug,%
  \footnote{This kind of bugs was inspired by the real bug \#1 we found prior
    this step.}
  %
  replacing the implementation of an instruction with a compatible one
  (e.g., \texttt{i32.add} by \texttt{i32.sub}).
\end{inparaenum}


%% During this process, we quickly discovered 3 bugs and 2 discrepancies on this
%% engine with respect to the reference implementation.
%% %


%% %
%% Here, we avoided spending countless hours writing an extensive property-based
%% specification to mimic the reference WebAssembly specification.
%% %
%% Instead, we take advantage of the readily available reference implementation via
%% differential testing.
%% %
%% In this light, our testing properties assert that any result produced by
%% \textit{haskell-wasm} matches that of the reference implementation.

%% The rest of this section introduces the test suite we used to test the different
%% parts of \textit{haskell-wasm}.

\paragraph{Testing the WebAssembly Validator}

%% We begin by designing a property to test the WebAssembly validator implemented
%% in \textit{haskell-wasm}.
%% %
To keep things simple, we assert that, whenever a randomly generated (or
mutated) WebAssembly module is valid according to \textit{haskell-wasm}, then
the reference implementation agrees upon it.
%
%% In other words, we are testing for false negatives.
%
In Haskell, we write the property:

\begin{verbatim}
prop_validator m = isValidHaskellWasm m ==> isValidRefImpl m
\end{verbatim}

\noindent The precondition (\texttt{isValidHaskellWasm m}) runs the input
WebAssembly module \texttt{m} against \textit{haskell-wasm}'s validator, whereas
the postcondition (\texttt{isValidRefImpl m}) serializes \texttt{m}, runs it
against the reference implementation and checks that no errors are produced.

We note that, although we only focus on finding false negatives, a comprehensive
test suite should also test for false positives, i.e., when a module is valid
and \textit{haskell-wasm} rejects it.
%
%% This can be easily done by inverting the direction of the implication
%% (\texttt{<==}) in the property above.
%
%% However, the resulting property is much slower, as every test case will always
%% be serialized and run against the reference implementation.


%% \begin{verbatim}
%% isValidHaskellWasm m =
%%   case validate m of
%%     Left  validationError -> return False
%%     Right validModule     -> return True
%% \end{verbatim}

%% \begin{verbatim}
%% isValidRefImpl m = do
%%   writeFile "testcase.wasm" (dumpModule m)
%%   res <- shell "./wasm" ["-d", "-i", "testcase.wasm"]
%%   return (res == "")
%% \end{verbatim}

\paragraph{Testing the WebAssembly Interpreter}

Testing the WebAssembly interpreter is substantially more complicated than
testing the validator, since it requires actually running and comparing the
output of test case programs.
%
To achieve this, the generated test cases need to comply a with a common
interface that can be invoked both by \textit{haskell-wasm} and the reference
implementation.
%
To simplify things, we write a helper function \texttt{mk\_module} to build a
stub WebAssembly module that initializes one memory block and exports a single
function.
%
This helper is parameterized by the definition of the module's single function,
along with its type signature and name.
%
%% In Haskell:
%
%% \begin{verbatim}
%% mk_module ty name fun =
%%   emptyModule { types     = [ ty ]
%%               , functions = [ fun ]
%%               , exports   = [ Export name (ExportFunc 0) ]
%%               , mems      = [ Memory (Limit 1 Nothing) ] }
%% \end{verbatim}
%
Then, we can use \texttt{mk\_module} to define a testing property parameterized
by a function type, along with its definition and invocation arguments:

%% define a testing property parameterized by a function type, a
%% function body and invocation arguments:

%% function type signature, the function implementation, and a list of invocation
%% arguments:

\begin{verbatim}
prop_interpreter ty fun args =
  do let m = mk_module fun ty "f"
     resHs   <- invokeHaskellWasm m "f" args
     resSpec <- invokeRefImpl     m "f" args
     return (equivalent resHs resSpec)
\end{verbatim}

This testing property instantiates a module stub \texttt{m} with the input
WebAssembly function (\texttt{fun}) and its type signature (\texttt{ty}).
%
Then, it invokes the function \texttt{f} of the module \texttt{m} both on
\textit{haskell-wasm} and the reference interpreter with the provided arguments
(\texttt{args}).
%
Finally, the property asserts whether their results are equivalent.%
%
\footnote{In our implementation, we additionally set a short timeout to discard
  potentially diverging programs with infinite loops.}
%
Interestingly, equivalence in does not imply equality.
%
Non-deterministic operations in WebAssembly like NaN reinterpretations can
produce different equivalent results (as exposed by the discrepancy \#5 in Table
\ref{table:wasm:bugs}), and our equivalence relation needs to take that into
account.
%


Using this testing property directly might not sound like a great idea, as
randomly generated lists of input arguments will be very unlikely to match the
type signature of randomly generated functions.
%
However, it lets us test what happens when programs are not properly invoked,
and it quickly discovered the previously unknown bug \#3 in
\textit{haskell-wasm} mentioned above.
%
Having fixed this issue, we define a more useful specialized version of
\texttt{prop\_interpreter} that fixes the type of the generated function to take
two arguments (of type \texttt{I32} and \texttt{F32}) and return an \texttt{I32}
as a result:

\begin{verbatim}
prop_interpreter_i32 fun i f = prop_interpreter
  (FuncType { params = [I32, F32], result = [I32] }) fun [VI32 i, VF32 f]
\end{verbatim}

\noindent This property lets us generate functions with a fixed type and invoke
them with the exact number and type of arguments required.
%
%% In our experiments (presented in \S \ref{sec:evaluation}),
%
We use this property when finding all the injected bugs into the
\textit{haskell-wasm} interpreter in the next section.
%
%% As before, we remark that a complete test suite should at least include
%% different variants of this property testing functions of several different
%% types, as well as properties testing multiple functions simultaneously.

%% \subsubsection{Real Bugs and Discrepancies found by \mutagen in \textit{haskell-wasm}}

%% \paragraph{Bug \#1: Invalid Memory Alignment Validation}
%% \paragraph{Bug \#2: Lax invocation checks}
%% \paragraph{Bug \#3: Allowed Out-of-bounds Memory Access}
%% \paragraph{Discrepancy \#1: Different Reinterpretation Semantics of NaN Values}
%% \paragraph{Discrepancy \#2: Allowed Blocks with Multiple Return Types}

% ----------------------------------------

\section{Evaluation}
\label{sec:evaluation}

We performed all experiments in a dedicated workstation with 32GB of RAM and an
Intel Core i7-8700 CPU running at 3.20GHz.
%
We repeated each experiment 30 times except for the ones testing the WebAssembly
interpreter, which were run 10 times due to time constraints.
%
From there, we followed the approach taken by
\citeauthor{lampropoulos2019coverage} and collected the mean-time-to-failure
(MTTF) of each bug, i.e., how quickly a bug can be found in wall clock time.
%
We used a one-hour timeout to stop the execution of both tools if they have not
yet found a counterexample.
%
In addition, we collected the failure rate (FR) observed for each bug, i.e. the
proportion of times each tool finds each bug within the one-hour testing budget.
%
%% We found this metric crucial to be analyzed when replicating \fuzzchick's
%% results, as opposed to just paying attention at the MTTF.

In both case studies, we show how the two \mutagen heuristics described in \S
\ref{sec:heuristics} affect the testing performance by individually disabling
them.
%
We call these variants \textit{no LIFO} and \textit{no reset}, respectively.
%
In the case of \textit{no reset}, the amount of random mutations is no longer
controlled by this heuristic, so we arbitrarily fixed it to 25 random mutations
throughout the execution of testing loop.

\subsection{IFC Stack Machine}

\begin{figure}[b]
  \centering
  \includegraphics[width=\linewidth]{tikz/ifc-mttf.tikz}
  \includegraphics[width=\linewidth]{tikz/ifc-fr.tikz}
  \vspace{-20pt}
  \caption{\label{fig:results:ifc} Comparison between \fuzzchick and \mutagen
    across 20 different bugs for the IFC stack machine. }
\vspace{-5pt}
\end{figure}


The results of this case study are shown in Fig. \ref{fig:results:ifc}.
%
%% --- we encourage the reader to obtain a color copy of this work.
%
The injected bugs are ordered by the failure rate achieved by \fuzzchick in
decreasing order.
%
Moreover, notice the logarithmic scale used on the MTTF.



As introduced earlier, \fuzzchick can only find 5 out of the 20 bugs injected
with a 100\% success rate within the one hour testing budget.
%
In turn, our tool manages to find every bug on all runs, and taking less than a
minute in the worst absolute case.
%
While these results are somewhat pessimistic towards \fuzzchick if compared to
the ones presented originally by \citeauthor{lampropoulos2019coverage}, we
remark that our experiments encompass 30 independent runs instead of the 5 used
in the original study.
%
Moreover, since the MTTF simply aggregates all runs, regardless of if they found
a bug or timed out, this metric is sensitive to the timeout used on each
experiment, and will inflate the results as soon as the failure rate goes below
1.
%
For this reason, comparing the failure rate between tools gives a better
estimation of their reliability, where \mutagen shows a clear improvement.

%% If we only consider the succesfull runs where \fuzzchick does not time out, we
After carefully analyzing the results obtained using \fuzzchick, we observed a
peculiarity.
%
This tool either finds bugs quickly (after a few thousands tests) or does not
find them at all within the one-hour budget.
%% , suggesting that its power
%% scheduler assigns some enqueued test cases too little energy before they become
%% uninteresting and get ultimately thrown away.
%
With this in mind, it is fair to think that \fuzzchick might be a better choice
if we set a shorter timeout and run it several times in a row.
%
Thus, to be an improvement over \fuzzchick, our tool must find bugs not only
more reliably, but also fast!

\setlength{\intextsep}{10pt}%
\setlength{\columnsep}{10pt}%
\begin{wrapfigure}{r}{0.5\linewidth}
%% \begin{wrapfigure}{r}{0.5\textwidth}
%% \centering
\vspace{-10pt}
\scriptsize

\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1}

%% \begin{minipage}{0.5\linewidth}
\begin{tabular}{ccccc}
\hline
\multicolumn{1}{c}{
  \multirow{2}{*}{Bug}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\fuzzchick}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\mutagen}} &
  \multicolumn{2}{c}{$A_{12}$ measure} \\
\cline{4-5}
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{Value} &
\multicolumn{1}{c}{Estimate} \\
\hline
1  & 13974.9 & 3632.7 & 0.89 & {\color{blue} large} \\ \hline
2  & 23962.9 & 7122.3 & 0.90 & {\color{blue} large} \\ \hline
3  & 19678.5 & 4633.9 & 0.89 & {\color{blue} large} \\ \hline
4  & 20398.4 & 16831.2 & 0.72 & {\color{blue} medium} \\ \hline
5  & 17348.1 & 2326.6 & 0.94 & {\color{blue} large} \\ \hline
6  & 10727.1 & 2312.9 & 0.92 & {\color{blue} large} \\ \hline
7  & 5070.7 & 332.9 & 0.91 & {\color{blue} large} \\ \hline
8  & 5596.4 & 298.7 & 0.90 & {\color{blue} large} \\ \hline
9  & 16402.4 & 26342.5 & 0.51 & negligible \\ \hline
10 & 11553.0 & 25044.5 & 0.55 & negligible \\ \hline
11 & 11304.3 & 2536.8 & 0.82 & {\color{blue} large} \\ \hline
12 & 18507.3 & 14482.7 & 0.65 & {\color{blue} small} \\ \hline
13 & 17961.5 & 13454.9 & 0.65 & {\color{blue} small} \\ \hline
14 & 10621.9 & 3928.3 & 0.78 & {\color{blue} large} \\ \hline
15 & 23866.3 & 43678.2 & 0.23 & {\color{red} large} \\ \hline
16 & 25321.7  & 27602.0 & 0.54 & negligible \\ \hline
17 & 28515.6 & 52344.9 & 0.47 & negligible \\ \hline
18 & 24218.6 & 123401.3 & 0.14 & {\color{red} large} \\ \hline
19 & 18638.9 & 30682.3 & 0.45 & negligible \\ \hline
20 & 18883.1 & 15841.9 & 0.60 & {\color{blue} small} \\ \hline
   & & Mean & 0.67 & {\color{blue} medium} \\
\hline
\end{tabular}
%% \end{minipage}

\captionof{table}{\label{table:ifc:tests} Mean tests until first failure and effect size
  comparison between \fuzzchick and \mutagen when considering only successful
  runs.}
\vspace{-20pt}
%% \end{wrapfigure}
\end{wrapfigure}

Table \ref{table:ifc:tests} shows a comparison between the mean number of tests
required by both tools to find the first failure \emph{when we only consider
  successful runs.}
%
Additionally, as advised by \citeauthor{arcuri2014hitchhiker}
\cite{arcuri2014hitchhiker}, we computed the non-parametric Vargha-Delaney
$A_{12}$ measure \cite{vargha2000critique}.
%
Intuitively, this measure encodes the probability of \mutagen to yield better
results than \fuzzchick.
%
The \textit{Estimate} of this measure simply denotes the statistical difference
between the compared distributions in a categorical manner.
%
Estimates in {\color{red} red} and {\color{blue} blue} indicate results
favoring \fuzzchick and \mutagen, respectively.
%
As it can be observed,
%
%% using an exhaustive mutation approach can be not only
%% more reliable, but also faster.
%
\mutagen is likely to find bugs faster than \fuzzchick in 14 of the 20 bugs
injected into the abstract machine, being substantially slower in only 2 cases.



In terms of the \mutagen heuristics,
%
%% we can also arrive to some conclusions.
%% %
%% Firstly,
%
this case study is not strongly affected by using LIFO scheduling.
%% --- the same
%% benchmark takes only roughly 20\% longer to complete when disabling this
%% heuristic.
%
We noticed that when a new interesting test case gets enqueued, all its mutants
(and their descendants) are quickly processed before new ones start piling up,
keeping the mutation queues empty most of the time (generation mode).
%
Hence, the small proportion of time this heuristic is active (when the mutation
queues are not empty) tends to be inconsequential.
%% under this case study.


Moreover, disabling the trace saturation heuristic (\textit{no reset}) we
observed two effects:
%
fixing the number of random mutations to 25 adds a constant overhead when
finding most of the (easier) bugs, suggesting that we are spending too time
mutating numeric subexpressions inside the generated machine states.
%
%% and that a
%% smaller number of random mutations could be equally effective to find such bugs.
%
However, some of the hardest-to-find bugs cannot be reliably exposed using this
fixed amount of random mutations (\#4, \#9, \#12, \#18, and \#10), suggesting
that one should increase this number even further to be able to discover all
bugs within the time budget.
%
In consequence, we argue that our heuristic is effective at automatically tuning
this parameter.
%
%% , especially when the user is unsure about what the best value for it might be.


\subsection{WebAssembly Engine}

The results of this case study are shown in Fig. \ref{fig:results:wasm}, ordered
by the MTTF achieved by \mutagen.
%% %
%% Given that we used different properties to test the WebAssembly validator and
%% interpreter, we will focus on these results independently.
%
We first focus on the bugs injected in the validator (Fig.
\ref{fig:results:wasm} left).
%
There, we quickly conclude that \quickcheck is not well suited to find most of
the bugs --- it merely finds the easier bugs \#4 and \#5 in just 1 out of 30
runs.
%
The reason behind this simple:
%
%% WebAssembly modules comprise several different
%% interconnected components, e.g., types and functions definitions, memory blocks,
%% imports, exports, etc.
%% %
%% All these components need to be valid and agree with each other in order for the
%% module to be correctly validated (modulo the injected bugs).
%
using an automatically derived generator is virtually unable to produce valid
WebAssembly modules apart from the trivial empty one.
%
Using the same random generator, however, \mutagen is able to consistently find
every bug in less than 20 seconds in the absolute worst case.


In terms of the \mutagen heuristics, we observe two clear phenomena.
%
On one hand, disabling the LIFO scheduling (case \textit{no LIFO}), the
hardest-to-find bugs (\#7, \#10, and \#1) are no longer found on every run.
%
Moreover, although bugs \#2, \#9, \#8, and \#6 are found with 100\% success rate
across runs, it takes several times longer for \mutagen to find them.
%
On the other hand, disabling the trace saturation heuristic (case \textit{no
  reset}) and using a fixed number of 25 random mutation does not affect the
effectiveness of our tool, apart from adding again a constant time overhead.
%% ---
%% suggesting that the value we arbitrarily chose for this parameter is too large
%% and should be left to be handled automatically by our tool.


If we now pay attention to the bugs injected into the interpreter (Fig.
\ref{fig:results:wasm} right), we notice that finding bugs requires
substantially more time (minutes instead of seconds), since both interpreters
need to validate and run the inputs before producing a result to compare.
%
We can also observe a significant improvement in the performance of \quickcheck
in terms of failure rate.
%
This is of no surprise: we deliberately reduced the search space by using a
module stub when defining \texttt{prop\_interpreter} in our test suite.
%
Notably, \quickcheck finds counterexamples for the bug \#14 almost instantly.
%
The reason behind this is that this bug can be found using a very small
counterexample, and \quickcheck prefers sampling small test cases at the
beginning of the testing loop.
%
Our tool follows this same approach when in generation mode.
%
However, when a test case is found interesting, the scheduler does not take its
size into account while computing its priority --- future work should
investigate this possibility.
%
Nonetheless, \mutagen still outperforms \quickcheck on the remaining bugs both
in terms of failure rate and mean time to failure.
%
Moreover, by disabling the LIFO scheduling we observe that the performance of
our tool becomes unstable, where bugs \#12 and \#11 take substantially less time
to be found, whereas bugs \#15 and \#14 cannot be found on every run.
%
Finding where the best trade-off lies in cases where the heuristics cause mixed
results like this is another interesting direction to aim our future work.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{tikz/wasm-mttf.tikz}
  \includegraphics[width=\linewidth]{tikz/wasm-fr.tikz}\vspace{-5pt}
  \includegraphics[width=\linewidth]{tikz/wasm-legend.tikz}
  \vspace{-20pt}
  \caption{\label{fig:results:wasm} Comparison between \quickcheck and \mutagen
    across 15 different bugs injected into \textit{haskell-wasm}. }
%% \vspace{-5pt}
\end{figure}

\begin{table}[t]
\footnotesize
%% \vspace{-5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{
  \multirow{2}{*}{Property}}
  & \multicolumn{2}{c|}{\quickcheck}
  & \multicolumn{2}{c|}{\mutagen} \\
  \cline{2-5}
  \multicolumn{1}{|c|}{}
  & \multicolumn{1}{c|}{total}
  & passed
  & \multicolumn{1}{c|}{total}
  & passed \\
\hline
\textit{prop\_validator}
& 41374.92
& 0.00029
& 2243.26
& 519.83\\
\hline
\textit{prop\_interpreter\_i32}
& 134000
& 23.16
& 1815.81
& 351.16 \\
\hline
\end{tabular}
\caption{\label{table:wasm:overhead}Mean number of tests per second executed by
  each tool on the WebAssembly case study.}
\vspace{-10pt}
\end{table}


Finally, this case study allows us to analyze the overhead introduced by the
code instrumentation and internal processing used in \mutagen versus the
stateless black-box approach of \quickcheck.
%
Table \ref{table:wasm:overhead} compares the total number of executed and passed
tests per second that we observed using each tool.
%
\mutagen executes tests several times slower than \quickcheck --- roughly 20x
and 75x slower when testing \texttt{prop\_validator} and
\texttt{prop\_interpreter\_i32}, respectively.
%% %
%% This slowdown is partly caused because the code instrumentation our tool
%% currently uses is built upon a superficial transformation that logs traces at
%% the user level.
%% %
%% We anticipate that this overhead could be reduced considerably by integrating
%% the instrumentation directly into the Haskell runtime --- a substantial
%% challenge to tackle in the future.
%
However, our tool is capable of running substantially more tests that pass the
sparse preconditions than \quickcheck in the same amount of time, \emph{which
  can ultimately lead us to find more bugs}.

% ----------------------------------------

%% \section{Implementation}
%% \label{sec:implementation}

%% \subsection{Deriving Mutation Machinery}

%% Integer placerat tristique nisl. Praesent augue. Fusce commodo. Vestibulum
%% convallis, lorem a tempus semper, dui dui euismod elit, vitae placerat urna
%% tortor vitae lacus. Nullam libero mauris, consequat quis, varius et, dictum id,
%% arcu. Mauris mollis tincidunt felis. Aliquam feugiat tellus ut neque. Nulla
%% facilisis, risus a rhoncus fermentum, tellus tellus lacinia purus, et dictum
%% nunc justo sit amet elit.

%% \subsection{Tracing Haskell Programs}

%% \begin{verbatim}
%%   sorted []       = True
%%   sorted [x]      = True
%%   sorted (x:y:xs) = if x <= y then sorted (y:xs) else False
%% \end{verbatim}

%% \begin{verbatim}
%%   sorted []       = _trace_ 1 (True)
%%   sorted [x]      = _trace_ 2 (True)
%%   sorted (x:y:xs) = _trace_ 3 (if x <= y then _trace_ 4 (sorted (y:xs)) else _trace_ 5 (False))
%% \end{verbatim}

%% Pellentesque dapibus suscipit ligula. Donec posuere augue in quam. Etiam vel
%% tortor sodales tellus ultricies commodo. Suspendisse potenti. Aenean in sem ac
%% leo mollis blandit. Donec neque quam, dignissim in, mollis nec, sagittis eu,
%% wisi. Phasellus lacus. Etiam laoreet quam sed arcu. Phasellus at dui in ligula
%% mollis ultricies.

% ----------------------------------------

\section{Related work}
\label{sec:related}

There exists a vast literature on fuzzing and property-based testing.
%
In this section we focus on three main topics:
%
automated random data generation using static information,
%
coverage-guided fuzzing, and
%
exhaustive bounded testing, all of which inspired this work.


\paragraph{Automated Random Data Generation}

Obtaining random generators automatically from static information (e.g.,
grammars or data type definitions) has been tackled from different angles.
%
% DRAGEN and DRAGEN2
DRAGEN \cite{DBLP:conf/haskell/MistaRH18} is a meta-programming tool that
synthesises random generators from data types definitions, using stochastic
models to predict and optimize their distribution based on a target one set by
the user.
%
DRAGEN2 \cite{Mista2019GeneratingRS} extends this idea with support for
generating richer random data by extracting library APIs and function input
patterns from the codebase.
%
%% % Bolztmann-brain
%% Similarly, \cite{Bendkowski2017} developed a tuning mechanism based on Boltzmann
%% samplers \cite{Duchon2004} that synthesizes generators with approximate-size
%% distributions for combinatorial structures like lists or trees.
%
%
% QuickFuzz
%
\emph{QuickFuzz} \cite{GriecoCB16, grieco2017} is a fuzzer that exploits these
ideas to synthesize random generators from existing Haskell libraries. These
generators are used in tandem with existing low-level fuzzers to find bugs in
heavily used programs.



Automatically deriving random generators is substantially more complicated when
the generated data must satisfy (often sparse) preconditions.
%
% Generating Constrained Random Data with Uniform Distribution
%
\citeauthor{ClaessenDP14} \citeyearpar{ClaessenDP14} developed an algorithm for
generating inputs constrained by boolean preconditions with almost-uniform
distribution.
%
% Luck
%
Later, \citeauthor{LampropoulosGHH17} \citeyearpar{LampropoulosGHH17} extended
this approach by adding a limited form of constraint solving controllable by the
user in a domain-specific language called \emph{Luck}.
%
% Deriving good generators
%
Recently, \citeauthor{Lampropoulos2017} \citeyearpar{Lampropoulos2017} proposed
a mechanism to obtain constrained random generators by defining their target
structure using inductively defined relations in Coq.



We consider all these generational approaches to be orthogonal to the ideas
behind \mutagen.
%
While our tool is tailored to improve the performance of poor automatically
derived generators, specialized ones can be used directly by \mutagen, and
combining them with structure-preserving mutations is part of our future work.


\paragraph{Coverage-guided Fuzzing}

% AFL
\emph{AFL} \cite{afl} is the reference tool when it comes to coverage-guided
fuzzing.
%
% AFLfast
\emph{AFLFast} \cite{bohme2017coverage} extends AFL using Markov chain models to
tune the power scheduler towards testing low-frequency paths.
%
In our tool, the scheduler does not account for path frequency.
%
Instead, it favors a breadth-first traversal of the execution path space.
%
% CollAFL
Similar to \mutagen, \emph{CollAFL} \cite{gan2018collafl} is a variant of AFL
that uses path- instead of edge-based coverage to distinguish executions more
precisely by reducing path collisions.

%AFLGo
If the source code history is available, \emph{AFLGo} \cite{bohme2017directed}
is a fuzzer that can be targeted to exercise the specific parts of the code
affected by recent commits in order to find potential new bugs.
%
In PBT, a related idea called \emph{targeted property-based testing} (TPBT) is
to use fitness functions to guide testing efforts towards user defined goals
\cite{loscher2017targeted, loscher2018automating}.


%% % Coverage-guided fuzzing + symbolic execution
%% %
%% Moreover, a popular technique is to combine coverage-guided fuzzing with ideas
%% borrowed from symbolic execution tools like \emph{KLEE} \cite{cadar2008klee} to
%% avoid getting stuck in superficial paths \cite{stephens2016driller}.
%% %
%% When using off-the-shelf symbolic executors, this technique is often limited by
%% the path explosion problem, although recent tools like \emph{QSYM}
%% \cite{yun2018qsym} demonstrate that using tailored concolic executors can help
%% to overcome this limitation.
%
All these ideas can potentially be incorporated in our tool, and we keep them as
a challenge for future work.


\paragraph{Exhaustive Bounded Testing}

A popular category of property-based testing tools does not rely on randomness.
%
Instead, all possible inputs can be enumerated and tested from smaller to larger
up to a certain size bound.
%
%
% Feat
%
\emph{Feat} \cite{DuregardJW12} formalizes the notion of \emph{functional
  enumerations}.
%
For any algebraic type, it synthesizes a bijection between a finite prefix of
the natural numbers and a set of increasingly larger values of the input type.
%
Later, this bijection can be traversed exhaustively or, more interestingly,
randomly accessed.
%
This allows the user to easily generate random values uniformly simply by
sampling natural numbers.
%
However, values are enumerated based only on their type definition, so this
technique is not suitable for testing properties with sparse preconditions
expressed elsewhere.
%
% SmallCheck and LazySmallCheck
\emph{SmallCheck} \cite{runciman2008smallcheck} is a Haskell tool that follows
this idea.
%
It progressively executes the testing properties against all possible input
values up to a certain size bound.
%
% Korat
\emph{Korat} \cite{boyapati2002korat} is a Java tool that uses method
specification predicates to automatically generate all non-isomorphic test cases
up to a given small size.


These approaches rely on pruning mechanisms to avoid populating unevaluated
subexpresions exhaustively before the computational cost of being exhaustive
becomes too restrictive.
%
\emph{LazySmallCheck} is a variant of \emph{SmallCheck} that uses lazy
evaluation to automatically prune the search space by detecting unevaluated
subexpressions.
%
In the case of \emph{Korat}, pruning is done by instrumenting method
precondition predicates and analyzing which parts of the execution trace
correspond to each evaluated subexpression.


In this work we use exhaustiveness as a way to reliably enforce that all
possible mutants of an interesting test case are executed.
%
In contrast to fully-exhaustive testing tools, \mutagen relies on randomness to
find initial interesting mutable test cases.
%
In terms of pruning, it is possible to instruct \mutagen to detect unused
subexpressions to avoid producing mutations over their corresponding positions.
%
This could improve the overall performance when testing non-strict properties
that leave parts of their input unevaluated.
%
In our case studies, however, we observed that the properties we tested
%% their preconditions tend to be
are quite strict when executing test cases obtained by mutating valid existing ones,
fully evaluating their input before executing the postcondition.
%
Thus, we do not forsee considerable improvements by applying lazy-prunning in
this scenario --- gathering evidence about lazy pruning in \mutagen is as part
of our future work.


% ----------------------------------------

\section{Conclusions}
\label{sec:conclusions}

We presented \mutagen, a coverage-guided, property-based testing framework.
%written in Haskell.
%
%% Inspired by exhaustive bounded testing tools,
%
Our tool extends the CGPT approach
%introduced by
%\citeauthor{lampropoulos2019coverage}
with an exhaustive mutation mechanism that generates every possible mutant for
each interesting test case, and schedules it to be tested exactly once.

%% , our tool uses coverage information
%% to guide automatically derived mutators producing high-level, type-preserving
%% mutations.
%% %
%% However, instead of relying heavily on randomness and power schedules to find
%% bugs, our tool uses an exhaustive mutation approach that generates every
%% possible mutant for each interesting input candidate, and schedules it to be
%% tested exactly once.
%% %
%% This is in turn inspired by exhaustive bounded testing tools that focus on
%% testing every possible input value of the system under test up to a certain size
%% bound.
%% %
%% Moreover, we foresee no problems into translating our tool in other programming
%% languages beyond Haskell.

%% --- a more generic technique of limited applicability.


Our experimental results indicate that \mutagen outperforms the simpler CGPT
approach implemented in \fuzzchick in terms of both failure rate and tests until
first failure.
%
Moreover, we show how our tool can be applied in a real-world testing scenario,
where it quickly discovers several planted and some previously unknown bugs.


As indicated throughout this work, there are several directions for future work:
%
combining \mutagen with specialized generators; enhancing the code
instrumentation to be able target mutations towards specific parts of the code;
and evaluating the effect of lazy pruning; among others.
%% %
%% In addition, we want to investigate how to redefine our automatically
%% synthesized mutators in a stateful manner.
%% %
%% This way, it would be possible to apply mutations that preserve complex
%% properties of the generated data simply by construction, e.g., mutations that
%% \emph{always} produce well-typed programs where variables are always in scope.
%% %
%% The main challenge will be to achieve this while keeping the testing process as
%% automatable as possible.


% ----------------------------------------

%% The acknowledgments section is defined using the "acks" environment (and NOT
%% an unnumbered section). This ensures the proper identification of the section
%% in the article metadata, and the consistent spelling of the heading.

%% \begin{acks}
%% To Robert, for the bagels and explaining CMYK and color spaces.
%% \end{acks}

% ----------------------------------------

\balance

%% The next two lines define the bibliography style to be used, and the
%% bibliography file.
\bibliographystyle{ACM-Reference-Format}
%% \bibliographystyle{plainnat}
\bibliography{references}

% ----------------------------------------

%% If your work has an appendix, this is the place to put it.
%% \appendix

\end{document}
\endinput
%%
%% End of `main.tex'.
