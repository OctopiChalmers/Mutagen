%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf, anonymous, review]{acmart}

% ----------------------------------------

%% Packages

\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{color}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{caption}
\usepackage[ruled,vlined,noend]{algorithm2e}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{tikzscale}
\usepackage{pgfplots}
\usepackage{paralist}
\usepackage{enumitem}

\setitemize{noitemsep,nosep,wide,topsep=0pt,parsep=0pt,partopsep=0pt}

\pgfplotsset{compat=1.3}

\usetikzlibrary{trees}

\makeatletter
\patchcmd{\@verbatim}
  {\verbatim@font}
  {\verbatim@font\footnotesize}
  {}{}
\makeatother

\let\oldv\verbatim
\let\oldendv\endverbatim

\def\verbatim{\par\setbox0\vbox\bgroup\oldv}
\def\endverbatim{\oldendv\egroup\fboxsep0pt \noindent\colorbox[gray]{0.95}{\usebox0}\par}

\linepenalty=1000
\widowpenalty=10000
\clubpenalty=10000

\setlength{\skip\footins}{5pt}

% ----------------------------------------

%% Rights management information. This information is sent to you when you
%% complete the rights form. These commands have SAMPLE values in them; it is
%% your responsibility as an author to replace the commands and values with
%% those provided to you when you complete the rights form.
\setcopyright{acmcopyright}
%% \copyrightyear{2021}
%% \acmYear{2021}
%% \acmDOI{10.1145/1122445.1122456}

%% These commands are for a JOURNAL article.
\acmConference[ICSE 2022]{The 44th International Conference on Software
  Engineering}{May 21â€“29, 2022}{Pittsburgh, PA, USA}

%% \acmJournal{JACM}
%% \acmVolume{37}
%% \acmNumber{4}
%% \acmArticle{111}
%% \acmMonth{8}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
\citestyle{acmauthoryear}

% ----------------------------------------
%% Macros

\newcommand{\quickcheck}{\textit{QuickCheck}\xspace}
\newcommand{\quickchick}{\textit{QuickChick}\xspace}
\newcommand{\fuzzchick}{\textit{FuzzChick}\xspace}
\newcommand{\mutagen}{\textsc{Mutagen}\xspace}

% ----------------------------------------

%% end of the preamble, start of the body of the document source.

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[\mutagen]{\mutagen: Coverage-Guided, Property-Based Testing using
  Exhaustive Type-Preserving Mutations}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Agust\'in Mista}
\email{mista@chalmers.se}
\affiliation{%
  \institution{Chalmers University of Technology}
  \city{Gothenburg}
  \country{Sweden}
}

\author{Alejandro Russo}
\email{russo@chalmers.se}
\affiliation{%
  \institution{Chalmers University of Technology}
  \city{Gothenburg}
  \country{Sweden}
}

%% By default, the full list of authors will be used in the page headers. Often,
%% this list is too long, and will overlap other information printed in the page
%% headers. This command allows the author to define a more concise list of
%% authors' names for this purpose.

%% \renewcommand{\shortauthors}{Mista and Russo}


% ----------------------------------------
%% The abstract is a short summary of the work to be presented in the article.

\begin{abstract}
Automatically synthesized random data generators are an appealing option when
using property-based testing.
%
Such generators can be obtained using a variety of techniques that extract
static information from the codebase in order to produce random test cases.
%
However, such techniques are not suitable for deriving generators producing
random values satisfying complex invariants, which in turn complicates testing
properties with sparse preconditions.


Coverage-guided, property-based testing (CGPT) is a recent technique that
alleviates this limitation by enhancing automatically synthesized generators
with type-preserving mutations guided by execution traces.
%
Albeit effective, the initial CGPT approach is limited by factors like large
reliance on randomness and poor mutants scheduling, reducing its applicability
and bug-finding capacity.


In this work we present \mutagen, a CGPT framework that overcomes these
limitations by generating mutants in an \emph{exhaustive} manner.
%
This is coupled with heuristics that help to schedule mutants based on their
novelty and to minimize mutating subexpressions of large enumeration types more
than necessary.
%
Our results indicate that our approach is capable of outperforming existing CGPT
tools, as well as finding bugs in real-world scenarios.
\end{abstract}

% ----------------------------------------

%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. Please
%% copy and paste the code instead of the example below.
\begin{CCSXML}
% CCS XML here
\end{CCSXML}

%% \ccsdesc[500]{Computer systems organization~Embedded systems}
%% \ccsdesc[300]{Computer systems organization~Redundancy}
%% \ccsdesc{Computer systems organization~Robotics}
%% \ccsdesc[100]{Networks~Network reliability}

% ----------------------------------------

%% Keywords. The author(s) should pick words that accurately describe the work
%% being presented. Separate the keywords with commas.
\keywords{random testing, mutations, heuristics}

% ----------------------------------------

%% This command processes the author and affiliation and title information and
%% builds the first part of the formatted document.
\maketitle

% ----------------------------------------

%% \vspace{-5pt}

\section{Introduction}
\label{sec:intro}

%% PBT and generation of interesting data for free
%
Popularized by \quickcheck \cite{ClaessenH00}, Random Property-Based Testing
(RPBT) is a popular technique for finding bugs
\cite{ClaessenH00,hughes2003erlang, papadakis2011proper, bulwahn2012new,
  denes2014quickchick}.
%
This is achieved by using randomly generated inputs to instantiate executable
properties encoding the expected behavior of the system under test.
%
%% Exemplified by frameworks like Haskell's \quickcheck \cite{ClaessenH00} and the
%% Abundant ports of this tool in other programming languages
%% \cite{hughes2003erlang, papadakis2011proper, bulwahn2012new,
%%   denes2014quickchick}
%
%% The most common PBT approach is to instantiate the testing properties using
%% randomly generated inputs.
%
However, users of \quickcheck (and RPBT in general) are well aware that one of
the biggest challenges while using this technique is to provide suitable random
data generators needed to instantiate the testing properties.
%
In some extreme cases, writing highly-tuned random generators capable of
exercising every part of a complex system on a reasonable basis can take several
thousand person-hours of trial and error \cite{lampropoulos2019coverage}.
%
To alleviate this, there exist several techniques to automatically synthesize
random data generators by reifying the static information present in the
codebase, e.g., data type definitions, application public interfaces (APIs),
etc. \cite{GriecoCB16, DBLP:conf/haskell/MistaRH18, Mista2019GeneratingRS,
  DuregardJW12, Lampropoulos2017, Bendkowski2017}.
%
These techniques, however, are unable to synthesize random generators capable of
producing data satisfying complex invariants not easily derivable from the
codebase.
%
Randomly generating valid programs is a common example of this problem, where
developers are forced to put substantial efforts in writing specialized random
generators by hand \cite{Palka11, perenyi2020stack, yang2011finding}.
%
Moreover, automatically synthesized random generators offer particularly poor
results when the testing properties are constrained by sparse preconditions.
%
In this scenario, most of the generated tests are simply thrown away before they
can be used to test the main components of the system under test.


%% FuzzChick: PBT + code instrumentation + type-preserving mutations
%% \fuzzchick \cite{lampropoulos2019coverage} is a property-based testing framework
%% for the Coq programming language that
%
Coverage-Guided, Property-Based Testing (CGPT) \cite{lampropoulos2019coverage}
is a technique that borrows ideas from the fuzzing community to generate highly
structured values while still using automatically derived generators.
%
%% Notably, this tool is implemented as an extension of \quickchick
%% \cite{denes2014quickchick}, Coq's own reimplementation of \quickcheck.
%
Instead of continuously generating random invalid test cases from scratch, CGPT
works by keeping a queue of \emph{interesting} previously executed test cases
that can be mutated to produce new ones.
%
Mutations are achieved using high-level, structure-preserving transformations
that can be derived directly from the codebase.
%
Intuitively, mutating an interesting test case in a small way (at the data
constructor level) has a higher probability of producing a new interesting test
case than generating a new one using a na\"ive generator.
%
In this light, CGPT is likely to preserve the semantic structure of the mutated
data, as mutations are applied directly at the data type level, e.g., the
program's AST in case of generating code.
%
Moreover, CGPT relies on execution traces to distinguish which executed test
cases were interesting and are therefore worth mutating --- a popular technique
known as \emph{coverage-guided fuzzing} and popularized by tools like \emph{AFL}
\cite{afl}.
%
Mutated test cases are considered interesting for mutation only when they
produce new execution traces --- any other test case is simply thrown away.
%
Worth mentioning, this technique is not meant to replace sophisticated, manually
written random generators, but rather to provide an adequate testing solution
for early stages of development.
%, and while a better test suite using manually-written generators is
%still under construction.


%% Of course, this technique is not meant to replace smart manually-written
%% generators.
%% %
%% In turn, it provides an acceptable solution that can be used in the early stages
%% of development, and while a better test suite using manually-written generators
%% is still under construction.
%% %
%% %% Problems of FuzzChick: mutations are superficial, long queues, power
%% %% scheduler
%% %
%% %% In this work, we take a close look at \fuzzchick for opportunities to improve.
%% %
%% %% Notably, we establish several aspects of \fuzzchick that can be
%% %% revised.%
%% %
%% But \fuzzchick suffers several disadvantages.%
%% \footnote{Some of these were already identified by the authors of \fuzzchick in
%%   their original work.}
%% %
%% In particular:
%% %
%% \begin{inparaenum}
%% \item the type-preserving mutations produced by this tool are superficial
%%   and rely heavily on randomness to produce diverse mutants,
%% \item the queueing mechanism can cause large delays when interesting
%%   values are enqueued frequently, and
%% \item the heuristic used to assign a certain ``mutation energy'' to each
%%   interesting test case (often referred to as the \emph{power schedule})
%%   requires fine tuning and can be hard to generalize to work well under
%%   different testing scenarios.
%% \end{inparaenum}
%% %
%% Moreover, we replicated the Information-Flow Control (IFC) stack machine case
%% study bundled with \fuzzchick and observed a surprising limitation:
%% %
%% \emph{after repeating each experiment 30 times, \fuzzchick was only able to find
%%   5 out of the 20 injected bugs with a 100\% efficacy, the hardest one being
%%   found only around 13\% of the time after an hour of testing.}
%% %
%% Despite this limitation, \citeauthor{lampropoulos2019coverage} show that
%% \fuzzchick is far better than using normal random testing coupled with na\"ive
%% random generators by comparing the mean-time-to-failure (MTTF) against
%% \emph{QuickChick}.
%% %
%% However, we believe that a proper evaluation ought to
%% %
%% take failure to find a counterexample as an
%% %
%% important metric when comparing PBT tools, not just mean time to failure ---
%% when one is found.

%
%% Problems of FuzzChick: mutations are superficial, long queues, power
%% scheduler
%
%% In this work, we take a close look at \fuzzchick for opportunities to improve.
%
%% Notably, we establish several aspects of \fuzzchick that can be
%% revised.%
%
%% The approach taken by \fuzzchick leaves considerable room for
%% improvements in several aspects.%
%

In this work, we establish several aspects of the original CGPT approach by
\citeauthor{lampropoulos2019coverage} that leave considerable room for
improvement.%
%
\footnote{Some of these were already identified by
  \citeauthor{lampropoulos2019coverage} in their original work.}
%
In particular:
%
\begin{inparaenum}
\item structure-preserving mutations, if not done carefully, can become
  superficial and rely heavily on randomness to produce diverse mutants,
\item the queuing mechanism can cause large delays if interesting values are
  enqueued frequently and there is no way to prioritize them, and
\item using an heuristic to assign a certain ``mutation budget'' to each
  interesting test case (often referred to as a \emph{power schedule}) requires
  fine tuning and can be hard to generalize to work well under different testing
  scenarios.
\end{inparaenum}
%
%% Moreover, we replicated the Information-Flow Control (IFC) stack machine case
%% study bundled with \fuzzchick and observed a surprising limitation:
%% %
%% after repeating each experiment 30 times, \fuzzchick could only find 5 out of
%% the 20 injected bugs with a 100\% efficacy.
%% %
%% %% the hardest one being found only around 13\% of the time after an hour of
%% %% testing.
%% %
%% Despite this, \citeauthor{lampropoulos2019coverage} show that \fuzzchick is far
%% better than using normal random testing coupled with na\"ive random generators
%% by comparing the mean-time-to-failure (MTTF) against \emph{QuickChick}.
%% %
%% However, we believe that a more meticulous evaluation ought to take failure to
%% find a counterexample as an important metric when comparing PBT tools, not just
%% mean time to failure --- when one is found.


%% Our solution
In this work, we introduce \mutagen, a CGPT framework that tackles the
aforementioned limitations by applying mutations \emph{exhaustively} (see
Section \ref{sec:mutagen}).
%
%% implemented in Haskell
%% that also builds upon code instrumentation and type-preserving mutations
%
%
% the basic ideas behind \fuzzchick, i.e.,
%
%% \mutagen is introduced in detail in Section \ref{sec:mutagen}.
%
%% Exhaustive mutations
%% Unlike \fuzzchick, mutations in \mutagen are applied \emph{exhaustively}.
%
That is, given an interesting, previously executed test case, our tool
precomputes and schedules every structure-preserving mutation that can be
applied to it.
%
This approach has two inherent advantages. %when compared to \fuzzchick.
%
On one hand, mutations do not rely on randomness to be generated.
%
Instead, every subexpression of the input test case is mutated on the same
basis, which guarantees that no interesting mutation is omitted due to
randomness.
%
%% On the other hand, scheduling mutations exhaustively eliminates the need for a
%% power schedule to assign a given mutation energy to each input test case.
%
On the other hand, scheduling mutations exhaustively eliminates the need for a
power schedule to assign a given mutation energy to each input test case.


%% Notably, \emph{scheduling mutations exhaustively does not mean that mutations
%%   are always exhaustive.}
%
Our tool distinguishes two kinds of mutations, those that can be computed
deterministically, yielding a single mutated value; and those that can be
obtained non-deterministically.
%
On one hand, deterministic mutations encode transformations that swap data
constructors around, as well as return or rearrange subexpressions.
%
Non-deterministic (random) mutations, on the other hand, are useful to represent
transformations over large enumeration types, e.g., numbers, characters, etc.
%
This mechanism let us selectively escape the scalability issues of
exhaustiveness by randomly sampling a small random generator a reduced number of
times.
%
In this way, our tool avoids, for instance, mutating a given number into every
other number in the 32 or 64 bits range.


%% Heuristics to improve testing loop
Additionally, the testing loop of our tool incorporates two heuristics that help
finding bugs faster and more reliably (Section \ref{sec:heuristics}).
%
In the first place, \mutagen uses first-in first-out (FIFO) scheduling with
priority to enqueue interesting test cases for mutation.
%
This scheduling algorithm is indexed by the novelty of each new test case with
respect to the ones already seen.
%
In this light, interesting test cases that discover new parts of the code
earlier during execution are given a higher priority.
%
Moreover, our scheduling lets the testing loop jump back and forth between
mutable test cases as soon as a new more interesting one is enqueued for
mutation, eliminating potential delays when the mutation queues grow large.


The second heuristic adjusts the number of times our tool samples random
mutants, i.e., those corresponding to large enumeration types as described
above.
%
For this, we keep track of how often we generate interesting test cases.
%
If this frequency suddenly stalls, \mutagen resets the testing loop increasing
the amount of times we sample such mutants.
%
This automatically finds a suitable value for this external parameter on the
fly.


%% Validation
To validate our ideas, we use two main case studies (described in detail in
Section \ref{sec:casestudies}): the Information-Flow Control (IFC) stack machine
used by \citeauthor{lampropoulos2019coverage} in their original work, and an
existing WebAssembly engine of industrial strenght written in Haskell.
%
In both case studies, we additionally compare the effect of the heuristics
implemented on top of the base testing loop of \mutagen.
%
In the IFC stack machine case study, we compare \mutagen against \fuzzchick, the
reference CGPT implementation by \citeauthor{lampropoulos2019coverage}
implemented in Coq.
%
Our results (Section \ref{sec:evaluation}) indicate that \mutagen outperforms
\fuzzchick both in terms of time to failure and failure rate.
%
On the other hand, our WebAssembly case study shows the performance of our tool
in a realistic scenario.
%
There, \mutagen is capable of reliably finding 15 planted bugs in the validator
and interpreter, as well as 3 previously unknown bugs that flew under the radar
of the existing unit test suite of this engine.
%
This case study also help us to compare the performance of our tool against a
more traditional PBT approach that does not rely on code instrumentation.
%
All in all, our evaluation encompasses more than 600 hours of computing time and
suggests that testing mutants exhaustively can be an appealing technique for
finding bugs reliably without sacrificing speed.

%% Last two sections
Finally, we discuss related work in Section \ref{sec:related} and conclude
in Section \ref{sec:conclusions}.


% ----------------------------------------
%% \vspace{-5pt}

\section{Background}
\label{sec:background}

In this section, we briefly introduce the concept of Property-Based Testing
along with \quickcheck, one of the most popular tools of this sort and often
used as the baseline when comparing PBT algorithms.
%
We also describe in detail the ideas and limitations behind \fuzzchick.

%% which served as the foundation while designing \mutagen.

%% \vspace{-5pt}
\subsection{Property-Based Testing and \quickcheck}

Property-based testing is a powerful technique for finding bugs without having
to write test cases by hand.
%
Popularized by Haskell's \quickcheck, this technique focuses on aiming the
developer's efforts into testing systems via executable properties using
randomly generated inputs.
%
%% Moreover, tools like \quickchick and Isabelle's \quickcheck demonstrate
%% that PBT can also be used in the formal verification realm.
%% %
%% There, one can quickly spot bugs in system specifications before directing the
%% efforts into pointlessly trying to prove bogus propositions.


In the simplest form, there are four main elements the user needs to provide in
order to perform property-based testing on their systems:
%
\begin{itemize}
\item one or more \emph{executable properties}, often implemented simply as
  boolean predicates,
\item \emph{random data generators}, used to repeatedly instantiate the testing
  properties,
\item \emph{printers}, used to show the user the random inputs that falsify some
  testing property (the counterexample) whenever a bug is found, and
\item \emph{shrinkers}, to minimize counterexamples making them easier to
  understand by humans.
\end{itemize}

\noindent In this work we focus solely on the first two elements introduced
above, namely the testing properties and the random data generators used to feed
them.
%
Printers and shrinkers, for the most part, can often be obtained automatically
using generic programming capabilities present in the compiler, and although
being crucial for the testing process as a whole, their role becomes irrelevant
when it comes to \emph{finding} bugs.

\begin{algorithm}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{QC}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\QC{P, N, gen}}{
    i $\gets$ 0\;
    \While{i $<$ N}{
      x $\gets$ Sample(gen)\;
      \lIf{\KwNot P(x)}{\KwRet Bug(x)}
      i $\gets$ i+1 \;
    }
    \KwRet Ok\;
  }
\caption{\label{algo:quickcheck}\quickcheck Testing Loop}
\end{algorithm}

Perhaps the simplest PBT technique is to repeatedly generate random inputs and
use them instantiate the testing properties until they either get falsified by a
counterexample, or we ran a sufficiently large amount of tests --- suggesting
that the properties hold.
%
\quickcheck implements a testing loop that closely follows this simple idea,
which is outlined in Algorithm \ref{algo:quickcheck}, where $P$ is the testing
property, $N$ is the maximum number of tests to perform, and $gen$ is the random
generator used to instantiate $P$.



To illustrate this technique, let us focus on the same motivating example used
by \citeauthor{lampropoulos2019coverage}, who propose a simple property defined over
binary trees.
%
Such data structure can be defined in Haskell using a custom data type with two
data constructors for leaves and branches respectively:

\begin{verbatim}
data Tree a = Leaf a | Branch (Tree a) a (Tree a)
\end{verbatim}

\noindent The type parameter \texttt{a} indicates that trees can be instantiated
using any other type as payload, so the value \texttt{Leaf True} has type
\texttt{Tree Bool}, whereas the value \texttt{Branch (Leaf 1) 2 (Leaf 3)} has
type \texttt{Tree Int}.
%
Then, we can define tree reflections using a simple recursive function that
pattern matches against the two possible constructors, inverting the order of
the subtrees whenever it encounters a branch:

\begin{verbatim}
mirror :: Tree a -> Tree a
mirror (Leaf x)       = Leaf x
mirror (Branch l x r) = Branch (mirror r) x (mirror l)
\end{verbatim}

Later, a reasonable requirement to assert for is that \texttt{mirror} must be
\emph{involutive}, i.e., reflecting a tree twice always yields the original
tree.
%
We can simply capture this property using a boolean predicate written as a
normal function:

\begin{verbatim}
prop_mirror :: Tree Int -> Bool
prop_mirror t = mirror (mirror t) == t
\end{verbatim}

\noindent For simplicity, here we instantiate the tree payload with integers,
although this predicate should hold for any other type with a properly defined
notion of equality as well.


The last missing piece is a random generator of trees.
%
In \quickcheck, this is commonly done via the type class mechanism
\cite{jones1997type}, instantiating the \texttt{Arbitrary} type class with a
random generator as the implementation of the overloaded \texttt{arbitrary}
operation:

\begin{verbatim}
instance Arbitrary a => Arbitrary (Tree a) where
  arbitrary = sized gen where
  gen 0 = Leaf <$> arbitrary
  gen n = oneof [ Leaf <$> arbitrary
                , Branch <$> gen (n-1) <*> arbitrary <*> gen (n-1) ]
\end{verbatim}

\noindent Let us break this definition into parts.
%
The first line states that we will provide an \texttt{Arbitrary} instance for
trees with a payload of type \texttt{a}, provided that values of type \texttt{a}
can also be randomly generated.
%
This allows us to use \texttt{arbitrary} to generate \texttt{a}'s inside the
definition of our tree generator.


Moreover, \quickcheck internally keeps track of the \emph{maximum generation
  size}, a parameter that can be tuned by the user to limit the size of the
randomly generated values.
%
Our definition exposes this size via \quickcheck's \texttt{sized} combinator,
allowing us to parameterize the maximum size of the randomly generated trees.
%
If the generation size is zero (\texttt{gen 0}), our generator is limited to
produce just leaves with randomly generated payloads.
%
In turn, when the generation size is strictly positive (\texttt{gen n}), the
generator can perform a random uniform choice (\texttt{oneof}) between
generating either a single leaf or a branch.
%
When generating branches, the generator calls itself recursively in order to
produce random subtrees (\texttt{gen (n-1)}).
%
Notice the importance of reducing the generation size on each recursive call.
%
This way we ensure that randomly generated trees using a generation size
\texttt{n} are always finite and have at most \texttt{n} levels.

Finally, we are ready to let \quickcheck test \texttt{prop\_mirror} against a
large number of inputs (100 by default) produced by our brand new random tree
generator:

\begin{verbatim}
quickCheck prop_mirror
+++ OK, passed 100 tests.
\end{verbatim}

\noindent Should we mistakenly introduce a bug in \texttt{mirror}, e.g., by
dropping the right subtree altogether:

\begin{verbatim}
mirror (Branch l x r) = Branch (mirror l) x (mirror l)
\end{verbatim}

\noindent then \quickcheck will quickly falsify \texttt{prop\_mirror}, reporting
a minimized counterexample that we can use to find the root of the issue:

\begin{verbatim}
quickCheck prop_mirror
*** Failed! Falsified (after 2 tests and 2 shrinks):
Branch (Leaf 0) 0 (Leaf 1)
\end{verbatim}

At this point, it is clear that the \emph{quality} of our random generators is
paramount to the performance of the overall PBT process.
%
Random generators that rarely produce interesting values will fail to trigger
bugs in our code, potentially leaving entire parts of the codebase virtually
untested.


Recalling our tree generator, the reader (far from mistaken) might already have
imagined better ways for implementing it.
%
For most practical purposes, this generator is in fact quite bad.
%
However, it follows a simple type-directed fashion, and it is a good example of
what to expect from a random generator synthesized automatically using a process
that knows very little about the values to be generated apart from their
(syntactic) data type structure.

As introduced earlier, there exist multiple tools that can automatically derive
better random generators solely from the static information present in the
codebase.
%
%% Such tools vary on the degree of invariants and requirements
%% %
%% However, such approaches are unable to derive useful generators whenever the
%% target data involves complex invariants that cannot be easily extracted from the
%% codebase or that cannot be easily expressed in terms of inductive relations.
%
Sadly, these tools lack the domain knowledge required to generate random data
with complex invariants --- especially those present in programming languages
like well-scopedness and well-typedeness.


In particular, \emph{automatically derived generators are remarkably ineffective
  when used to test properties with sparse preconditions.}
%
Let us continue with the example by \citeauthor{lampropoulos2019coverage} to
illustrate this problem in more detail.
%
For this, consider that we want to use our \texttt{Tree} data type to encode
binary-search trees (BST) --- this requires some minor tweaks in practice.
%
Then, given a predicate \texttt{isBST} that asserts if a tree satisfies the BST
invariants, we might want to use it as pre- and post-condition to assert that
BST operations like \texttt{insert} preserve them:

\begin{verbatim}
prop_bst_insert :: Tree a -> a -> Bool
prop_bst_insert t a =
  isBST t ==> isBST (insert a t)
\end{verbatim}

\noindent Attempting to test this property using \quickcheck does not work well:

\begin{verbatim}
quickCheck prop_bst_insert
*** Gave up! Passed only 44 tests; 1000 discarded tests.
\end{verbatim}

\noindent \quickcheck \emph{discards} random inputs as soon as it finds they do
not pass the precondition (\texttt{isBST t}).
%
Sadly, most of the inputs generated by our na\"ive generator suffer from this
problem, and the interesting part of the property (\texttt{isBST (insert a t)})
is tested very sporadically as a result.


At this point it is reasonable to think that, to obtain the best results when
using PBT over complex systems, one is forced to put a large amount of time into
developing manually-written generators.
%
In practice, that is most often the case, no automatic effort can beat a
well-thought manually-written generator that produces interesting complex values
and finds bugs in very few tests.
%
Not all is lost, however.
%
It is still possible to obtain acceptable results automatically by incorporating
dynamic information from the system under test into the testing loop.
%
The next subsection introduces the clever technique used by \fuzzchick to find
bugs in complex systems while using simple automatically derived random
generators.

%% Far from trying to compete against manual-efforts, our work focuses on improving
%% automatic approaches so they can be used paliatively in earlier development
%% stages.

\subsection{Coverage-Guided Property-Based Testing with \fuzzchick}

To alleviate the problem of testing properties with sparse preconditions while
using automatically derived random generators, \fuzzchick introduces
\emph{coverage-guided, property-based testing} (CGPT), a technique that enhances
the testing process with two key characteristics:
%
\begin{inparaenum}
\item \emph{target code instrumentation}, to capture execution information from
  each test case; and
\item \emph{high-level, type-preserving mutations}, to produce syntactically
  valid test cases by altering existing ones at the data type level.
\end{inparaenum}

Using code instrumentation in tandem with mutations is a well-known technique in
the fuzzing community.
%
Generic fuzzing tools like \emph{AFL} \cite{afl}, \emph{HonggFuzz}
\cite{honggfuzz}, or \emph{libFuzzer} \citeyearpar{libfuzzer} as well as
language-specific ones like Crowbar \cite{dolan2017testing} or \emph{Kelinci}
\cite{kersten2017poster} use execution traces to recognize interesting test
cases, e.g, those that exercise previously undiscovered parts of the target
code.
%
Later, such tools use generic mutators to combine and produce new test cases
from previously executed interesting ones.
%
\fuzzchick, however, does this in a clever way.
%
Instead of mutating any previously executed test case that discovers a new part
of the code, \fuzzchick integrates these fuzzing techniques into the PBT testing
loop itself.


Since it is possible to distinguish semantically valid test cases from invalid
ones, i.e., those passing the sparse preconditions of our testing properties as
opposed to those that are discarded early, \fuzzchick exploits this information
in order to focus the testing efforts into mutating valid test cases with a
higher priority than those that were discarded.


In addition, high-level, type-preserving mutators are better suited for
producing syntactically valid mutants, avoiding the time wasted by using generic
low-level mutators that act over the ``serialized'' test cases and know very
little about their structure, thus producing syntactically broken mutants most
of the time.
%
This grammar-aware mutation technique has shown to be quite useful when fuzzing
systems accepting structurally complex inputs.
%
Tools like LangFuzz \cite{holler2012fuzzing}, Superion \cite{wang2019superion},
XSmith \cite{xsmith} use existing grammars to tailor the mutators to the
specific input structure used by the system under test.
%
In \fuzzchick, external grammars are not required.
%
The data types used by the inputs of the testing properties already describe the
structure of the random data we want to mutate in a concrete manner, and
specialized mutators acting at the data constructor level can be automatically
derived directly from their definition.

The next subsections describe \fuzzchick's testing loop and type-preserving
mutations in detail.

\subsubsection{Testing loop}

Outlined in Algorithm \ref{algo:fuzzchick:loop}, the process starts by creating
two queues, \textit{QSucc} and \textit{QDisc} for valid and discarded previously
executed test cases, respectively.
%
Enqueued values are stored along with a given mutation energy, that controls how
many times a given test case can be mutated before being finally thrown away.


Once inside of the main loop, \fuzzchick picks the next test case using a simple
criterion: if there are valid values enqueued for mutation, it picks the first
one, mutates it and returns it, decreasing its energy by one.
%
If \textit{QSucc} is empty, then the same is attempted using \textit{QDisc}.
%
If none of the mutation queues contain any candidates, \fuzzchick generates a
new value from scratch.
%
This selection process is illustrated in detail in Algorithm
\ref{algo:fuzzchick:pick}.


Having selected the next test case, the main loop proceeds to execute it,
capturing both the result (passed, discarded due to preconditions, or failed)
and its execution trace over the system under test.
%
If the test case fails, it is immediately reported as a bug.
%
If not, \fuzzchick evaluates whether it was interesting (i.e., it exercises a
new path) based on its trace information and the one from previously executed
test cases (represented by \textit{TLog}).
%
If the test case does in fact discover a new path, it is enqueued at the end of
its corresponding queue, depending on whether it passed or was discarded.
%
%% (i.e., it failed the precondition).
%
This process alternates between generation and mutation until a bug is found or
we reach the test limit.


The energy assigned to each test case follows that of AFL's power schedule: more
energy to test cases that lead to shorter executions, or that discover more
parts of the code.
%
Moreover, to favor mutating interesting valid test cases, they get more energy
than those that were discarded.



\begin{algorithm}
  \SetArgSty{textnormal}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{FC}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FC{P, N, gen, mut}}{
    i $\gets$ 0\;
    TLog, QSucc, QDisc $\gets$ $\varnothing$\;
    \While{i $<$ N}{
      x $\gets$ Pick(QSucc, QDisc, gen, mut)\;
      (result, trace) $\gets$ WithTrace(P(x))\;
      \lIf{\KwNot result}{\KwRet Bug(x)}
      \If{Interesting(TLog, trace)}{
        e $\gets$ Energy(TLog, x, trace)\;
        \eIf{\KwNot Discarded(result)}{
          Enqueue(QSucc, (x, e))\;
        }{
          Enqueue(QDisc, (x, e))\;
        }
      }
      i $\gets$ i+1\;
    }
    \KwRet Ok\;
  }
\caption{\label{algo:fuzzchick:loop}\fuzzchick Testing Loop}
\end{algorithm}

\begin{algorithm}
  \SetArgSty{textnormal}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{Pick}{Pick}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Pick{QSucc, QDisc, gen, mut}}{
    \If{\KwNot Empty(QSucc)}{
      (x,e) $\gets$ Deque(QSucc)\;
      \If{e $>$ 0}{
        PushFront(QSucc, (x, e-1))
      }
      \KwRet Sample(mut(x))\;
    }
    \uElseIf{\KwNot Empty(QDisc)}{
      (x,e) $\gets$ Deque(QDisc)\;
      \If{e $>$ 0}{
        PushFront(QDisc, (x, e-1))
      }
      \KwRet Sample(mut(x))\;
    }
    \lElse{\KwRet Sample(gen)}
  }
\caption{\label{algo:fuzzchick:pick}\fuzzchick Seed Selection}
\end{algorithm}

\subsubsection{Type-preserving mutations}

Mutators in \fuzzchick are no more than specialized random generators,
parameterized by the original input to be mutated.
%
They use a simple set of mutation operations that are randomly applied at the
data type level.
%
In simple terms, these operations encompass
%
\begin{inparaenum}
\item \emph{shrinking the value}, replacing its outermost data constructor with
  one that contains a subset of its fields, reusing existing subexpressions;
\item \emph{growing the value}, replacing its outermost data constructor with
  one that contains a superset of its fields, reusing existing subexpressions
  and generating random ones when needed;
\item \emph{returning a subexpression of the same type}; and
\item \emph{mutating recursively}, applying a mutation operation over an
  immediate subexpression.
\end{inparaenum}


%% \begin{figure}
%% \vspace{-5pt}
%% \begin{verbatim}
%% mutate_tree :: (a -> Gen a) -> Tree a -> Gen (Tree a)
%% mutate_tree mutate_a (Leaf x) =
%%   oneof [ do { x' <- mutate_a x; return (Leaf x')}                       -- Mutate recursively
%%         , do { l <- arbitrary; r <- arbitrary; return (Branch l x r) } ] -- Grow constructor
%% mutate_tree mutate_a (Branch l x r) =
%%   oneof [ return l                                                       -- Return subexpression
%%         , return r                                                       -- Return subexpression
%%         , return (Leaf x)                                                -- Shrink constructor
%%         , do { l' <- mutate_tree l; return (Branch l' x r) }             -- Mutate recursively
%%         , do { x' <- mutate_a    x; return (Branch l x' r) }             -- Mutate recursively
%%         , do { r' <- mutate_tree r; return (Branch l x r') } ]           -- Mutate recursively
%% \end{verbatim}
%% \caption{\label{fig:fuzzchick:mutator}\fuzzchick mutator for the \texttt{Tree} data type. }
%% \end{figure}

\begin{figure}
%% \vspace{-5pt}
\begin{verbatim}
mutate_tree :: (a -> Gen a) -> Tree a -> Gen (Tree a)
mutate_tree mutate_a (Leaf x) =
  oneof [ Leaf <$> mutate_a x
        , Branch <$> arbitrary <*> return x <*> arbitrary ]
mutate_tree mutate_a (Branch l x r) =
  oneof [ return l
        , return r
        , Leaf   <$> return x
        , Branch <$> mutate_tree l <*> return x   <*> return r
        , Branch <$> return l      <*> mutate_a x <*> return r
        , Branch <$> return l      <*> return x   <*> mutate_tree r
\end{verbatim}
\caption{\label{fig:fuzzchick:mutator}\fuzzchick mutator for the \texttt{Tree} data type. }
\end{figure}

Fig. \ref{fig:fuzzchick:mutator} illustrates a \fuzzchick-like mutator for our
previously used \texttt{Tree} data type example.
%
Since trees are parametric, for clarity this definition is also parameterized by
a mutator for the payload (\texttt{mutate\_a}), although this can be abstracted
away using the type class system.

%% (where \texttt{mutate\_tree} and
%% \texttt{mutate\_a} can be transparently coerced into an overloaded
%% \texttt{mutate} function.)


In this mutator, branches can be shrunk into leaves by dropping the subtrees,
whereas leaves can grow into branches, by reusing the payload and generating two
random subtrees.
%
Moreover, branches can be replaced with one of their subtrees.
%
Finally, mutations can be recursively applied over both the payload and the
subtrees.
%
At the top level, all these operations are put together using the \texttt{oneof}
combinator that randomly picks one of them with uniform probability.

%
% FuzzChick Limitations
%
\subsubsection{Limitations of \fuzzchick}

\citeauthor{lampropoulos2019coverage} demonstrated empirically that \fuzzchick
lies comfortably in the middle ground between using pure random testing with
na\"ive automatically derived random generators and complex manually-written
ones.
%
Their results suggest that CGPT is an appealing technique for finding bugs while
still using a mostly automated workflow.
%
However, the authors acknowledge that certain parts of its implementation have
room for improvement, especially when it comes to the mutator's design.
%
%% Moreover, we replicated the Information-Flow Control (IFC) stack machine case
Moreover, we replicated the IFC stack machine case study bundled with \fuzzchick
and observed a surprising limitation:
%
\emph{after repeating each experiment 30 times, \fuzzchick could only find 5 out
  of the 20 injected bugs with a 100\% efficacy, the hardest one being found
  only around 13\% of the time after an hour of testing.}
%
Despite this, the authors show that \fuzzchick outperforms normal random testing
coupled with na\"ive random generators by comparing the mean-time-to-failure
(MTTF) against \emph{QuickChick}.
%
In turn, we believe that a more meticulous evaluation ought to take failure to find a
counterexample as an important metric when comparing PBT tools, not just mean
time to failure --- when one is found.
%
%% In tandem with the lack of efficacy observed when replicating the evaluation of
%% the IFC stack machine case study,


All these observations led us to consider three main aspects of \fuzzchick that
can be improved upon --- and that constitute the main goal of this work.
%
%% As mentioned earlier, when we recreated the evaluation of the IFC stack machine
%% case study (described in detail in Section \ref{sec:casestudies}), we found that
%% after 30 runs (as opposed to the 5 runs used original by
%% \citeauthor{lampropoulos2019coverage}), \emph{\fuzzchick was only able to find 5
%%   out of the 20 injected bugs with a failure rate of 1}, the hardest one being
%% found only around 13\% of the time after an hour of testing.
%% %
%% These results are presented in detail in Section \ref{sec:evaluation}.
%
%% At the light of these observations, we identified three main aspects of
%% \fuzzchick that can be improved upon --- and that constitute the main goal of
%% this work.
%
In no particular order:

\begin{itemize}
\item \emph{Mutators distribution:}
%
if we inspect the mutator defined in Fig. \ref{fig:fuzzchick:mutator}, there are
two compromises that the authors of \fuzzchick adopted for the sake of
simplicity.
%
On one hand, deep recursive mutations are very unlikely, since their probability
decreases multiplicatively with each recursive call.
%
For instance, mutating a subexpression that lies on the fourth level of a
\texttt{Tree} happens with a probability smaller than $(1/6)^3 = \sim 0.0046$,
i.e., when applying a recursive mutation (with probability $1/6$) three
consecutive times.
%
This only worsens as the type of the mutated value becomes more complex.
%
Hence, \fuzzchick mutators can only be effectively used to transform shallow
data structures, potentially excluding interesting applications that might
require producing deeper valid values, e.g., programs, network protocol
interactions, etc.
%
Ideally, mutations should be able to happen on every subexpression of the input
seed on a reasonable basis.

On the other hand, using random generators to produce needed subexpressions
when growing data constructors can be dangerous, as we are introducing the very
same ``uncontrolled'' randomness that we wanted to mitigate in the first place!
%
If the random generator produces an invalid subexpression (something quite
likely), this might just invalidate the whole mutated test case.
%
We believe that growing data constructors needs to be done carefully.
%
For instance, by using just a minimal piece of data to make the overall mutated
test case type correct.
%
If that mutated test case turns to be interesting, that new subexpression can
always be mutated later.

\item \emph{Enqueuing mutation candidates:}
%
\fuzzchick uses two single queues for keeping valid and discarded mutation
candidates.
%
Whenever a new test case is found interesting, it is placed \emph{at the end of
  its corresponding queue}.
%
If this test case happens to have discovered a whole new portion of the target
code, it will not be further mutated until the rest of the queue ahead of it
gets processed.
%
This can limit the effectiveness of the testing loop if the queues tend to grow
more often than they tend to shrink, as interesting mutation candidates can get
buried at the end of a long queue that only exercises the same portion of the
target code.
%
In an extreme case, they might not be processed at all within the testing
budget.
%
Ideally, one would like a mechanism that prioritizes mutating test cases that
discover new portions of the code right away, and that is capable of jumping
back and forth from mutation candidates whenever this happens.
%
%% We show in Section \ref{sec:heuristics} how this can be achieved by analyzing
%% the execution information in order to prioritize test cases with novel execution
%% traces.


\item \emph{Power schedule:}
%
It is not clear how the power schedule used to assign energy to each mutable
test case in \fuzzchick works in the context of high-level type-preserving
mutations.
%
If it assigns too much energy to certain not-so-interesting seeds, some bugs
might not be discovered on a timely basis.
%
Conversely, assigning too little energy to interesting test cases might mean
that some bugs cannot be discovered at all unless the right mutation happens
within the small available energy window --- randomly generating the same test
case again later does not help, as it becomes uninteresting based on historic
trace information.

To keep the comparison fair, the authors replicated the same power schedule
configuration used in AFL.
%
However, AFL uses a different mutation approach that works at the bit level.
%
This raises the question about what is the best power schedule configuration
when using a high-level mutation approach --- something quite challenging to
characterize in general given the expressivity of the data types used to drive
the mutators.\\

\end{itemize}


The next section introduces \mutagen, our CGPT tool written in Haskell that aims
to tackle the main limitations of \fuzzchick using an exhaustive mutation
approach that requires very little randomness and no power schedule.

% ----------------------------------------

\section{\mutagen: Testing Mutants Exhaustively}
\label{sec:mutagen}

In this section we describe the base ideas behind \mutagen, our CGPT tool
written in Haskell.
%
Notably, the ideas presented in this work extend beyond Haskell and functional
programming languages in general.
%
%% Later, Section \ref{sec:heuristics} introduces heuristics that help finding bugs
%% faster in certain testing scenarios.

In contrast with \fuzzchick, \mutagen does not employ a power schedule to assign
energy to mutable candidates.
%
In turn, it resorts to mutate them on an exhaustive and precise manner, where
%
\begin{inparaenum}
  \item each subexpression of a mutation candidate is associated with a set of
    type-preserving mutations, and
  \item for every mutable subexpression, each one of these mutations is
    scheduled \emph{exactly once}.
\end{inparaenum}
%
%% There is a small exception to this rules that we will introduce soon.

This idea is inspired by exhaustive bounded testing tools like \emph{SmallCheck}
\cite{runciman2008smallcheck} or \emph{Korat} \cite{boyapati2002korat}, that
produce test cases exhaustively.
%
In simple words, such tools work by enumerating all possible values of the data
types used in the testing properties, and then executing them from smaller to
larger until a bug is found, or a certain size bound is reached.
%
The main problem with this approach is that the space of all possible test cases
often grows exponentially as we increment the size bound, and the user
experiences what it looks like ``hitting a wall'', where no larger test cases
can be evaluated until we exhausted all the immediately smaller ones
\cite{DuregardJW12}.
%
To alleviate this problem, such tools often rely on heuristics to prune the
search space based on detecting unused subexpressions --- this is discussed in
Section \ref{sec:related}.

%% In consequence, such tools can only be applied to relatively simple systems,
%% where the space of inputs does not grow extremely fast.


Not to be confused by these tools, in \mutagen we do not enumerate all possible
test cases exhaustively.
%
Our tool uses random generators to find interesting initial seeds, and when it
finds them, only then proceeds to schedule all possible mutations.
%
Moreover, as in \fuzzchick, the testing loop of our tool automatically filters
the test cases that are worth mutating by keeping only those that discover new
execution paths in the code under test.


\subsection{Exhaustive Mutations}

To describe \mutagen's testing loop, we first need to introduce the mechanism
used for testing mutations exhaustively.
%
In contrast to \fuzzchick, where mutators are parameterized random generators,
in our tool we define mutations as the set of mutants that can be obtained by
altering the input value at the top-level (the root).
%
In Haskell, we define mutations as:

\begin{verbatim}
type Mutation a = a -> [Mutant a]
\end{verbatim}

\noindent Where \texttt{Mutant}s come in two flavours, pure and random:

\begin{verbatim}
data Mutant a = PURE a | RAND (Gen a)
\end{verbatim}

Pure mutants are used most of the time, and encode simple deterministic
transformations over the outermost data constructor of the input --- recursive
mutations will be introduced soon.
%
These transformations can:
%
\begin{inparaenum}
\item return an immediate subexpression of the same type as the input;
\item swap the outermost data constructor with any other constructor of the
  same type, reusing existing subexpressions; and
\item rearrange and replace fields using existing ones of the same type.
\end{inparaenum}
%
To illustrate this, Fig. \ref{fig:mutagen:mutator} outlines a mutator for the
\texttt{Tree} data type.
%
Notice how this definition simply enumerates mutants that transform the
outermost data constructor, hence no recursion is needed here.
%
Moreover, notice how a default value (\texttt{def}) used to fill the subtrees
when transforming a leaf into a branch.
%
This value corresponds to the simplest expression we can construct for the
mutant to be type-correct.
%
In practice (\texttt{def = Leaf def}), where the inner \texttt{def} is the
simplest value of the payload --- we use the type class system to abstract this
complexity away in our implementation.
%
Using a small value is again inspired by exhaustive bounded testing tools, and
avoids introducing unnecessary randomness when growing data constructors.

%% \begin{figure}
%% \begin{verbatim}
%% mutate (Leaf x)       = [ PURE (Branch def x def) ]   -- Swap constructor
%% mutate (Branch l x r) = [ PURE l, PURE r              -- Return subexpression
%%                         , PURE (Leaf x)               -- Swap constructor
%%                         , PURE (Branch l x l)         -- Rearrange subexpressions
%%                         , PURE (Branch r x r)         -- Rearrange subexpressions
%%                         , PURE (Branch r x l) ]       -- Rearrange subexpressions
%% \end{verbatim}
%% \vspace{-5pt}
%% \caption{\label{fig:mutagen:mutator}\mutagen mutator for the \texttt{Tree} data
%%   type.}
%% \vspace{-10pt}
%% \end{figure}

\begin{figure}
\begin{verbatim}
mutate (Leaf x)       = [ PURE (Branch def x def) ]
mutate (Branch l x r) = [ PURE l, PURE r
                        , PURE (Leaf x)
                        , PURE (Branch l x l)
                        , PURE (Branch r x r)
                        , PURE (Branch r x l) ]
\end{verbatim}
%% \vspace{-5pt}
\caption{\label{fig:mutagen:mutator}\mutagen mutator for the \texttt{Tree} data
  type.}
%% \vspace{-10pt}
\end{figure}

%% At this point, the reader might consider:
%% %
%% what about large enumeration types like integers or characters?
%% %
%% Does \mutagen transform them exaustively too?
%% %
%% \emph{Certainly not.}
%
Random mutants, on the other hand, serve as a way to break exhaustiveness when
mutating values of large enumeration types --- or any other type the user might
want to use random mutations with.
%
Instead of mutating every numerical or character subexpression exhaustively, we
resolve in using a random generator and sample a small number of values from it
--- the precise amount is a tunable parameter of \mutagen.
%
This way, a mutator for integers simply becomes:

\begin{verbatim}
mutate n = [ RAND arbitrary ]
\end{verbatim}


\subsubsection{Mapping top-level mutations everywhere}

So far we have defined mutations that transform only the root of the input.
%
Now it is time to apply these mutations to every subexpression as well.
%
To do so, we will use two functions that can be derived from the data type
definition.


In first place, a function \texttt{positions} traverses the input and builds a
Rose tree of mutable positions.
%
These positions are essentially a list of indices encoding the path from the
root to every mutable subexpression.
%
For instance, the positions of a \texttt{Tree} are computed as follows:

\begin{verbatim}
positions (Leaf x)       = node [ (0, positions x) ]
positions (Branch l x r) = node [ (0, positions l)
                                , (1, positions x)
                                , (2, positions r) ]
\end{verbatim}

\noindent where the function \texttt{node} simply builds a node of the positions
Rose tree and propagates the accumulated prefix (the path from the root) to its
children.
%
In this light, the mutable positions of the value \texttt{Branch (Leaf 1) 2
  (Leaf 3)} are as follows:

\begin{equation}
  \nonumber
  {\small \texttt{positions}}\quad
  \left(
  \tikz[
    baseline=-7mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\small \texttt{Branch}}
      child {node {\small \texttt{Leaf}}
        child {node {\small \texttt{1}}}
      }
      child {node {\small \texttt{2}}}
      child {node {\small \texttt{Leaf}}
        child {node {\small \texttt{3}}}
      };
  }
  \right)
  \quad
  =
  \quad
  \tikz[
    baseline=-7mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\texttt{\small []}}
      child {node {\small \texttt{[0]}}
        child {node {\small \texttt{[0,0]}}}
      }
      child {node {\small \texttt{[1]}}}
      child {node {\small \texttt{[2]}}
        child {node {\small \texttt{[2,0]}}}
      };
  }
\end{equation}

\noindent Later, given the desired position to mutate within a test case, we
define another function \texttt{inside} that finds the subexpression
corresponding to it and applies the mutation.
%
A slightly simplified version of this function for the \texttt{Tree} data type is
as follows:

\begin{verbatim}
inside []      x              = mutate x
inside (0:pos) (Leaf x)       = [ Leaf x'       | x' <- inside pos x ]
inside (0:pos) (Branch l x r) = [ Branch l' x r | l' <- inside pos l ]
inside (1:pos) (Branch l x r) = [ Branch l x' r | x' <- inside pos x ]
inside (2:pos) (Branch l x r) = [ Branch l x r' | r' <- inside pos r ]
\end{verbatim}

\noindent This function simply traverses the desired position, calling itself
recursively until it reaches the desired subexpression, where the mutation can
be applied locally at the top-level (case \texttt{inside [] x}).
%
The rest of the function takes care of unwrapping and rewrapping the
intermediate subexpressions and is not particularly relevant for the point being
made.


\subsection{Testing loop}

Having the exhaustive mutation mechanism in place, we can finally introduce the
base testing loop used by \mutagen.
%
This is outlined in Algorithm \ref{algo:mutagen:loop}.
%
As it can be observed, we closely follow \fuzzchick's testing loop, using two
queues to keep mutable candidates, and enqueueing and retrieving interesting
test cases from them before falling back to random generation.

\begin{algorithm}
  \SetArgSty{textnormal}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetKw{KwIn}{in}
  \SetKwFor{KwRepeat}{repeat}{times}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{Muts}{Mutate}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Muts{x, mut, R}}{
    batch $\gets$ $\varnothing$\;
    \For{pos \KwIn Flatten(Positions(x))}{
      \For{mutant \KwIn Inside(pos, mut, x)}{
        \Switch{mutant}{
          \Case{\texttt{PURE} $\hat{x}$}{
            Enqueue($\hat{x}$, batch)\;
          }
          \Case{\texttt{RAND} gen}{
            \KwRepeat{R}{
              $\hat{x}$ $\gets$ Sample(gen)\;
              Enqueue($\hat{x}$, batch)\;
            }
          }
        }
      }
    }
    \KwRet batch\;
  }
\caption{\label{algo:mutagen:init}Mutants Initialization}
\end{algorithm}


The main difference lies in that we precompute all the mutations of a given
mutation candidate before enqueueing them.
%
These mutations are put together into lists that we call \emph{mutation batches}
--- one for each mutated test case.
%
To initialize a mutation batch (outlined in Algorithm \ref{algo:mutagen:init}),
we first flatten all the mutable positions of the input value in level order
(recall that positions are stored as a Rose tree).
%
Then, we iterate over all of them and retrieve all the mutants defined for each
subexpression.
%
For each one of these, there are two possible cases:
%
\begin{inparaenum}
\item if it is a pure mutant carrying a concrete mutated value, we enqueue it
  into the mutation batch directly; otherwise
\item it is a random mutant that carries a random generator with it (e.g.,
  corresponding to a numeric subexpression), in which case we sample and enqueue
  \textit{R} random values using this generator, where \textit{R} is a parameter
  set by the user.
\end{inparaenum}
%
At the end, we simply return the accumulated batch.


Finally, the seed selection algorithm (Algorithm \ref{algo:mutagen:pick}) simply
selects the next test case using the same criteria as \fuzzchick, prioritizing
valid candidates over discarded ones, falling back to random generation when
both queues are empty.
%
Since mutations are precomputed, this function only needs to pick the next test
case from the current batch, until it becomes empty and can switch to the next
precomputed one in line.


\begin{algorithm}
  \SetArgSty{textnormal}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{MG}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\MG{P, N, R, gen, mut}}{
    i $\gets$ 0\;
    TLog, QSucc, QDisc $\gets$ $\varnothing$\;
    \While{i $<$ N}{
      x $\gets$ Pick(QSucc, QDisc, gen)\;
      (result, trace) $\gets$ WithTrace(P(x))\;
      \lIf{\KwNot result}{\KwRet Bug(x)}
      \If{Interesting(TLog, trace)}{
        \If{\KwNot Discarded(result)}{
          batch $\gets$ Mutate(x, mut, R)\;
          Enqueue(QSucc, batch)\;
        }
        \uElseIf{Passed(Parent(x))}{
          batch $\gets$ Mutate(x, mut, R)\;
          Enqueue(QDisc, batch)\;
        }
      }
      i $\gets$ i+1\;
    }
    \KwRet Ok\;
  }
\caption{\label{algo:mutagen:loop}\mutagen Testing Loop}
\end{algorithm}

\begin{algorithm}
  \SetArgSty{textnormal}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{Pick}{Pick}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Pick{QSucc, QDisc, gen}}{
    \If{\KwNot Empty(QSucc)}{
      batch $\gets$ Deque(QSucc)\;
      \If{Empty(batch)}{
        Pick(QSucc, QDisc, gen)\;
      } \uElse{
        PushFront(QSucc, Rest(batch))\;
        \KwRet First(batch)\;
      }
    }
    \If{\KwNot Empty(QSucc)}{
      batch $\gets$ Deque(QSucc)\;
      \If{Empty(batch)}{
        Pick(QSucc, QDisc, gen)\;
      } \uElse{
        PushFront(QSucc, Rest(batch))\;
        \KwRet First(batch)\;
      }
    }
    \lElse{\KwRet Sample(gen)}
  }
\caption{\label{algo:mutagen:pick}\mutagen Seed Selection}
\end{algorithm}

Another small difference between \mutagen and \fuzzchick is the criterion for
enqueuing discarded tests.
%
We found that, especially for large data types, the queue of discarded
candidates tends to grow disproportionately large during testing, making them
hardly usable and consuming large amounts of memory.
%
To improve this, we resort to mutate discarded tests cases only when we have
some evidence that they are ``almost valid.''
%
For this, each mutated test case remembers whether its parent (the original test
case they derive from after being mutated) was valid.
%
Then, we enqueue discarded test cases only if they meet this condition.
%
As a result, we fill the discarded queue with lesser but much more interesting
mutation candidates.
%
Moreover, this can potentially lead to \emph{2-step mutations}, where an initial
mutation breaks a valid test case in a small way, it gets enqueued as discarded,
and later a subsequent mutation fixes it by changing a different subexpression.


The next section introduces two heuristics we added to the base testing loop of
\mutagen based on the limitations we found in \fuzzchick.
%
%% These heuristics can that offer a substantial improvement in practice.

% ----------------------------------------

\section{\mutagen Heuristics}
\label{sec:heuristics}

In this section we introduce two heuristics implemented on top of the base
testing loop of our tool.
%
\mutagen enables them all by default, although they can be individually disabled
by the user if deems appropriate.

\subsection{Priority FIFO Scheduling}

This heuristic tackles the issue of enqueuing new interesting mutation
candidates at the end of (possibly) long queues of not-so-interesting previously
executed ones.

\fuzzchick uses AFL instrumentation under the hood, which in turn uses an
\emph{edge coverage} criterion to distinguish novel executions and to assign
each mutation candidate a given energy.
%
In contrast, execution traces \mutagen represent the specific \emph{path} in the
code taken by the program, as opposed to just the (unordered) set of edges
traversed in the control-flow graph (CFG).
%
Using this criterion lets us gather precise information from each new execution.
%
%% To keep track of the execution path of each test, our tool keeps an internal
%% prefix tree that records every new entry by traversing this data structure and
%% inserting new nodes after a new path prefix gets discovered.
%% %
In particular, we are interested in the \emph{depth} where each new execution
branches from already seen ones.
%
Our assumption here is that test cases that differ (branch) at shallower depths
from the ones already executed are more likely to discover completely new
portions of the code under test, and hence we want to assign them a higher
priority.


In this light, every time we insert a new execution path into the internal trace
log, we calculate the number of new nodes that were executed, as well as the
\emph{branching depth} where they got inserted.
%
The former is used to distinguish interesting test cases (whether or not new
nodes were inserted), whereas the latter is used by this heuristic to schedule
mutation candidates.
%
Fig. \ref{fig:mutagen:tracelog} illustrates this idea, inserting two execution
traces (one after another) into a trace log that initially contains a single
execution path.
%
The second insertion (with trace $1 \rightarrow 2 \rightarrow 6 \rightarrow 7$)
branches at a shallower depth than the first one (2 vs. 3), hence its
corresponding test case should be given a higher priority.

\begin{figure}[t]
%% \vspace{-15pt}
\newcommand\stacked[2]{\underset{\textstyle #2\mathstrut}{#1}}
\begin{equation}
  \nonumber
  \left[
  \tikz[
    baseline=-10mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{4}}}
        }
      }
  }
  \right]
  \;
  +
  \;
  \tikz[
    baseline=-10mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{5}}}
        }
      }
  }
  \;
  =
  \;
  \left[
  \tikz[
    baseline=-10mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\color{red} \small \texttt{1}}
      child {node {\color{red} \small \texttt{2}}
        child {node {\color{red} \small \texttt{3}}
          child {node {\small \texttt{4}}}
          child {node {\color{blue} \small \texttt{5}}}
        }
      }
  }
  \right]
  ,
  \stacked{{\color{blue} new=1\phantom{x}}}{{\color{red} depth=3}}
  \end{equation}
  \begin{equation}
  \nonumber
  %% \\
  %% \qquad
  %% \qquad
  \left[
  \tikz[
    baseline=-10mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{4}}}
          child {node {\small \texttt{5}}}
        }
      }
  }
  \right]
  \;
  +
  \;
  \tikz[
    baseline=-10mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=0.75cm}
  ]{
    \node {\small \texttt{1}}
      child {node {\small \texttt{2}}
        child {node {\small \texttt{6}}
          child {node {\small \texttt{7}}
          }
        }
      }
  }
  \;
  =
  \;
  \left[
  \tikz[
    baseline=-10mm,
    level distance=0.6cm,
    level 1/.style={sibling distance=0.75cm},
    level 2/.style={sibling distance=1cm}
  ]{
    \node {\color{red} \small \texttt{1}}
      child {node {\color{red} \small \texttt{2}}
        child {node {\small \texttt{3}}
          child {node {\small \texttt{4}}}
          child {node {\small \texttt{5}}}
        }
        child {node {\color{blue}\small \texttt{6}}
          child {node {\color{blue}\small \texttt{7}}}
        }
      }
  }
  \right]
  ,
  \stacked{{\color{blue} new=2\phantom{x}}}{{\color{red} depth=2}}
\end{equation}
%% \vspace{-10pt}
\caption{\label{fig:mutagen:tracelog}Inserting two new execution traces into the
  internal trace log (represented using brackets).}
\end{figure}

With this mechanism in place, we can modify \mutagen's base testing loop by
replacing each mutation queue with a priority queue indexed by the branching
depth of each new execution.
%
These changes are illustrated in Algorithm \ref{algo:mutagen:fifo}.
%
Statements in {\color{red} red} indicate important changes to the base
algorithm, whereas ellipses denote parts of the code that are not relevant for
the point being made.


To pick the next test case, we simply retrieve the one with the highest priority
(the smallest branching depth).
%
Then, whenever we find a new interesting test case, we enqueue it at the
beginning of the queue of its corresponding priority.
%
This allows the testing loop to jump immediately onto processing new interesting
candidates as soon as they are found (even at the same priority), and to jump
back to previous candidates as soon as their mutants become progressively less
novel.

\begin{algorithm}
  \SetArgSty{textnormal}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{MG}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\MG{P, N, R, gen, mut}}{
    $\cdots$\;
    x $\gets$ Pick(QSucc, QDisc, gen, mut)\;
    (result, trace) $\gets$ WithTrace(P(x))\;
    $\cdots$\;
    \If{Interesting(TLog, trace)}{
      \If{\KwNot Discarded(result)}{
        batch $\gets$ Mutate(x, mut, R)\;
        {\color{red} prio $\gets$ BranchDepth(TLog, trace)}\; %% NEW
        {\color{red} PushFront(QSucc, prio, batch)}\;          %% NEW
      }
      $\cdots$\;
    }
  }
  %% \vspace{5pt}
  \SetKwFunction{Pick}{Pick}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Pick{QSucc, QDisc, gen}}{
    \If{\KwNot Empty(QSucc)}{
      {\color{red} (batch, prio) $\gets$ DequeMin(QSucc)}\; %%NEW
      \If{Empty(batch)}{
        Pick(QSucc, QDisc, gen)\;
      } \uElse{
        {\color{red} PushFront(QSucc, prio, Rest(batch))}\; %% NEW
        \KwRet First(batch)\;
      }
    }
    $\cdots$\;
  }

\caption{\label{algo:mutagen:fifo}Priority FIFO Heuristic}
\end{algorithm}

\begin{algorithm}
  \SetArgSty{textnormal}
  \SetInd{0em}{0.75em}
  \SetKw{KwNot}{not}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{MG}{Loop}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\MG{P, N, gen, mut}}{
    {\color{red}
    boring  $\gets$ 0\;
    reset   $\gets$ 1000\;
    R       $\gets$ 1\;
    }
    $\cdots$\;
    \While{i $<$ N}{
      {\color{red}
      \If{boring $>$ reset}{
        TLog   $\gets$ $\varnothing$\;
        reset $\gets$ reset * 2\;
        R     $\gets$ R * 2\;
      }
      }
      $\cdots$\;
      \lIf{\KwNot result}{\KwRet Bug(x)}
      \If{Interesting(TLog, trace)}{
        {\color{red} boring  $\gets$ 0}\;
        $\cdots$\;
      }
      \lElse {
        {\color{red} boring $\gets$ boring + 1}
      }
      $\cdots$\;
    }
  }

\caption{\label{algo:mutagen:reset}Trace Saturation Heuristic}
\end{algorithm}

%% \caption{\label{fig:heuristics} The two main heuristics implemented in \mutagen.
%%   Statements in {\color{red} red} indicate important changes to the base
%%   algorithm. Ellipses denote parts of the code that are not relevant for the
%%   point being made. }


\subsection{Detecting Trace Space Saturation And Tuning Random Mutations Parameter}

As introduced in Section \ref{sec:mutagen}, our tool is parameterized by the
number of random mutations to be generated over each mutable subexpression
defined using a random mutant, e.g., numeric values, characters, etc.
%
But, how many random mutations should we use? A single one? A few tenths? A few
hundred?
%
Using too little can put finding bugs at risk.
%
For instance, when the system under test branches based on numeric values, we
should make sure that we set enough random mutations to test each branch on a
reasonable basis.
%
Using too many, on the other hand, can degrade the performance of the testing
loop, as it will spend too much time producing uninteresting mutations.
%
This can happen for instance if the subexpressions defined using random mutants
are only used as payloads, and their value does not affect the execution in any
way.


Answering this question precisely is not an easy task, and the second heuristic
we introduce in this work aims to tackle this issue.
%
We found that, the smaller the number of random mutations we set, the easier it
is for the trace log that records executions to start getting saturated, i.e.,
when interesting test cases stop getting discovered or are discovered very
sporadically.
%
We realized that we can use this information to automatically optimize the
number of random mutations used by our tool.
%
This idea is described in Algorithm \ref{algo:mutagen:reset}.
%
The process is simple:
%
\begin{inparaenum}
  \item we start the testing loop with the number of random mutations set to
    one,
  %
  \item each time we find that a test is not interesting (i.e. boring), we
    increment a counter,
  \item if we have not produced any interesting test case after a certain
    threshold (1000 tests seems to be a reasonable value in practice), we
    increment the number of random mutations and the threshold by twice the
    current amount.
    %
    Additionally, each time this happens we also reset the trace log, so
    interesting test cases found on a previous iteration can be found and
    enqueued for mutation again --- this time with a higher effort dedicated to
    producing random mutations.
\end{inparaenum}

Then, if the execution of the system under test depends heavily on the values
stored at randomly mutable subexpressions, starting with a single random
mutation will quickly saturate the trace space, and this heuristic will
continuously increase the random mutations parameter until that stops happening.

%% \subsection{Mutation Inheritance}

%% The last heuristic we discuss in this work attempts to reduce the amount of
%% mutations required when testing properties that lazily consume foldable inputs
%% like lists or trees.
%% %
%% To illustrate this, suppose we define a property that aims to test a list
%% sorting algorithm:


% ----------------------------------------

\section{Case studies}
\label{sec:casestudies}

We evaluated the performance of \mutagen using two main case studies.
%
The first one is a simple abstract stack machine that enforces the
hyper-property \emph{noninterference} \cite{goguen1982security} using runtime
checks.
%
The implementation of this case study was originally proven correct by
\citeauthor{10.1145/2578855.2535839} \citeyearpar{10.1145/2578855.2535839} in
Coq, and subsequently degraded by systematically introducing 20 bugs on its
enforcing mechanism.
%
\citeauthor{lampropoulos2019coverage} used this same case study to compare
\fuzzchick against random testing approaches using na\"ive and manually-written
smart random generators.
%
In this work, we replicate their results and compare them against our tool.
%
Worth mentioning, since this case study was originally implemented in Coq, we
first translated it to Haskell in order to run \mutagen on its test suite.


The second case study aims to evaluate \mutagen in a more realistic scenario,
and focuses on testing \textit{haskell-wasm} \cite{haskellwasm}, an existing
WebAssembly engine of industrial strength written in Haskell.
%
We manually injected 10 bugs in the validator as well as 5 bugs in the
interpreter of this engine (see Table \ref{table:wasm:injectedbugs}), and used
the reference WebAssembly implementation to find them via differential testing
\cite{mckeeman1998differential}.
%
During this process, we quickly discovered 3 bugs and 2 discrepancies on this
engine with respect to the reference implementation.
%
All of them were reported and later confirmed by the authors of
\textit{haskell-wasm}.
%
Sadly, comparing \mutagen with \fuzzchick has unfortunately been out of the
scope of this work.
%
The main reason behind this is that porting \textit{haskell-wasm} into Coq, and
later adapting our test suite to work with \fuzzchick requires a subtantial
effort.
%
Instead, we focus on comparing \mutagen against a more traditional PBT approach
using \quickcheck, to evaluate the relative overhead of the code instrumentation
used in our tool.


\begin{table}[t]
\footnotesize
\begin{tabular}{|c|c|l|}
\hline
\textbf{Id}
& \textbf{Subsystem}
& \textbf{Description} \\
\hline
1
& Validator
& Wrong if-then-else type validation on else branch \\
\hline
2
& Validator
& Wrong stack type validation \\
\hline
3
& Validator
& Removed function type mismatch assertion \\
\hline
4
& Validator
& Removed max memory instances assertion \\
\hline
5
& Validator
& Removed function index out-of-range assertion \\
\hline
6
& Validator
& Wrong type validation on \texttt{i64.eqz} instruction \\
\hline
7
& Validator
& Wrong type validation on \texttt{i32} binary operations \\
\hline
8
& Validator
& Removed memory index out-of-range assertion \\
\hline
9
& Validator
& Wrong type validation on \texttt{i64} constants \\
\hline
10
& Validator
& Removed memory alignment validation on \texttt{i32.load} instruction \\
\hline
11
& Interpreter
& Wrong interpretation of \texttt{i32.sub} instruction \\
\hline
12
& Interpreter
& Wrong interpretation of \texttt{i32.lt\_u} instruction \\
\hline
13
& Interpreter
& Wrong interpretation of \texttt{i32.shr\_u} instruction \\
\hline
14
& Interpreter
& Wrong local variable initialization \\
\hline
15
& Interpreter
& Wrong memory address casting on \texttt{i32.load8\_s} instruction \\
\hline
\end{tabular}
\caption{\label{table:wasm:injectedbugs}Bugs injected into
  \textit{haskell-wasm}.}
%% \vspace{-15pt}
\end{table}


\subsection{IFC Stack Machine}

The abstract stack machine used in this case study consists of four main
elements: a program counter, a stack, and data- and instruction memories.
%
Moreover, every runtime value is labeled with a security level, i.e., L (for
``low'' or public) or H (for ``high'' or secret).
%
Labels form a trivial 2-lattice where information can either stay at the same
level, or public information ``can flow'' to secret one but not the opposite
(represented using the familiar $\sqsubseteq$ binary operator).
%
Security labels are propagated throughout the execution of the program every
time the machine executes an instruction.
%
There are eight different instructions defined as:
%
\begin{verbatim}
data Instr = Nop | Push Int | Call Int | Ret | Add | Load | Store | Halt
\end{verbatim}

\noindent Control flow is achieved using the \texttt{Call} and \texttt{Ret}
instructions, which let the program jump back and forth within the instruction
memory using specially labeled values in the stack representing memory
addresses.
%
The argument in the \texttt{Push} instruction represents a value to be
inserted in the stack, whereas the argument of the \texttt{Call} instruction
encodes the number of elements of the stack to be treated as arguments.
%
Then, programs are simply modeled as sequences of instructions.
%
Finally, machine states can be modeled using a 4-tuple \textit{(pc, stk, m, i)}
that represents a particular configuration of the program counter, the stack and
the data- and instruction memories, respectively.
%
To preserve space, we encourage the reader to refer to the work of
\citeauthor{hritcu2013testing} \citeyearpar{hritcu2013testing,
  hrictcu2016testing} as well as the original \fuzzchick paper for more details
about the implementation and semantics of this case study.

\subsubsection{Single-Step Noninterference (SSNI)}

This abstract machine is designed to enforce noninterference, which is based on
the notion of \emph{indistinguishability}.
%
Intuitively, two machine states are indistinguishable from each other if they
only differ on secret data.
%
Using this notion, the particular variant of noninterference we are interested
in this work is called \emph{single-step noninterference}
\cite{hritcu2013testing}.
%
In simple terms, this property asserts that, given two indistinguishable machine
states, running a single instruction on both machines brings them to resulting
states that are again indistinguishable.

The tricky part about this property is, of course, satisfying its sparse
precondition: we need to generate two valid indistinguishable machine states in
order to even proceed to execute the next instruction.
%
As demonstrated by \citeauthor{lampropoulos2019coverage}, generating two
independent machine states using \quickcheck has virtually no chance of
producing valid indistinguishable ones.
%
However, having the mutation mechanism available, we can use a clever trick: we
can obtain a pair of valid indistinguishable machine states by generating a
single valid machine state (something still hard but much easier than before),
and then producing a similar mutated copy.
%
By doing this, we have a much higher chance of producing two almost identical
states that pass the sparse precondition.

\subsubsection{IFC Rules}

In this abstract machine, the enforced IFC rules are implemented using a rule
table indexed by the instruction that the machine is about to perform.
%
This table contains: the dynamic check that the abstract machine needs to
perform in order to enforce the IFC policy, along with the corresponding
security label of the program counter and the instruction result after the
operation is executed.
%
For instance, to execute the \texttt{Store} instruction (which stores a value in
a memory pointer) the machine needs to check that both the label of the program
counter and the label of the pointer together can flow to the label of the
destination memory cell.
%
If this condition is not met, the machine then halts as indication of a
violation in the IFC policy.
%
After this check, this instruction overwrites the value at the destination cell
and updates its label with the maximum sensibility of the involved labels.
%
In the rule table, this looks as follows:

\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Instruction}
& \textbf{Precondition Check}
& \textbf{Final PC Label}
& \textbf{Final Result Label} \\
\hline
\texttt{Store}
& $l_{pc} \vee l_{p} \sqsubseteq l_{v}$
& $l_{pc}$
& $l_{v'} \vee l_{pc} \vee l_{p}$ \\
\hline
\end{tabular}
\end{center}

Where $l_{pc}$, $l_{p}$, $l_{v}$, and $l_{v'}$ represent the labels of: the
program counter, the memory pointer, and the old a new values stored in that
memory cell.
%
The symbol $\vee$ simply denotes the join of two labels, i.e., the
\emph{maximum} of their sensibilities.

In this case study, we systematically introduce several bugs on the IFC
enforcing mechanism by removing or weakening the checks stored in this rule
table.

\subsection{WebAssembly Engine}

WebAssembly \cite{haas2017bringing} is a popular assembly-like language designed
to be an open standard for executing low-level code in the web, although it has
become increasingly popular in standalone, non-web contexts as well.
%
WebAssembly programs are first validated (using a simple form of type-checking)
and later executed in a sandboxed environment (a virtual machine).
%
%% , making this language an attractive target for virtualization in
%% \emph{functions-as-a-service} platforms.
%
The language is relatively simple, in essence
%
\begin{inparaenum}
  \item it contains only four numerical types, representing both integers and
    IEEE754 floating-point numbers of either 32 or 64 bits;
  \item values of these types are by manipulated by functions written using
    sequences of stack instructions;
  \item functions are organized in modules and must be explicitly imported and
    exported;
  \item memory blocks can be imported, exported and grown dynamically;
\end{inparaenum}
%
among others.
%
%% To give the reader a taste, Fig. \ref{fig:wasm:factorial} the WebAssembly text
%% representation of module that exports a recursive implementation of the
%% factorial function.


Unlike most other programming languages, its behavior is fully specified, and
WebAssembly programs are expected to be consistently interpreted across engines
--- despite some subtle details that we will address soon.
%
For this purpose, the WebAssembly standard provides a reference implementation
with all the basic functionality expected from a compliant WebAssembly engine.

%% \begin{wrapfigure}{r}{0.35\textwidth}
%% \vspace{-10pt}
%% \begin{verbatim}
%% (module
%% (func $f (param f64) (result f64)
%%   get_local 0
%%   f64.const 1
%%   f64.lt
%%   if (result f64)
%%     f64.const 1
%%   else
%%     get_local 0
%%     get_local 0
%%     f64.const 1
%%     f64.sub
%%     call $fact
%%     f64.mul
%%   end)
%% (export "fact" (func $f)))
%% \end{verbatim}
%% \caption{\label{fig:wasm:factorial}Factorial in WebAssembly}
%% \vspace{-10pt}
%% \end{wrapfigure}

In this work, we are interested in using \mutagen to test the two most complex
subsystems of \textit{haskell-wasm}: the \emph{validator} and the
\emph{interpreter} --- both being previously tested using a unit test suite.
%
Our tool is an attractive match for testing \textit{haskell-wasm}, as the space
of WebAssembly programs that can be represented using its AST contains mostly
invalid ones, and automatically derived random generators cannot satisfy all the
invariants required to produce interesting test cases.
%
Here, we avoided spending countless hours writing an extensive property-based
specification to mimic the reference WebAssembly specification.
%
Instead, we take advantage of the readily available reference implementation via
differential testing.
%
In this light, our testing properties assert that any result produced by
\textit{haskell-wasm} matches that of the reference implementation.


\begin{table}[t]
\scriptsize
\begin{tabular}{|c|c|c|l|}
\hline
\textbf{Id}
& \textbf{Subsystem}
& \textbf{Category}
& \textbf{Description} \\
\hline
1
& Validator
& Bug
& Invalid memory alignment validation \\
\hline
2
& Validator
& Discrepancy
& Validator accepts blocks returning multiple values  \\
\hline
3
& Interpreter
& Bug
& Instance function invoker silently ignores arity mismatch \\
\hline
4
& Interpreter
& Bug
& Allowed out-of-bounds memory access \\
\hline
5
& Interpreter
& Discrepancy
& NaN reinterpretation does not follow reference \\
\hline
\end{tabular}
\caption{\label{table:wasm:bugs}Bugs and discrepancies found by \mutagen in
  \textit{haskell-wasm}.}
%% \vspace{-20pt}
\end{table}


Unsurprisingly, this engine had several subtle latent bugs that were not caught
by the existing unit tests and that we discovered using \mutagen while
developing the test suite used in this work.
%
Moreover, \mutagen exposed two discrepancies between \textit{haskell-wasm} and
the reference implementation.
%
Not severe enough to be classified as bugs, these discrepancies trigger parts of
the WebAssembly specification that are either not yet supported by the reference
implementation (multi-value blocks), or that produce a well-known
non-deterministic undefined behavior allowed by the specification (NaN
reinterpretation) \cite{perenyi2020stack}.
%
These findings are briefly outlined in Table \ref{table:wasm:bugs}.

%% The rest of this section introduces the test suite we used to test the different
%% parts of \textit{haskell-wasm}.

\subsubsection{Testing the WebAssembly Validator}

We begin by designing a property to test the WebAssembly validator implemented
in \textit{haskell-wasm}.
%
To keep things simple, we can simply assert that, whenever a randomly generated
(or mutated for that matter) WebAssembly module is valid according to
\textit{haskell-wasm}, then the reference implementation agrees upon it.
%
In other words, we are testing for false negatives.
%
In Haskell, we write the following testing property:

\begin{verbatim}
prop_validator m =
  isValidHaskellWasm m ==> isValidRefImpl m
\end{verbatim}

Where the precondition (\texttt{isValidHaskellWasm m}) runs the input WebAssembly
module \texttt{m} against the \textit{haskell-wasm} validator, whereas the
postcondition (\texttt{isValidRefImpl m}) serializes \texttt{m} to a file, runs it
against the reference implementation validator and checks that no errors are
produced.

We want to remark that, although here for simplicity we only focus on finding
false negatives, in a realistic test suite, one would also want to test for
false positives, i.e., when a module is valid and \textit{haskell-wasm} rejects
it.
%
This can be easily done by inverting the direction of the implication
(\texttt{<==}) in the property above.
%
However, the resulting property is much slower, as every tested module will
always be serialized and run against the reference implementation.


%% \begin{verbatim}
%% isValidHaskellWasm m =
%%   case validate m of
%%     Left  validationError -> return False
%%     Right validModule     -> return True
%% \end{verbatim}

%% \begin{verbatim}
%% isValidRefImpl m = do
%%   writeFile "testcase.wasm" (dumpModule m)
%%   res <- shell "./wasm" ["-d", "-i", "testcase.wasm"]
%%   return (res == "")
%% \end{verbatim}

\subsubsection{Testing the WebAssembly Interpreter}

Testing the WebAssembly interpreter is substantially more complicated than
testing the validator, since it requires running actual programs.
%
To achieve this, we need the generated test cases to comply a with a common
interface that can be invoked both by \textit{haskell-wasm} and the reference
WebAssembly implementation.


To keep things simple here as well, we write a function \texttt{mk\_module} to
build a stub module which initializes one memory block and exports a single
WebAssembly function.
%
\texttt{mk\_module} is parameterized by the definition of the single WebAssembly
function, along with its name and type signature.
%
%% In Haskell:
%
%% \begin{verbatim}
%% mk_module ty name fun =
%%   emptyModule { types     = [ ty ]
%%               , functions = [ fun ]
%%               , exports   = [ Export name (ExportFunc 0) ]
%%               , mems      = [ Memory (Limit 1 Nothing) ] }
%% \end{verbatim}
%
This way, we can define a testing property that can be instantiated with
randomly generated combinations of function types, function definitions and
lists of invocation arguments:

%% function type signature, the function implementation, and a list of invocation
%% arguments:

\begin{verbatim}
prop_interpreter ty fun args =
  do let m = mk_module ty "foo" fun
     resHs   <- invokeHaskellWasm m "foo" args
     resSpec <- invokeRefImpl     m "foo" args
     return (equivalent resHs resSpec)
\end{verbatim}

This property instantiates the module stub using the input function and its type
signature, and uses it to invoke the function \texttt{foo} both on the
\textit{haskell-wasm} and reference implementation interpreter with the provided
arguments.
%
Then, the property asserts whether their results are equivalent.%
%
\footnote{In our implementation, we additionally set a short timeout to discard
  potentially diverging programs with infinite loops.}
%
Interestingly, equivalence in this context does not imply equality.
%
Non-deterministic operations in WebAssembly like NaN reinterpretations can
produce different equivalent results (as exposed by the discrepancy \#5 in Table
\ref{table:wasm:bugs}), and our equivalence relation needs to take that into
account.
%


Using this testing property directly might not sound like a great idea, as
randomly generated lists of input arguments will be very unlikely to match the
type signature of randomly generated functions.
%
However, it lets us test what happens when programs are not properly invoked,
and it quickly discovered the previously unknown bug \#3 in
\textit{haskell-wasm} mentioned above.
%
Having solved this issue in \textit{haskell-wasm}, we proceed to define a more
useful specialized version of \texttt{prop\_interpreter} that fixes the type of
the generated function to take two arguments (of type \texttt{I32} and
\texttt{F32}) and return an \texttt{I32} as a result:

\begin{verbatim}
prop_interpreter_i32 fun i f =
  prop_interpreter
    (FuncType { params = [I32, F32], result = [I32] })
    fun
    [VI32 i, VF32 f]
\end{verbatim}

This specialized property lets us generate functions using this fixed type and
invoke them with the exact number and type of arguments required.
%
In our experiments (presented in the next section), we use this property when
finding all the injected bugs into the \textit{haskell-wasm} interpreter.
%
Worth mentioning again, a realistic test suite should at least include different
variants of this property testing functions of several different types, as well
as properties testing multiple functions simultaneously.

%% \subsubsection{Real Bugs and Discrepancies found by \mutagen in \textit{haskell-wasm}}

%% \paragraph{Bug \#1: Invalid Memory Alignment Validation}
%% \paragraph{Bug \#2: Lax invocation checks}
%% \paragraph{Bug \#3: Allowed Out-of-bounds Memory Access}
%% \paragraph{Discrepancy \#1: Different Reinterpretation Semantics of NaN Values}
%% \paragraph{Discrepancy \#2: Allowed Blocks with Multiple Return Types}

% ----------------------------------------

\section{Evaluation}
\label{sec:evaluation}

All the experiments were performed in a dedicated workstation with an Intel Core
i7-8700 CPU running at 3.20GHz, and equipped with 32GB of RAM.
%
We ran each experiment 30 times except for the ones involving the bugs on the
WebAssembly interpreter, which were run 10 times due to time constraints.
%
From there, we followed the same approach taken by
\citeauthor{lampropoulos2019coverage} and collected the mean-time-to-failure
(MTTF) of each bug, i.e., how quickly a bug can be found in wall clock time.
%
In all cases, we used a one-hour timeout to stop the execution of both tools if
they have not yet found a counterexample.

Additionally, we collected the failure rate observed for each bug, i.e. the
proportion of times each tool finds each bug within the one-hour testing budget.
%
We found this metric crucial to be analyzed when replicating \fuzzchick's
results, as opposed to just paying attention at the MTTF.

In both case studies, we additionally show how the FIFO scheduling and trace
reset heuristics described in Section \ref{sec:heuristics} affect the testing
performance by disabling them when using our tool.
%
We call these variants \textit{no FIFO} and \textit{no reset}, respectively.
%
In the case of \textit{no reset}, the amount of random mutations is no longer
controlled by this heuristic, so we arbitrarily fixed it to 25 random mutations
throughout the execution of the tool.

\subsection{IFC Stack Machine}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{tikz/ifc-mttf.tikz}
  \includegraphics[width=\linewidth]{tikz/ifc-fr.tikz}
  %% \vspace{-20pt}
  \caption{\label{fig:results:ifc} Comparison between \fuzzchick and \mutagen
    across 20 different bugs for the IFC stack machine. }
%% \vspace{-10pt}
\end{figure}


The results of this case study are shown in Fig. \ref{fig:results:ifc} --- we
encourage the reader to obtain a color copy of this work.
%
The injected bugs are ordered by the failure rate achieved by \fuzzchick in
decreasing order.
%
Moreover, notice the logarithmic scale used on the MTTF.

As introduced earlier, \fuzzchick can only find 5 out of the 20 bugs injected
with a \%100 success rate within the one hour testing budget.
%
In turn, our tool manages to find every bug on all runs, and taking less than a
minute in the worst absolute case.
%
These results are somewhat pessimistic towards \fuzzchick if compared to the
ones presented originally by \citeauthor{lampropoulos2019coverage}.
%
However, we want to remark that our experiments encompass 30 independent runs
instead of the 5 originally used when presenting \fuzzchick
\citeyearpar{lampropoulos2019coverage}.
%
Moreover, since the MTTF simply aggregates all runs, regardless of if they found
a bug or timed out, this metric is quite sensitive to the particular timeout
used on each experiment, and will tend to inflate the results as soon as the
failure rate goes below 1.
%
For this reason, additionally comparing the failure rate between tools give us a
better estimation of the reliability of both tools, where \mutagen shows a clear
improvement.

%% If we only consider the succesfull runs where \fuzzchick does not time out, we
After analyzing the results obtained using \fuzzchick, we observed a
peculiarity.
%
This tool either finds bugs relatively quickly (after a few thousands tests) or
does not find them at all within the time budget, which suggests that its power
scheduler assigns some mutation candidates too little energy before they become
uninteresting and get ultimately thrown away.
%
Under this consideration, it is fair to think that \fuzzchick might be a better
choice if we set a shorter timeout and run it several times.
%
Thus, to be an improvement over \fuzzchick, our tool must be able to find bugs
not only more reliably, but also relatively fast!



Table \ref{table:ifc:tests} shows a comparison between the mean number of tests
required by both tools to find the first failure \emph{when we only consider
  successful runs.}
%
Additionally, as advised by \cite{arcuri2014hitchhiker} when comparing random
testing tools, we computed the non-parametric Vargha-Delaney $A_{12}$ measure
\cite{vargha2000critique}.
%
Intuitively, this measure encodes the probability of \mutagen to yield better
results than \fuzzchick.
%
As it can be observed, using an exhaustive mutation approach can be not only
more reliable, but also faster.
%
\mutagen is likely to find bugs faster than \fuzzchick in 14 of the 20 bugs
injected into the abstract machine, while being substantially slower in only 2
cases.

\begin{figure}
%% \begin{wrapfigure}{r}{0.5\textwidth}
%% \centering
%% \vspace{-20pt}
\small
\begin{tabular}{ccccc}
\hline
\multicolumn{1}{c}{
  \multirow{2}{*}{Bug}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\fuzzchick}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\mutagen}} &
  \multicolumn{2}{c}{$A_{12}$ measure} \\
\cline{4-5}
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{} &
\multicolumn{1}{c}{Value} &
\multicolumn{1}{c}{Estimate} \\
\hline
1  & 13974.9 & 3632.7 & 0.89 & {\color{red} large} \\ \hline
2  & 23962.9 & 7122.3 & 0.90 & {\color{red} large} \\ \hline
3  & 19678.5 & 4633.9 & 0.89 & {\color{red} large} \\ \hline
4  & 20398.4 & 16831.2 & 0.72 & {\color{red} medium} \\ \hline
5  & 17348.1 & 2326.6 & 0.94 & {\color{red} large} \\ \hline
6  & 10727.1 & 2312.9 & 0.92 & {\color{red} large} \\ \hline
7  & 5070.7 & 332.9 & 0.91 & {\color{red} large} \\ \hline
8  & 5596.4 & 298.7 & 0.90 & {\color{red} large} \\ \hline
9  & 16402.4 & 26342.5 & 0.51 & negligible \\ \hline
10 & 11553.0 & 25044.5 & 0.55 & negligible \\ \hline
11 & 11304.3 & 2536.8 & 0.82 & {\color{red} large} \\ \hline
12 & 18507.3 & 14482.7 & 0.65 & {\color{red} small} \\ \hline
13 & 17961.5 & 13454.9 & 0.65 & {\color{red} small} \\ \hline
14 & 10621.9 & 3928.3 & 0.78 & {\color{red} large} \\ \hline
15 & 23866.3 & 43678.2 & 0.23 & {\color{blue} large} \\ \hline
16 & 25321.7  & 27602.0 & 0.54 & negligible \\ \hline
17 & 28515.6 & 52344.9 & 0.47 & negligible \\ \hline
18 & 24218.6 & 123401.3 & 0.14 & {\color{blue} large} \\ \hline
19 & 18638.9 & 30682.3 & 0.45 & negligible \\ \hline
20 & 18883.1 & 15841.9 & 0.60 & {\color{red} small} \\ \hline
& & Mean & 0.67 & {\color{red} medium} \\
\hline
\end{tabular}
\captionof{table}{\label{table:ifc:tests} Mean tests until first failure and effect size
  comparison between \fuzzchick and \mutagen when considering only successful
  runs.
  %
  Estimates in {\color{blue} blue} and {\color{red} red} indicate results
  favoring \fuzzchick and \mutagen, respectively. }
%% \vspace{-25pt}
%% \end{wrapfigure}
\end{figure}


In terms of the \mutagen heuristics, we can also arrive to some conclusions.
%
Firstly, this case study does not seem to be strongly affected by using a FIFO
scheduling, where the same benchmark takes only roughly \%20 longer to complete
when disabling this heuristic.
%
Upon inspection, we found that the reason behind this is that the mutation
candidate queues remain empty most of the time (generation mode), and when a new
interesting candidate gets inserted, all its mutants (and their descendants) are
processed before the next one is enqueued.
%
So, the proportion of time this heuristic is actually active tends to be minor
under this case study.


On the other hand, disabling the trace saturation heuristic (\textit{no reset})
we observed two interesting effects:
%
having fixed the number of random mutations to 25 adds a seemingly constant
overhead when finding most of the (easier) bugs, suggesting that we are spending
worthless time mutating numeric subexpressions inside the generated machine
states and that a smaller number of random mutations could be equally effective
to find such bugs.
%
However, some of the hardest to find bugs cannot be reliably exposed using this
fixed amount of random mutations (\#4, \#9, \#12, \#18, and \#10), suggesting
that one should increase this number even further to be able to discover all
bugs within the time budget.
%
In consequence, we believe this heuristic is effective at automatically tuning
this internal parameter of our tool, especially when the user is unsure about
what the best value for it might be.


\subsection{WebAssembly Engine}

The results of this case study are shown in Fig. \ref{fig:results:wasm}, ordered
by the MTTF achieved by \mutagen.
%
Given that we used different properties to test the WebAssembly validator and
interpreter, we will focus on these results independently.


We first focus on the bugs injected in the validator (Fig.
\ref{fig:results:wasm} left).
%
There, we can quickly conclude that \quickcheck is not well suited to find most
of the bugs --- it merely finds a counterexample for the easier bugs \#4 and \#5
in just 1 out of 30 runs!
%
The reason behind this simple: WebAssembly modules comprise several different
interconnected components, e.g., types and functions definitions, memory blocks,
imports, exports, etc.
%
All these components need to be valid and agree with each other in order for the
module to be correctly validated (modulo the injected bugs).
%
Using an automatically derived generator that generates each one of these
components independently is virtually unable to produce valid WebAssembly
modules apart from the trivial empty one.
%
Using the same random generator, however, \mutagen is able to consistently find
every bug in less than 20 seconds in the absolute worst case.


In terms of the heuristics implemented in our tool, we can observe two clear
phenomena.
%
On one hand, disabling the FIFO scheduling (case \textit{no FIFO}), the
hardest-to-find bugs (\#7, \#10, and \#1) are no longer found on every run.
%
Moreover, although bugs \#2, \#9, \#8, and \#6 are found with \%100 success rate
across runs, it takes several times longer for \mutagen to find them.
%
On the other hand, disabling the trace saturation heuristic (case \textit{no
  reset}) and using a fixed number of 25 random mutation does not affect the
effectiveness of our tool, apart from adding again a mostly constant overhead on
the time required to find every bug --- suggesting that the value we arbitrarily
chose for this parameter is too large and should be left to be handled
automatically by our tool.


If we now pay attention to the bugs injected into the interpreter (Fig.
\ref{fig:results:wasm} right), we notice that finding bugs requires
substantially more time (minutes instead of seconds), since both interpreters
need to validate and run the inputs before producing a result to compare.
%
Perhaps more interestingly, we can observe a significant improvement in the
performance of \quickcheck in terms of failure rate.
%
This is of no surprise: we deliberately reduced the search space by using a
module stub when defining \texttt{prop\_interpreter} in our test suite.
%
Notably, \quickcheck finds counterexamples for the bug \#14 almost instantly.
%
The reason behind this is that this bug can be found using a rather small
counterexample, and \quickcheck prefers sampling small test cases at the
beginning of the testing loop.
%
Our tool follows this same approach when in generation mode.
%
However, when a test case is found interesting, the scheduler does not take its
size into account while computing its priority --- future work should
investigate this possibility.
%
Nonetheless, \mutagen still outperforms \quickcheck on the remaining bugs both
in terms of failure rate and mean time to failure.
%
Moreover, by disabling the FIFO scheduling we observe that the performance of
our tool gets more unstable, where the bugs \#12 and \#11 take substantially
less time to be found, whereas the bugs \#15 and \#14 cannot be found on every
run.
%
Finding where the best trade-off lies in cases where the heuristics cause mixed
results like this is another interesting direction to aim our future work.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{tikz/wasm-mttf.tikz}
  \includegraphics[width=\linewidth]{tikz/wasm-fr.tikz}\vspace{-5pt}
  \includegraphics[width=\linewidth]{tikz/wasm-legend.tikz}
  %% \vspace{-20pt}
  \caption{\label{fig:results:wasm} Comparison between \quickcheck and \mutagen
    across 15 different bugs injected into \textit{haskell-wasm}. }
%% \vspace{-5pt}
\end{figure}


Finally, this case study allows us to analyze the overhead introduced by the
code instrumentation and internal processing used in \mutagen versus the
stateless black-box approach of \quickcheck.
%
Table \ref{table:wasm:overhead} compares the total number of executed and passed
tests per second that we observed using each tool.
%
It is easy to conclude that \mutagen executes tests several times slower than
\quickcheck --- roughly 20x and 75x slower when testing \texttt{prop\_validator}
and \texttt{prop\_interpreter\_i32}, respectively.
%
This slowdown is partly caused because the code instrumentation our tool
currently uses is built upon a superficial transformation that logs traces at
the user level.
%
We anticipate that this overhead could be reduced considerably by integrating
the instrumentation directly into the Haskell runtime --- a substantial
challenge to tackle in the future.
%
Despite this, our tool is still capable of running substantially more tests that
pass the sparse preconditions than \quickcheck in the same amount of time, and
which ultimately leads us to find bugs.

\begin{table}
\footnotesize
%% \vspace{-5pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{
  \multirow{2}{*}{Property}}
  & \multicolumn{2}{c|}{\quickcheck}
  & \multicolumn{2}{c|}{\mutagen} \\
  \cline{2-5}
  \multicolumn{1}{|c|}{}
  & \multicolumn{1}{c|}{total}
  & passed
  & \multicolumn{1}{c|}{total}
  & passed \\
\hline
\textit{prop\_validator}
& 41374.92
& 0.00029
& 2243.26
& 519.83\\
\hline
\textit{prop\_interpreter\_i32}
& 134000
& 23.16
& 1815.81
& 351.16 \\
\hline
\end{tabular}
\caption{\label{table:wasm:overhead}Mean number of tests per second executed by
  each tool on the WebAssembly case study.}
%% \vspace{-10pt}
\end{table}

% ----------------------------------------

%% \section{Implementation}
%% \label{sec:implementation}

%% \subsection{Deriving Mutation Machinery}

%% Integer placerat tristique nisl. Praesent augue. Fusce commodo. Vestibulum
%% convallis, lorem a tempus semper, dui dui euismod elit, vitae placerat urna
%% tortor vitae lacus. Nullam libero mauris, consequat quis, varius et, dictum id,
%% arcu. Mauris mollis tincidunt felis. Aliquam feugiat tellus ut neque. Nulla
%% facilisis, risus a rhoncus fermentum, tellus tellus lacinia purus, et dictum
%% nunc justo sit amet elit.

%% \subsection{Tracing Haskell Programs}

%% \begin{verbatim}
%%   sorted []       = True
%%   sorted [x]      = True
%%   sorted (x:y:xs) = if x <= y then sorted (y:xs) else False
%% \end{verbatim}

%% \begin{verbatim}
%%   sorted []       = _trace_ 1 (True)
%%   sorted [x]      = _trace_ 2 (True)
%%   sorted (x:y:xs) = _trace_ 3 (if x <= y then _trace_ 4 (sorted (y:xs)) else _trace_ 5 (False))
%% \end{verbatim}

%% Pellentesque dapibus suscipit ligula. Donec posuere augue in quam. Etiam vel
%% tortor sodales tellus ultricies commodo. Suspendisse potenti. Aenean in sem ac
%% leo mollis blandit. Donec neque quam, dignissim in, mollis nec, sagittis eu,
%% wisi. Phasellus lacus. Etiam laoreet quam sed arcu. Phasellus at dui in ligula
%% mollis ultricies.

% ----------------------------------------

\section{Related work}
\label{sec:related}

There exists a vast literature on fuzzing and property-based testing.
%
In this section we only focus on three main topics:
%
automated random data generation using static information,
%
fuzzing using coverage information, and
%
exhaustive bounded testing, all of which inspired the design of \mutagen.


\paragraph{Automated Random Data Generation}

Obtaining good random generators automatically from static information (e.g.,
grammars or data type definitions) has been tackled from different angles.

% DRAGEN and DRAGEN2
In Haskell, DRAGEN \cite{DBLP:conf/haskell/MistaRH18} is a meta-programming tool
that synthesises random generators from data types definitions, using stochastic
models to predict and optimize their distribution based on a target one set by
the user.
%
DRAGEN2 \cite{Mista2019GeneratingRS} extends this idea adding support for
generating richer random data by extracting static information like library APIs
and function input patterns from the codebase.
%
% Bolztmann-brain
Similarly, \cite{Bendkowski2017} developed a polynomial tuning mechanism based
on Boltzmann samplers \cite{Duchon2004} that synthesizes generators with
approximate-size distributions for combinatorial structures like lists or trees.

%
% QuickFuzz
%
Exploiting some of the ideas described above, \emph{QuickFuzz} \cite{GriecoCB16,
  grieco2017} is a generational fuzzer that leverages existing Haskell libraries
describing common file formats to synthesize random data generators that are
used in tandem with existing off-the-shelf low-level fuzzers to find bugs in
heavily used programs.



Automatically deriving random generators is substantially more complicated when
the generated data must satisfy (often sparse) preconditions.
%
% Generating Constrained Random Data with Uniform Distribution
%
\citeauthor{ClaessenDP14} \citeyearpar{ClaessenDP14} developed an algorithm for
generating inputs constrained by boolean preconditions with almost-uniform
distribution.
%
% Luck
%
Later, \citeauthor{LampropoulosGHH17} \citeyearpar{LampropoulosGHH17} extends
this approach by adding a limited form of constraint solving controllable by the
user in a domain-specific language called \emph{Luck}.
%
% Deriving good generators
%
Recently, \citeauthor{Lampropoulos2017} \citeyearpar{Lampropoulos2017} proposed
a derivation mechanism to obtain constrained random generators directly by
defining their structure using inductively defined relations in Coq.



We consider all these generational approaches to be orthogonal to the ideas
behind \mutagen.
%
In principle, our tool is tailored to improve the performance of poor
automatically derived generators.
%
However, more specialized generators can be used directly by \mutagen, and
combining them with type-preserving mutations using a hybrid technique is an
idea that we aim to address in the future.


\paragraph{Coverage-guided Fuzzing}

% AFL
\emph{AFL} \cite{afl} is the reference tool when it comes to coverage-guided
fuzzing.
%
% AFLfast
\emph{AFLFast} \cite{bohme2017coverage} is an extension to AFL that uses Markov
chain models to tune the power scheduler towards testing low-frequency paths.
%
In \mutagen, the scheduler does not account for path frequency.
%
Instead, it favors a breadth-first traversal of the execution path space.
%
% CollAFL
Similar to our tool, \emph{CollAFL} \cite{gan2018collafl} is a variant of AFL
that uses path- instead of edge-based coverage, which helps distinguishing
executions more precisely by reducing path collisions.

%AFLGo
When the source code history is available, \emph{AFLGo} \cite{bohme2017directed}
is a fuzzer that can be targeted to exercise the specific parts of the code
affected by recent commits in order to find potential new bugs.
%
Back to PBT, a related idea called \emph{targeted property-based testing} (TPBT)
is to use fitness functions to guide the testing efforts towards user defined
goals \cite{loscher2017targeted, loscher2018automating}.


% Coverage-guided fuzzing + symbolic execution
%
Moreover, a popular technique is to combine coverage-guided fuzzing with ideas
borrowed from symbolic execution tools like \emph{KLEE} \cite{cadar2008klee} to
avoid getting stuck in superficial paths \cite{stephens2016driller}.
%
When using off-the-shelf symbolic executors, this technique is often limited by
the path explosion problem, although recent tools like \emph{QSYM}
\cite{yun2018qsym} demonstrate that using tailored concolic executors can help
to overcome this limitation.


Most of these ideas can potentially be incorporated in our tool, and we keep
them as a challenge for future work.


\paragraph{Exhaustive Bounded Testing}

A popular category of property-based testing tools does not rely on randomness.
%
Instead, all possible inputs can be enumerated and tested from smaller to larger
up to a certain size bound.
%
%
% Feat
%
\emph{Feat} \cite{DuregardJW12} formalizes the notion of \emph{functional
  enumerations}.
%
For any algebraic type, it synthesizes a bijection between a finite prefix of
the natural numbers and a set of increasingly larger values of the input type.
%
Later, this bijection can be traversed exhaustively or, more interestingly,
randomly accessed.
%
This allows the user to easily generate random values uniformly simply by
sampling natural numbers.
%
However, values are enumerated based only on their type definition, so this
technique is not suitable for testing properties with sparse preconditions
expressed elsewhere.
%
% SmallCheck and LazySmallCheck
\emph{SmallCheck} \cite{runciman2008smallcheck} is a Haskell tool that also
follows this idea.
%
It progressively executes the testing properties against all possible input
values up to a certain size bound.
%
% Korat
On the object-oriented realm, \emph{Korat} \cite{boyapati2002korat} is a Java
tool that uses method specification predicates to automatically generate all
non-isomorphic test cases up to a given small size.


Being exhaustive, these approaches rely on pruning mechanisms to avoid
populating unevaluated subexpresions exhaustively before the computational cost
becomes too restrictive.
%
\emph{LazySmallCheck} is a variant of \emph{SmallCheck} that uses lazy
evaluation to automatically prune the search space by detecting unevaluated
subexpressions.
%
In the case of \emph{Korat}, pruning is done by instrumenting method
precondition predicates and analyzing which parts of the execution trace
correspond to each evaluated subexpression.


In this work we use exhaustiveness as a way to reliably enforce that all
possible mutants of an interesting seed are executed.
%
In contrast to fully exhaustive testing tools, \mutagen initially relies on
randomness to find initial interesting mutable candidates.
%
In terms of pruning, it is possible to instruct \mutagen to detect unused
subexpressions (using a technique similar to that of \emph{LazySmallCheck}) to
avoid scheduling mutations over their corresponding positions.
%
This could, in principle, improve the overall performance when testing
properties that tend to short-circuit, leaving parts of the input unevaluated.
%
In our case studies, however, we observed that their preconditions tend to be
quite strict when executing test cases obtained by mutating passed existing
ones, fully evaluating their input before executing the postcondition.
%
Thus, we do not expect to see considerable improvements by applying this idea in
this particular scenario.
%
Gathering empirical evidence about using pruning via lazy evaluation in \mutagen
is an effort that we keep as future work.



% ----------------------------------------

\section{Conclusions}
\label{sec:conclusions}

We presented \mutagen, a coverage-guided, property-based testing framework
written in Haskell.
%
Inspired by \fuzzchick, our tool uses coverage information to guide
automatically derived mutators producing high-level, type-preserving mutations.
%
However, instead of relying heavily on randomness and power schedules to find
bugs, our tool uses an exhaustive mutation approach that generates every
possible mutant for each interesting input candidate, and schedules it to be
tested exactly once.
%
This is in turn inspired by exhaustive bounded testing tools that focus on
testing every possible input value of the system under test up to a certain size
bound.
%
Moreover, we foresee no problems into translating our tool in other programming
languages beyond Haskell.

%% --- a more generic technique of limited applicability.


Our experimental results indicate that \mutagen outperforms the simpler approach
taken by \fuzzchick in terms of both failure rate and tests until first failure.
%
Moreover, we show how our tool can be applied in a real-world testing scenario,
where it quickly discovers several planted and some previously unknown bugs.


As already indicated throughout this work, there are several directions for
future work we aim to investigate:
%
combining \mutagen with specialized generators; improving the code
instrumentation in order to effectively target mutations towards specific parts
of the code; and evaluating the effect of lazy pruning; among others.
%
In addition, we want to investigate how to redefine our automatically
synthesized mutators in a stateful manner.
%
This way, it would be possible to apply mutations that preserve complex
properties of the generated data simply by construction, e.g., mutations that
\emph{always} produce well-typed programs where variables are always in scope.
%
The main challenge will be to achieve this while keeping the testing process as
automatable as possible.


% ----------------------------------------

%% The acknowledgments section is defined using the "acks" environment (and NOT
%% an unnumbered section). This ensures the proper identification of the section
%% in the article metadata, and the consistent spelling of the heading.

%% \begin{acks}
%% To Robert, for the bagels and explaining CMYK and color spaces.
%% \end{acks}

% ----------------------------------------

%% The next two lines define the bibliography style to be used, and the
%% bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

% ----------------------------------------

%% If your work has an appendix, this is the place to put it.
%% \appendix

\end{document}
\endinput
%%
%% End of `main.tex'.
